{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266e3ae2",
   "metadata": {},
   "source": [
    "# Video Recognition\n",
    "\n",
    "Project on video recognition whith the dataset HMDB51 (https://serre.lab.brown.edu/hmdb51.html). A special focus is given to the efficiency of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c35e7",
   "metadata": {},
   "source": [
    "Training finora:\n",
    "- 5 epoche tutto il dataset lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a71d5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5339e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle\n",
    "\n",
    "# Import everything from your new file\n",
    "from video_recognition import (\n",
    "    VideoLoader, CNN, CNNLSTM, train, save_model, load_model, \n",
    "    replace_head_for_finetuning, MAX_POOL, get_persistent_splits\n",
    ")\n",
    "\n",
    "dataset_directory = \"./dataset\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026c4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 244\n",
    "FRAME_RATE_SCALER = 2\n",
    "BATCH_SIZE = 1\n",
    "ACCUM_STEPS = 16 \n",
    "EMBEDDING_DIM = 256\n",
    "LSTM_HIDDEN = 128\n",
    "LSTM_LAYERS = 1\n",
    "\n",
    "cnn_config = [\n",
    "    {'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 128, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3595200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes loaded: ['brush_hair', 'cartwheel', 'catch', 'chew', 'clap', 'climb', 'climb_stairs', 'dive', 'draw_sword', 'dribble', 'drink', 'eat', 'fall_floor', 'fencing', 'flic_flac', 'golf', 'handstand', 'hit', 'hug', 'jump', 'kick', 'kick_ball', 'kiss', 'laugh', 'pick', 'pour', 'pullup', 'punch', 'push', 'pushup', 'ride_bike', 'ride_horse', 'run', 'shake_hands', 'shoot_ball', 'shoot_bow', 'shoot_gun', 'sit', 'situp', 'smile', 'smoke', 'somersault', 'stand', 'swing_baseball', 'sword', 'sword_exercise', 'talk', 'throw', 'turn', 'walk', 'wave']\n",
      "Database size: 6341\n",
      "Loading existing split from pretrain_full...\n",
      "Pre-training on 51 classes...\n",
      "Epoch 1 Step [20/5072] Loss: 3.9530\n",
      "Epoch 1 Step [40/5072] Loss: 4.0157\n",
      "Epoch 1 Step [60/5072] Loss: 3.9837\n",
      "Epoch 1 Step [80/5072] Loss: 3.9517\n",
      "Epoch 1 Step [100/5072] Loss: 3.9158\n",
      "Epoch 1 Step [120/5072] Loss: 3.9606\n",
      "Epoch 1 Step [140/5072] Loss: 3.9523\n",
      "Epoch 1 Step [160/5072] Loss: 3.9214\n",
      "Epoch 1 Step [180/5072] Loss: 3.9013\n",
      "Epoch 1 Step [200/5072] Loss: 3.9309\n",
      "Epoch 1 Step [220/5072] Loss: 3.9365\n",
      "Epoch 1 Step [240/5072] Loss: 3.9146\n",
      "Epoch 1 Step [260/5072] Loss: 3.9273\n",
      "Epoch 1 Step [280/5072] Loss: 3.8983\n",
      "Epoch 1 Step [300/5072] Loss: 3.9343\n",
      "Epoch 1 Step [320/5072] Loss: 3.8665\n",
      "Epoch 1 Step [340/5072] Loss: 3.7585\n",
      "Epoch 1 Step [360/5072] Loss: 3.8550\n",
      "Epoch 1 Step [380/5072] Loss: 3.8937\n",
      "Epoch 1 Step [400/5072] Loss: 3.9541\n",
      "Epoch 1 Step [420/5072] Loss: 3.8104\n",
      "Epoch 1 Step [440/5072] Loss: 4.0513\n",
      "Epoch 1 Step [460/5072] Loss: 3.9385\n",
      "Epoch 1 Step [480/5072] Loss: 3.9307\n",
      "Epoch 1 Step [500/5072] Loss: 3.9425\n",
      "Epoch 1 Step [520/5072] Loss: 3.5980\n",
      "Epoch 1 Step [540/5072] Loss: 3.9917\n",
      "Epoch 1 Step [560/5072] Loss: 3.7110\n",
      "Epoch 1 Step [580/5072] Loss: 3.7282\n",
      "Epoch 1 Step [600/5072] Loss: 3.9703\n",
      "Epoch 1 Step [620/5072] Loss: 3.8531\n",
      "Epoch 1 Step [640/5072] Loss: 4.0222\n",
      "Epoch 1 Step [660/5072] Loss: 3.9791\n",
      "Epoch 1 Step [680/5072] Loss: 3.8589\n",
      "Epoch 1 Step [700/5072] Loss: 3.8613\n",
      "Epoch 1 Step [720/5072] Loss: 4.0247\n",
      "Epoch 1 Step [740/5072] Loss: 3.9419\n",
      "Epoch 1 Step [760/5072] Loss: 3.8327\n",
      "Epoch 1 Step [780/5072] Loss: 3.9032\n",
      "Epoch 1 Step [800/5072] Loss: 3.7962\n",
      "Epoch 1 Step [820/5072] Loss: 3.9615\n",
      "Epoch 1 Step [840/5072] Loss: 3.8774\n",
      "Epoch 1 Step [860/5072] Loss: 3.8090\n",
      "Epoch 1 Step [880/5072] Loss: 3.7166\n",
      "Epoch 1 Step [900/5072] Loss: 3.8731\n",
      "Epoch 1 Step [920/5072] Loss: 3.9351\n",
      "Epoch 1 Step [940/5072] Loss: 3.9039\n",
      "Epoch 1 Step [960/5072] Loss: 3.7020\n",
      "Epoch 1 Step [980/5072] Loss: 3.8425\n",
      "Epoch 1 Step [1000/5072] Loss: 4.0358\n",
      "Epoch 1 Step [1020/5072] Loss: 3.9109\n",
      "Epoch 1 Step [1040/5072] Loss: 3.8269\n",
      "Epoch 1 Step [1060/5072] Loss: 3.8001\n",
      "Epoch 1 Step [1080/5072] Loss: 3.7529\n",
      "Epoch 1 Step [1100/5072] Loss: 3.8789\n",
      "Epoch 1 Step [1120/5072] Loss: 3.8140\n",
      "Epoch 1 Step [1140/5072] Loss: 3.9695\n",
      "Epoch 1 Step [1160/5072] Loss: 3.7726\n",
      "Epoch 1 Step [1180/5072] Loss: 3.7447\n",
      "Epoch 1 Step [1200/5072] Loss: 3.8132\n",
      "Epoch 1 Step [1220/5072] Loss: 3.8024\n",
      "Epoch 1 Step [1240/5072] Loss: 3.6919\n",
      "Epoch 1 Step [1260/5072] Loss: 3.6938\n",
      "Epoch 1 Step [1280/5072] Loss: 3.5245\n",
      "Epoch 1 Step [1300/5072] Loss: 3.6606\n",
      "Epoch 1 Step [1320/5072] Loss: 3.7665\n",
      "Epoch 1 Step [1340/5072] Loss: 3.6415\n",
      "Epoch 1 Step [1360/5072] Loss: 3.6413\n",
      "Epoch 1 Step [1380/5072] Loss: 3.6993\n",
      "Epoch 1 Step [1400/5072] Loss: 3.9420\n",
      "Epoch 1 Step [1420/5072] Loss: 3.8402\n",
      "Epoch 1 Step [1440/5072] Loss: 3.7378\n",
      "Epoch 1 Step [1460/5072] Loss: 3.6840\n",
      "Epoch 1 Step [1480/5072] Loss: 3.8080\n",
      "Epoch 1 Step [1500/5072] Loss: 3.7309\n",
      "Epoch 1 Step [1520/5072] Loss: 3.6931\n",
      "Epoch 1 Step [1540/5072] Loss: 3.8006\n",
      "Epoch 1 Step [1560/5072] Loss: 3.6088\n",
      "Epoch 1 Step [1580/5072] Loss: 3.6931\n",
      "Epoch 1 Step [1600/5072] Loss: 3.5838\n",
      "Epoch 1 Step [1620/5072] Loss: 3.7521\n",
      "Epoch 1 Step [1640/5072] Loss: 3.6248\n",
      "Epoch 1 Step [1660/5072] Loss: 3.7790\n",
      "Epoch 1 Step [1680/5072] Loss: 3.7955\n",
      "Epoch 1 Step [1700/5072] Loss: 3.8017\n",
      "Epoch 1 Step [1720/5072] Loss: 3.7065\n",
      "Epoch 1 Step [1740/5072] Loss: 4.1149\n",
      "Epoch 1 Step [1760/5072] Loss: 3.7026\n",
      "Epoch 1 Step [1780/5072] Loss: 3.6541\n",
      "Epoch 1 Step [1800/5072] Loss: 3.8974\n",
      "Epoch 1 Step [1820/5072] Loss: 3.6493\n",
      "Epoch 1 Step [1840/5072] Loss: 3.9096\n",
      "Epoch 1 Step [1860/5072] Loss: 3.9239\n",
      "Epoch 1 Step [1880/5072] Loss: 4.0658\n",
      "Epoch 1 Step [1900/5072] Loss: 3.8271\n",
      "Epoch 1 Step [1920/5072] Loss: 3.8679\n",
      "Epoch 1 Step [1940/5072] Loss: 3.7832\n",
      "Epoch 1 Step [1960/5072] Loss: 3.8246\n",
      "Epoch 1 Step [1980/5072] Loss: 3.6068\n",
      "Epoch 1 Step [2000/5072] Loss: 3.8821\n",
      "Epoch 1 Step [2020/5072] Loss: 3.7789\n",
      "Epoch 1 Step [2040/5072] Loss: 3.8671\n",
      "Epoch 1 Step [2060/5072] Loss: 3.6404\n",
      "Epoch 1 Step [2080/5072] Loss: 3.9072\n",
      "Epoch 1 Step [2100/5072] Loss: 3.8082\n",
      "Epoch 1 Step [2120/5072] Loss: 3.7884\n",
      "Epoch 1 Step [2140/5072] Loss: 3.6605\n",
      "Epoch 1 Step [2160/5072] Loss: 3.5992\n",
      "Epoch 1 Step [2180/5072] Loss: 3.8035\n",
      "Epoch 1 Step [2200/5072] Loss: 3.8128\n",
      "Epoch 1 Step [2220/5072] Loss: 3.6858\n",
      "Epoch 1 Step [2240/5072] Loss: 3.5970\n",
      "Epoch 1 Step [2260/5072] Loss: 3.7664\n",
      "Epoch 1 Step [2280/5072] Loss: 3.6040\n",
      "Epoch 1 Step [2300/5072] Loss: 3.9042\n",
      "Epoch 1 Step [2320/5072] Loss: 3.7823\n",
      "Epoch 1 Step [2340/5072] Loss: 3.7705\n",
      "Epoch 1 Step [2360/5072] Loss: 3.5495\n",
      "Epoch 1 Step [2380/5072] Loss: 3.7920\n",
      "Epoch 1 Step [2400/5072] Loss: 3.6112\n",
      "Epoch 1 Step [2420/5072] Loss: 3.6593\n",
      "Epoch 1 Step [2440/5072] Loss: 3.3750\n",
      "Epoch 1 Step [2460/5072] Loss: 3.9838\n",
      "Epoch 1 Step [2480/5072] Loss: 3.9640\n",
      "Epoch 1 Step [2500/5072] Loss: 3.7691\n",
      "Epoch 1 Step [2520/5072] Loss: 3.5181\n",
      "Epoch 1 Step [2540/5072] Loss: 3.7869\n",
      "Epoch 1 Step [2560/5072] Loss: 4.1322\n",
      "Epoch 1 Step [2580/5072] Loss: 3.5741\n",
      "Epoch 1 Step [2600/5072] Loss: 3.8297\n",
      "Epoch 1 Step [2620/5072] Loss: 3.6009\n",
      "Epoch 1 Step [2640/5072] Loss: 3.7189\n",
      "Epoch 1 Step [2660/5072] Loss: 3.7224\n",
      "Epoch 1 Step [2680/5072] Loss: 3.7641\n",
      "Epoch 1 Step [2700/5072] Loss: 3.6308\n",
      "Epoch 1 Step [2720/5072] Loss: 3.7056\n",
      "Epoch 1 Step [2740/5072] Loss: 4.0790\n",
      "Epoch 1 Step [2760/5072] Loss: 3.4914\n",
      "Epoch 1 Step [2780/5072] Loss: 3.5580\n",
      "Epoch 1 Step [2800/5072] Loss: 3.7608\n",
      "Epoch 1 Step [2820/5072] Loss: 3.7791\n",
      "Epoch 1 Step [2840/5072] Loss: 3.7441\n",
      "Epoch 1 Step [2860/5072] Loss: 3.5783\n",
      "Epoch 1 Step [2880/5072] Loss: 3.8189\n",
      "Epoch 1 Step [2900/5072] Loss: 3.5296\n",
      "Epoch 1 Step [2920/5072] Loss: 3.4871\n",
      "Epoch 1 Step [2940/5072] Loss: 3.4883\n",
      "Epoch 1 Step [2960/5072] Loss: 3.9552\n",
      "Epoch 1 Step [2980/5072] Loss: 3.5657\n",
      "Epoch 1 Step [3000/5072] Loss: 3.6023\n",
      "Epoch 1 Step [3020/5072] Loss: 3.3292\n",
      "Epoch 1 Step [3040/5072] Loss: 3.8008\n",
      "Epoch 1 Step [3060/5072] Loss: 3.5344\n",
      "Epoch 1 Step [3080/5072] Loss: 3.3544\n",
      "Epoch 1 Step [3100/5072] Loss: 3.9429\n",
      "Epoch 1 Step [3120/5072] Loss: 3.4364\n",
      "Epoch 1 Step [3140/5072] Loss: 3.5617\n",
      "Epoch 1 Step [3160/5072] Loss: 3.5885\n",
      "Epoch 1 Step [3180/5072] Loss: 3.5567\n",
      "Epoch 1 Step [3200/5072] Loss: 3.5695\n",
      "Epoch 1 Step [3220/5072] Loss: 3.6373\n",
      "Epoch 1 Step [3240/5072] Loss: 3.5144\n",
      "Epoch 1 Step [3260/5072] Loss: 3.4074\n",
      "Epoch 1 Step [3280/5072] Loss: 4.0156\n",
      "Epoch 1 Step [3300/5072] Loss: 3.7741\n",
      "Epoch 1 Step [3320/5072] Loss: 3.7780\n",
      "Epoch 1 Step [3340/5072] Loss: 3.7784\n",
      "Epoch 1 Step [3360/5072] Loss: 3.7519\n",
      "Epoch 1 Step [3380/5072] Loss: 3.6555\n",
      "Epoch 1 Step [3400/5072] Loss: 3.6551\n",
      "Epoch 1 Step [3420/5072] Loss: 3.7145\n",
      "Epoch 1 Step [3440/5072] Loss: 3.7928\n",
      "Epoch 1 Step [3460/5072] Loss: 3.5928\n",
      "Epoch 1 Step [3480/5072] Loss: 3.7805\n",
      "Epoch 1 Step [3500/5072] Loss: 3.7796\n",
      "Epoch 1 Step [3520/5072] Loss: 4.1155\n",
      "Epoch 1 Step [3540/5072] Loss: 3.7439\n",
      "Epoch 1 Step [3560/5072] Loss: 3.8639\n",
      "Epoch 1 Step [3580/5072] Loss: 3.7085\n",
      "Epoch 1 Step [3600/5072] Loss: 3.6321\n",
      "Epoch 1 Step [3620/5072] Loss: 3.5421\n",
      "Epoch 1 Step [3640/5072] Loss: 3.8156\n",
      "Epoch 1 Step [3660/5072] Loss: 3.5242\n",
      "Epoch 1 Step [3680/5072] Loss: 3.7456\n",
      "Epoch 1 Step [3700/5072] Loss: 3.6359\n",
      "Epoch 1 Step [3720/5072] Loss: 3.8335\n",
      "Epoch 1 Step [3740/5072] Loss: 3.7343\n",
      "Epoch 1 Step [3760/5072] Loss: 3.4782\n",
      "Epoch 1 Step [3780/5072] Loss: 3.5114\n",
      "Epoch 1 Step [3800/5072] Loss: 3.5609\n",
      "Epoch 1 Step [3820/5072] Loss: 3.5912\n",
      "Epoch 1 Step [3840/5072] Loss: 3.7031\n",
      "Epoch 1 Step [3860/5072] Loss: 3.6312\n",
      "Epoch 1 Step [3880/5072] Loss: 3.7091\n",
      "Epoch 1 Step [3900/5072] Loss: 3.6506\n",
      "Epoch 1 Step [3920/5072] Loss: 3.6834\n",
      "Epoch 1 Step [3940/5072] Loss: 3.6067\n",
      "Epoch 1 Step [3960/5072] Loss: 3.8469\n",
      "Epoch 1 Step [3980/5072] Loss: 3.6323\n",
      "Epoch 1 Step [4000/5072] Loss: 3.4989\n",
      "Epoch 1 Step [4020/5072] Loss: 3.5445\n",
      "Epoch 1 Step [4040/5072] Loss: 3.8885\n",
      "Epoch 1 Step [4060/5072] Loss: 3.7077\n",
      "Epoch 1 Step [4080/5072] Loss: 3.5750\n",
      "Epoch 1 Step [4100/5072] Loss: 3.6493\n",
      "Epoch 1 Step [4120/5072] Loss: 3.9042\n",
      "Epoch 1 Step [4140/5072] Loss: 3.7477\n",
      "Epoch 1 Step [4160/5072] Loss: 3.6844\n",
      "Epoch 1 Step [4180/5072] Loss: 3.8547\n",
      "Epoch 1 Step [4200/5072] Loss: 3.4941\n",
      "Epoch 1 Step [4220/5072] Loss: 3.2607\n",
      "Epoch 1 Step [4240/5072] Loss: 3.6335\n",
      "Epoch 1 Step [4260/5072] Loss: 3.6215\n",
      "Epoch 1 Step [4280/5072] Loss: 3.2439\n",
      "Epoch 1 Step [4300/5072] Loss: 3.7530\n",
      "Epoch 1 Step [4320/5072] Loss: 3.8005\n",
      "Epoch 1 Step [4340/5072] Loss: 3.6981\n",
      "Epoch 1 Step [4360/5072] Loss: 3.4423\n",
      "Epoch 1 Step [4380/5072] Loss: 3.8432\n",
      "Epoch 1 Step [4400/5072] Loss: 3.6928\n",
      "Epoch 1 Step [4420/5072] Loss: 3.7326\n",
      "Epoch 1 Step [4440/5072] Loss: 3.6402\n",
      "Epoch 1 Step [4460/5072] Loss: 3.6879\n",
      "Epoch 1 Step [4480/5072] Loss: 3.6436\n",
      "Epoch 1 Step [4500/5072] Loss: 3.7818\n",
      "Epoch 1 Step [4520/5072] Loss: 3.8146\n",
      "Epoch 1 Step [4540/5072] Loss: 3.6379\n",
      "Epoch 1 Step [4560/5072] Loss: 3.5493\n",
      "Epoch 1 Step [4580/5072] Loss: 3.6820\n",
      "Epoch 1 Step [4600/5072] Loss: 3.6777\n",
      "Epoch 1 Step [4620/5072] Loss: 3.7318\n",
      "Epoch 1 Step [4640/5072] Loss: 3.6528\n",
      "Epoch 1 Step [4660/5072] Loss: 3.5342\n",
      "Epoch 1 Step [4680/5072] Loss: 3.7968\n",
      "Epoch 1 Step [4700/5072] Loss: 3.8383\n",
      "Epoch 1 Step [4720/5072] Loss: 3.8354\n",
      "Epoch 1 Step [4740/5072] Loss: 3.9336\n",
      "Epoch 1 Step [4760/5072] Loss: 3.7761\n",
      "Epoch 1 Step [4780/5072] Loss: 3.5247\n",
      "Epoch 1 Step [4800/5072] Loss: 3.6489\n",
      "Epoch 1 Step [4820/5072] Loss: 3.8828\n",
      "Epoch 1 Step [4840/5072] Loss: 3.8367\n",
      "Epoch 1 Step [4860/5072] Loss: 3.5770\n",
      "Epoch 1 Step [4880/5072] Loss: 3.6888\n",
      "Epoch 1 Step [4900/5072] Loss: 3.4938\n",
      "Epoch 1 Step [4920/5072] Loss: 3.6204\n",
      "Epoch 1 Step [4940/5072] Loss: 3.6346\n",
      "Epoch 1 Step [4960/5072] Loss: 3.6250\n",
      "Epoch 1 Step [4980/5072] Loss: 3.7246\n",
      "Epoch 1 Step [5000/5072] Loss: 3.7600\n",
      "Epoch 1 Step [5020/5072] Loss: 3.4336\n",
      "Epoch 1 Step [5040/5072] Loss: 3.7226\n",
      "Epoch 1 Step [5060/5072] Loss: 3.4741\n",
      "Epoch 1 Finished | Acc: 7.65% | Loss: 3.7446\n",
      "Epoch 2 Step [20/5072] Loss: 3.5867\n",
      "Epoch 2 Step [40/5072] Loss: 3.7790\n",
      "Epoch 2 Step [60/5072] Loss: 3.8436\n",
      "Epoch 2 Step [80/5072] Loss: 3.9111\n",
      "Epoch 2 Step [100/5072] Loss: 3.8146\n",
      "Epoch 2 Step [120/5072] Loss: 3.8332\n",
      "Epoch 2 Step [140/5072] Loss: 3.5353\n",
      "Epoch 2 Step [160/5072] Loss: 3.9161\n",
      "Epoch 2 Step [180/5072] Loss: 3.8038\n",
      "Epoch 2 Step [200/5072] Loss: 3.5068\n",
      "Epoch 2 Step [220/5072] Loss: 3.5323\n",
      "Epoch 2 Step [240/5072] Loss: 3.4767\n",
      "Epoch 2 Step [260/5072] Loss: 3.7604\n",
      "Epoch 2 Step [280/5072] Loss: 3.3122\n",
      "Epoch 2 Step [300/5072] Loss: 3.7297\n",
      "Epoch 2 Step [320/5072] Loss: 3.5370\n",
      "Epoch 2 Step [340/5072] Loss: 3.7686\n",
      "Epoch 2 Step [360/5072] Loss: 3.7825\n",
      "Epoch 2 Step [380/5072] Loss: 3.6649\n",
      "Epoch 2 Step [400/5072] Loss: 3.3038\n",
      "Epoch 2 Step [420/5072] Loss: 3.5537\n",
      "Epoch 2 Step [440/5072] Loss: 3.6373\n",
      "Epoch 2 Step [460/5072] Loss: 3.3826\n",
      "Epoch 2 Step [480/5072] Loss: 3.5508\n",
      "Epoch 2 Step [500/5072] Loss: 3.9080\n",
      "Epoch 2 Step [520/5072] Loss: 3.9389\n",
      "Epoch 2 Step [540/5072] Loss: 3.7859\n",
      "Epoch 2 Step [560/5072] Loss: 3.5269\n",
      "Epoch 2 Step [580/5072] Loss: 3.4928\n",
      "Epoch 2 Step [600/5072] Loss: 3.6928\n",
      "Epoch 2 Step [620/5072] Loss: 3.3948\n",
      "Epoch 2 Step [640/5072] Loss: 3.5863\n",
      "Epoch 2 Step [660/5072] Loss: 3.6777\n",
      "Epoch 2 Step [680/5072] Loss: 3.5685\n",
      "Epoch 2 Step [700/5072] Loss: 3.5954\n",
      "Epoch 2 Step [720/5072] Loss: 3.8542\n",
      "Epoch 2 Step [740/5072] Loss: 3.5989\n",
      "Epoch 2 Step [760/5072] Loss: 3.8873\n",
      "Epoch 2 Step [780/5072] Loss: 3.6229\n",
      "Epoch 2 Step [800/5072] Loss: 3.4335\n",
      "Epoch 2 Step [820/5072] Loss: 3.6260\n",
      "Epoch 2 Step [840/5072] Loss: 3.6668\n",
      "Epoch 2 Step [860/5072] Loss: 3.6310\n",
      "Epoch 2 Step [880/5072] Loss: 3.7473\n",
      "Epoch 2 Step [900/5072] Loss: 3.7823\n",
      "Epoch 2 Step [920/5072] Loss: 3.5706\n",
      "Epoch 2 Step [940/5072] Loss: 3.8809\n",
      "Epoch 2 Step [960/5072] Loss: 3.6704\n",
      "Epoch 2 Step [980/5072] Loss: 3.6865\n",
      "Epoch 2 Step [1000/5072] Loss: 3.5134\n",
      "Epoch 2 Step [1020/5072] Loss: 3.5614\n",
      "Epoch 2 Step [1040/5072] Loss: 3.4836\n",
      "Epoch 2 Step [1060/5072] Loss: 3.3884\n",
      "Epoch 2 Step [1080/5072] Loss: 3.7134\n",
      "Epoch 2 Step [1100/5072] Loss: 3.7212\n",
      "Epoch 2 Step [1120/5072] Loss: 3.8587\n",
      "Epoch 2 Step [1140/5072] Loss: 3.9341\n",
      "Epoch 2 Step [1160/5072] Loss: 3.7375\n",
      "Epoch 2 Step [1180/5072] Loss: 3.8588\n",
      "Epoch 2 Step [1200/5072] Loss: 3.6425\n",
      "Epoch 2 Step [1220/5072] Loss: 3.4345\n",
      "Epoch 2 Step [1240/5072] Loss: 3.6184\n",
      "Epoch 2 Step [1260/5072] Loss: 3.3288\n",
      "Epoch 2 Step [1280/5072] Loss: 3.4021\n",
      "Epoch 2 Step [1300/5072] Loss: 3.6246\n",
      "Epoch 2 Step [1320/5072] Loss: 3.7129\n",
      "Epoch 2 Step [1340/5072] Loss: 3.7501\n",
      "Epoch 2 Step [1360/5072] Loss: 3.7616\n",
      "Epoch 2 Step [1380/5072] Loss: 3.6797\n",
      "Epoch 2 Step [1400/5072] Loss: 3.5841\n",
      "Epoch 2 Step [1420/5072] Loss: 3.4806\n",
      "Epoch 2 Step [1440/5072] Loss: 3.6648\n",
      "Epoch 2 Step [1460/5072] Loss: 3.7836\n",
      "Epoch 2 Step [1480/5072] Loss: 3.5694\n",
      "Epoch 2 Step [1500/5072] Loss: 3.3241\n",
      "Epoch 2 Step [1520/5072] Loss: 3.5628\n",
      "Epoch 2 Step [1540/5072] Loss: 3.4920\n",
      "Epoch 2 Step [1560/5072] Loss: 3.6995\n",
      "Epoch 2 Step [1580/5072] Loss: 3.2907\n",
      "Epoch 2 Step [1600/5072] Loss: 3.7859\n",
      "Epoch 2 Step [1620/5072] Loss: 3.6031\n",
      "Epoch 2 Step [1640/5072] Loss: 3.5320\n",
      "Epoch 2 Step [1660/5072] Loss: 3.5076\n",
      "Epoch 2 Step [1680/5072] Loss: 3.7201\n",
      "Epoch 2 Step [1700/5072] Loss: 3.4796\n",
      "Epoch 2 Step [1720/5072] Loss: 3.6715\n",
      "Epoch 2 Step [1740/5072] Loss: 3.6841\n",
      "Epoch 2 Step [1760/5072] Loss: 3.5702\n",
      "Epoch 2 Step [1780/5072] Loss: 3.3682\n",
      "Epoch 2 Step [1800/5072] Loss: 3.6367\n",
      "Epoch 2 Step [1820/5072] Loss: 3.8762\n",
      "Epoch 2 Step [1840/5072] Loss: 3.7613\n",
      "Epoch 2 Step [1860/5072] Loss: 3.4057\n",
      "Epoch 2 Step [1880/5072] Loss: 3.8234\n",
      "Epoch 2 Step [1900/5072] Loss: 3.7380\n",
      "Epoch 2 Step [1920/5072] Loss: 3.7853\n",
      "Epoch 2 Step [1940/5072] Loss: 3.7504\n",
      "Epoch 2 Step [1960/5072] Loss: 3.4884\n",
      "Epoch 2 Step [1980/5072] Loss: 3.5089\n",
      "Epoch 2 Step [2000/5072] Loss: 3.7605\n",
      "Epoch 2 Step [2020/5072] Loss: 3.3514\n",
      "Epoch 2 Step [2040/5072] Loss: 3.6477\n",
      "Epoch 2 Step [2060/5072] Loss: 3.7586\n",
      "Epoch 2 Step [2080/5072] Loss: 3.5156\n",
      "Epoch 2 Step [2100/5072] Loss: 3.6195\n",
      "Epoch 2 Step [2120/5072] Loss: 3.9245\n",
      "Epoch 2 Step [2140/5072] Loss: 3.3844\n",
      "Epoch 2 Step [2160/5072] Loss: 3.7287\n",
      "Epoch 2 Step [2180/5072] Loss: 3.2650\n",
      "Epoch 2 Step [2200/5072] Loss: 3.8689\n",
      "Epoch 2 Step [2220/5072] Loss: 3.3248\n",
      "Epoch 2 Step [2240/5072] Loss: 3.4625\n",
      "Epoch 2 Step [2260/5072] Loss: 4.0281\n",
      "Epoch 2 Step [2280/5072] Loss: 3.6409\n",
      "Epoch 2 Step [2300/5072] Loss: 3.6912\n",
      "Epoch 2 Step [2320/5072] Loss: 3.6577\n",
      "Epoch 2 Step [2340/5072] Loss: 3.6236\n",
      "Epoch 2 Step [2360/5072] Loss: 3.8111\n",
      "Epoch 2 Step [2380/5072] Loss: 3.6858\n",
      "Epoch 2 Step [2400/5072] Loss: 3.5340\n",
      "Epoch 2 Step [2420/5072] Loss: 3.5724\n",
      "Epoch 2 Step [2440/5072] Loss: 3.6007\n",
      "Epoch 2 Step [2460/5072] Loss: 3.7845\n",
      "Epoch 2 Step [2480/5072] Loss: 3.7714\n",
      "Epoch 2 Step [2500/5072] Loss: 3.8563\n",
      "Epoch 2 Step [2520/5072] Loss: 3.4594\n",
      "Epoch 2 Step [2540/5072] Loss: 3.5079\n",
      "Epoch 2 Step [2560/5072] Loss: 3.6019\n",
      "Epoch 2 Step [2580/5072] Loss: 3.3907\n",
      "Epoch 2 Step [2600/5072] Loss: 3.5874\n",
      "Epoch 2 Step [2620/5072] Loss: 3.6116\n",
      "Epoch 2 Step [2640/5072] Loss: 3.7966\n",
      "Epoch 2 Step [2660/5072] Loss: 3.8611\n",
      "Epoch 2 Step [2680/5072] Loss: 3.5771\n",
      "Epoch 2 Step [2700/5072] Loss: 3.6544\n",
      "Epoch 2 Step [2720/5072] Loss: 3.5484\n",
      "Epoch 2 Step [2740/5072] Loss: 3.7300\n",
      "Epoch 2 Step [2760/5072] Loss: 3.5945\n",
      "Epoch 2 Step [2780/5072] Loss: 3.7538\n",
      "Epoch 2 Step [2800/5072] Loss: 3.5805\n",
      "Epoch 2 Step [2820/5072] Loss: 3.4842\n",
      "Epoch 2 Step [2840/5072] Loss: 3.6197\n",
      "Epoch 2 Step [2860/5072] Loss: 3.8105\n",
      "Epoch 2 Step [2880/5072] Loss: 3.7237\n",
      "Epoch 2 Step [2900/5072] Loss: 3.7547\n",
      "Epoch 2 Step [2920/5072] Loss: 3.6206\n",
      "Epoch 2 Step [2940/5072] Loss: 3.5655\n",
      "Epoch 2 Step [2960/5072] Loss: 3.5863\n",
      "Epoch 2 Step [2980/5072] Loss: 3.5507\n",
      "Epoch 2 Step [3000/5072] Loss: 3.4689\n",
      "Epoch 2 Step [3020/5072] Loss: 3.7246\n",
      "Epoch 2 Step [3040/5072] Loss: 3.7880\n",
      "Epoch 2 Step [3060/5072] Loss: 3.4786\n",
      "Epoch 2 Step [3080/5072] Loss: 3.7825\n",
      "Epoch 2 Step [3100/5072] Loss: 3.5496\n",
      "Epoch 2 Step [3120/5072] Loss: 3.8201\n",
      "Epoch 2 Step [3140/5072] Loss: 3.8212\n",
      "Epoch 2 Step [3160/5072] Loss: 3.6584\n",
      "Epoch 2 Step [3180/5072] Loss: 3.7358\n",
      "Epoch 2 Step [3200/5072] Loss: 3.7191\n",
      "Epoch 2 Step [3220/5072] Loss: 3.5432\n",
      "Epoch 2 Step [3240/5072] Loss: 3.5871\n",
      "Epoch 2 Step [3260/5072] Loss: 3.8224\n",
      "Epoch 2 Step [3280/5072] Loss: 3.5435\n",
      "Epoch 2 Step [3300/5072] Loss: 3.7071\n",
      "Epoch 2 Step [3320/5072] Loss: 3.6597\n",
      "Epoch 2 Step [3340/5072] Loss: 3.6243\n",
      "Epoch 2 Step [3360/5072] Loss: 3.5500\n",
      "Epoch 2 Step [3380/5072] Loss: 3.4361\n",
      "Epoch 2 Step [3400/5072] Loss: 3.4581\n",
      "Epoch 2 Step [3420/5072] Loss: 3.6665\n",
      "Epoch 2 Step [3440/5072] Loss: 3.3239\n",
      "Epoch 2 Step [3460/5072] Loss: 3.4733\n",
      "Epoch 2 Step [3480/5072] Loss: 3.3648\n",
      "Epoch 2 Step [3500/5072] Loss: 3.6679\n",
      "Epoch 2 Step [3520/5072] Loss: 3.5348\n",
      "Epoch 2 Step [3540/5072] Loss: 3.4907\n",
      "Epoch 2 Step [3560/5072] Loss: 3.7072\n",
      "Epoch 2 Step [3580/5072] Loss: 3.6721\n",
      "Epoch 2 Step [3600/5072] Loss: 3.8015\n",
      "Epoch 2 Step [3620/5072] Loss: 3.5827\n",
      "Epoch 2 Step [3640/5072] Loss: 3.6665\n",
      "Epoch 2 Step [3660/5072] Loss: 3.4296\n",
      "Epoch 2 Step [3680/5072] Loss: 3.6724\n",
      "Epoch 2 Step [3700/5072] Loss: 3.4750\n",
      "Epoch 2 Step [3720/5072] Loss: 3.4925\n",
      "Epoch 2 Step [3740/5072] Loss: 3.5811\n",
      "Epoch 2 Step [3760/5072] Loss: 3.5344\n",
      "Epoch 2 Step [3780/5072] Loss: 3.5155\n",
      "Epoch 2 Step [3800/5072] Loss: 3.7083\n",
      "Epoch 2 Step [3820/5072] Loss: 3.5841\n",
      "Epoch 2 Step [3840/5072] Loss: 3.7653\n",
      "Epoch 2 Step [3860/5072] Loss: 3.5835\n",
      "Epoch 2 Step [3880/5072] Loss: 3.8751\n",
      "Epoch 2 Step [3900/5072] Loss: 3.7472\n",
      "Epoch 2 Step [3920/5072] Loss: 3.7858\n",
      "Epoch 2 Step [3940/5072] Loss: 3.9809\n",
      "Epoch 2 Step [3960/5072] Loss: 3.6680\n",
      "Epoch 2 Step [3980/5072] Loss: 3.6315\n",
      "Epoch 2 Step [4000/5072] Loss: 3.4488\n",
      "Epoch 2 Step [4020/5072] Loss: 3.5915\n",
      "Epoch 2 Step [4040/5072] Loss: 3.1426\n",
      "Epoch 2 Step [4060/5072] Loss: 3.5806\n",
      "Epoch 2 Step [4080/5072] Loss: 3.6828\n",
      "Epoch 2 Step [4100/5072] Loss: 3.9430\n",
      "Epoch 2 Step [4120/5072] Loss: 3.6912\n",
      "Epoch 2 Step [4140/5072] Loss: 3.3211\n",
      "Epoch 2 Step [4160/5072] Loss: 3.7061\n",
      "Epoch 2 Step [4180/5072] Loss: 3.6798\n",
      "Epoch 2 Step [4200/5072] Loss: 3.6816\n",
      "Epoch 2 Step [4220/5072] Loss: 3.4010\n",
      "Epoch 2 Step [4240/5072] Loss: 3.3927\n",
      "Epoch 2 Step [4260/5072] Loss: 3.5019\n",
      "Epoch 2 Step [4280/5072] Loss: 3.4498\n",
      "Epoch 2 Step [4300/5072] Loss: 3.4941\n",
      "Epoch 2 Step [4320/5072] Loss: 3.4543\n",
      "Epoch 2 Step [4340/5072] Loss: 3.6856\n",
      "Epoch 2 Step [4360/5072] Loss: 3.8000\n",
      "Epoch 2 Step [4380/5072] Loss: 3.5977\n",
      "Epoch 2 Step [4400/5072] Loss: 3.4908\n",
      "Epoch 2 Step [4420/5072] Loss: 3.5102\n",
      "Epoch 2 Step [4440/5072] Loss: 3.7340\n",
      "Epoch 2 Step [4460/5072] Loss: 3.5485\n",
      "Epoch 2 Step [4480/5072] Loss: 3.6783\n",
      "Epoch 2 Step [4500/5072] Loss: 3.4915\n",
      "Epoch 2 Step [4520/5072] Loss: 3.6106\n",
      "Epoch 2 Step [4540/5072] Loss: 3.6559\n",
      "Epoch 2 Step [4560/5072] Loss: 3.3340\n",
      "Epoch 2 Step [4580/5072] Loss: 3.6184\n",
      "Epoch 2 Step [4600/5072] Loss: 3.5350\n",
      "Epoch 2 Step [4620/5072] Loss: 3.5108\n",
      "Epoch 2 Step [4640/5072] Loss: 3.5926\n",
      "Epoch 2 Step [4660/5072] Loss: 3.5907\n",
      "Epoch 2 Step [4680/5072] Loss: 3.8234\n",
      "Epoch 2 Step [4700/5072] Loss: 3.7277\n",
      "Epoch 2 Step [4720/5072] Loss: 3.7171\n",
      "Epoch 2 Step [4740/5072] Loss: 3.6365\n",
      "Epoch 2 Step [4760/5072] Loss: 3.4949\n",
      "Epoch 2 Step [4780/5072] Loss: 3.4635\n",
      "Epoch 2 Step [4800/5072] Loss: 3.9098\n",
      "Epoch 2 Step [4820/5072] Loss: 3.5691\n",
      "Epoch 2 Step [4840/5072] Loss: 3.6332\n",
      "Epoch 2 Step [4860/5072] Loss: 3.4186\n",
      "Epoch 2 Step [4880/5072] Loss: 3.4378\n",
      "Epoch 2 Step [4900/5072] Loss: 3.3191\n",
      "Epoch 2 Step [4920/5072] Loss: 3.6668\n",
      "Epoch 2 Step [4940/5072] Loss: 3.7613\n",
      "Epoch 2 Step [4960/5072] Loss: 3.4882\n",
      "Epoch 2 Step [4980/5072] Loss: 3.4636\n",
      "Epoch 2 Step [5000/5072] Loss: 3.8084\n",
      "Epoch 2 Step [5020/5072] Loss: 3.5044\n",
      "Epoch 2 Step [5040/5072] Loss: 3.4433\n",
      "Epoch 2 Step [5060/5072] Loss: 3.8221\n",
      "Epoch 2 Finished | Acc: 8.44% | Loss: 3.6237\n",
      "Epoch 3 Step [20/5072] Loss: 3.4492\n",
      "Epoch 3 Step [40/5072] Loss: 3.6416\n",
      "Epoch 3 Step [60/5072] Loss: 3.4795\n",
      "Epoch 3 Step [80/5072] Loss: 3.2327\n",
      "Epoch 3 Step [100/5072] Loss: 3.3894\n",
      "Epoch 3 Step [120/5072] Loss: 3.4930\n",
      "Epoch 3 Step [140/5072] Loss: 3.4462\n",
      "Epoch 3 Step [160/5072] Loss: 3.3435\n",
      "Epoch 3 Step [180/5072] Loss: 3.5833\n",
      "Epoch 3 Step [200/5072] Loss: 3.4575\n",
      "Epoch 3 Step [220/5072] Loss: 3.6398\n",
      "Epoch 3 Step [240/5072] Loss: 3.6909\n",
      "Epoch 3 Step [260/5072] Loss: 3.5217\n",
      "Epoch 3 Step [280/5072] Loss: 3.3528\n",
      "Epoch 3 Step [300/5072] Loss: 3.5150\n",
      "Epoch 3 Step [320/5072] Loss: 3.8873\n",
      "Epoch 3 Step [340/5072] Loss: 3.3902\n",
      "Epoch 3 Step [360/5072] Loss: 3.6557\n",
      "Epoch 3 Step [380/5072] Loss: 3.7853\n",
      "Epoch 3 Step [400/5072] Loss: 3.4020\n",
      "Epoch 3 Step [420/5072] Loss: 3.7395\n",
      "Epoch 3 Step [440/5072] Loss: 3.8222\n",
      "Epoch 3 Step [460/5072] Loss: 3.4659\n",
      "Epoch 3 Step [480/5072] Loss: 3.3231\n",
      "Epoch 3 Step [500/5072] Loss: 3.3546\n",
      "Epoch 3 Step [520/5072] Loss: 3.5922\n",
      "Epoch 3 Step [540/5072] Loss: 3.6458\n",
      "Epoch 3 Step [560/5072] Loss: 3.4443\n",
      "Epoch 3 Step [580/5072] Loss: 3.6863\n",
      "Epoch 3 Step [600/5072] Loss: 3.6011\n",
      "Epoch 3 Step [620/5072] Loss: 3.7685\n",
      "Epoch 3 Step [640/5072] Loss: 3.7701\n",
      "Epoch 3 Step [660/5072] Loss: 3.4266\n",
      "Epoch 3 Step [680/5072] Loss: 3.2780\n",
      "Epoch 3 Step [700/5072] Loss: 3.4477\n",
      "Epoch 3 Step [720/5072] Loss: 3.5862\n",
      "Epoch 3 Step [740/5072] Loss: 3.3571\n",
      "Epoch 3 Step [760/5072] Loss: 3.5040\n",
      "Epoch 3 Step [780/5072] Loss: 3.6109\n",
      "Epoch 3 Step [800/5072] Loss: 3.5149\n",
      "Epoch 3 Step [820/5072] Loss: 3.6985\n",
      "Epoch 3 Step [840/5072] Loss: 3.2955\n",
      "Epoch 3 Step [860/5072] Loss: 3.2357\n",
      "Epoch 3 Step [880/5072] Loss: 3.4222\n",
      "Epoch 3 Step [900/5072] Loss: 3.6450\n",
      "Epoch 3 Step [920/5072] Loss: 3.5406\n",
      "Epoch 3 Step [940/5072] Loss: 3.4880\n",
      "Epoch 3 Step [960/5072] Loss: 3.3995\n",
      "Epoch 3 Step [980/5072] Loss: 3.7758\n",
      "Epoch 3 Step [1000/5072] Loss: 3.5179\n",
      "Epoch 3 Step [1020/5072] Loss: 3.4824\n",
      "Epoch 3 Step [1040/5072] Loss: 3.4023\n",
      "Epoch 3 Step [1060/5072] Loss: 3.6371\n",
      "Epoch 3 Step [1080/5072] Loss: 3.4443\n",
      "Epoch 3 Step [1100/5072] Loss: 3.5392\n",
      "Epoch 3 Step [1120/5072] Loss: 3.5515\n",
      "Epoch 3 Step [1140/5072] Loss: 3.1090\n",
      "Epoch 3 Step [1160/5072] Loss: 3.7192\n",
      "Epoch 3 Step [1180/5072] Loss: 3.6041\n",
      "Epoch 3 Step [1200/5072] Loss: 3.6300\n",
      "Epoch 3 Step [1220/5072] Loss: 3.7561\n",
      "Epoch 3 Step [1240/5072] Loss: 3.7348\n",
      "Epoch 3 Step [1260/5072] Loss: 3.3710\n",
      "Epoch 3 Step [1280/5072] Loss: 3.5157\n",
      "Epoch 3 Step [1300/5072] Loss: 3.7420\n",
      "Epoch 3 Step [1320/5072] Loss: 3.4241\n",
      "Epoch 3 Step [1340/5072] Loss: 3.4322\n",
      "Epoch 3 Step [1360/5072] Loss: 3.4836\n",
      "Epoch 3 Step [1380/5072] Loss: 3.6772\n",
      "Epoch 3 Step [1400/5072] Loss: 3.7273\n",
      "Epoch 3 Step [1420/5072] Loss: 3.4697\n",
      "Epoch 3 Step [1440/5072] Loss: 3.5474\n",
      "Epoch 3 Step [1460/5072] Loss: 3.4937\n",
      "Epoch 3 Step [1480/5072] Loss: 3.7680\n",
      "Epoch 3 Step [1500/5072] Loss: 3.5044\n",
      "Epoch 3 Step [1520/5072] Loss: 3.4520\n",
      "Epoch 3 Step [1540/5072] Loss: 3.6661\n",
      "Epoch 3 Step [1560/5072] Loss: 3.7286\n",
      "Epoch 3 Step [1580/5072] Loss: 3.7313\n",
      "Epoch 3 Step [1600/5072] Loss: 3.9203\n",
      "Epoch 3 Step [1620/5072] Loss: 4.0789\n",
      "Epoch 3 Step [1640/5072] Loss: 3.5245\n",
      "Epoch 3 Step [1660/5072] Loss: 3.7071\n",
      "Epoch 3 Step [1680/5072] Loss: 3.5906\n",
      "Epoch 3 Step [1700/5072] Loss: 3.9113\n",
      "Epoch 3 Step [1720/5072] Loss: 3.4931\n",
      "Epoch 3 Step [1740/5072] Loss: 3.2886\n",
      "Epoch 3 Step [1760/5072] Loss: 3.8626\n",
      "Epoch 3 Step [1780/5072] Loss: 3.5368\n",
      "Epoch 3 Step [1800/5072] Loss: 3.7419\n",
      "Epoch 3 Step [1820/5072] Loss: 3.5507\n",
      "Epoch 3 Step [1840/5072] Loss: 3.4839\n",
      "Epoch 3 Step [1860/5072] Loss: 4.0067\n",
      "Epoch 3 Step [1880/5072] Loss: 3.4982\n",
      "Epoch 3 Step [1900/5072] Loss: 3.5265\n",
      "Epoch 3 Step [1920/5072] Loss: 3.7365\n",
      "Epoch 3 Step [1940/5072] Loss: 3.9414\n",
      "Epoch 3 Step [1960/5072] Loss: 3.9058\n",
      "Epoch 3 Step [1980/5072] Loss: 3.8035\n",
      "Epoch 3 Step [2000/5072] Loss: 3.7914\n",
      "Epoch 3 Step [2020/5072] Loss: 3.6087\n",
      "Epoch 3 Step [2040/5072] Loss: 3.7297\n",
      "Epoch 3 Step [2060/5072] Loss: 3.6115\n",
      "Epoch 3 Step [2080/5072] Loss: 3.5701\n",
      "Epoch 3 Step [2100/5072] Loss: 3.6585\n",
      "Epoch 3 Step [2120/5072] Loss: 3.8067\n",
      "Epoch 3 Step [2140/5072] Loss: 3.6570\n",
      "Epoch 3 Step [2160/5072] Loss: 3.3713\n",
      "Epoch 3 Step [2180/5072] Loss: 3.8118\n",
      "Epoch 3 Step [2200/5072] Loss: 3.6571\n",
      "Epoch 3 Step [2220/5072] Loss: 3.7568\n",
      "Epoch 3 Step [2240/5072] Loss: 3.4981\n",
      "Epoch 3 Step [2260/5072] Loss: 3.4674\n",
      "Epoch 3 Step [2280/5072] Loss: 3.3826\n",
      "Epoch 3 Step [2300/5072] Loss: 3.5510\n",
      "Epoch 3 Step [2320/5072] Loss: 3.4900\n",
      "Epoch 3 Step [2340/5072] Loss: 3.4832\n",
      "Epoch 3 Step [2360/5072] Loss: 3.5859\n",
      "Epoch 3 Step [2380/5072] Loss: 3.5668\n",
      "Epoch 3 Step [2400/5072] Loss: 3.5602\n",
      "Epoch 3 Step [2420/5072] Loss: 3.4751\n",
      "Epoch 3 Step [2440/5072] Loss: 3.7631\n",
      "Epoch 3 Step [2460/5072] Loss: 3.3096\n",
      "Epoch 3 Step [2480/5072] Loss: 3.5040\n",
      "Epoch 3 Step [2500/5072] Loss: 3.8209\n",
      "Epoch 3 Step [2520/5072] Loss: 3.8119\n",
      "Epoch 3 Step [2540/5072] Loss: 3.5744\n",
      "Epoch 3 Step [2560/5072] Loss: 3.5570\n",
      "Epoch 3 Step [2580/5072] Loss: 3.6372\n",
      "Epoch 3 Step [2600/5072] Loss: 3.9692\n",
      "Epoch 3 Step [2620/5072] Loss: 3.3845\n",
      "Epoch 3 Step [2640/5072] Loss: 3.5149\n",
      "Epoch 3 Step [2660/5072] Loss: 3.6966\n",
      "Epoch 3 Step [2680/5072] Loss: 3.5869\n",
      "Epoch 3 Step [2700/5072] Loss: 3.6906\n",
      "Epoch 3 Step [2720/5072] Loss: 3.5182\n",
      "Epoch 3 Step [2740/5072] Loss: 3.7460\n",
      "Epoch 3 Step [2760/5072] Loss: 3.8824\n",
      "Epoch 3 Step [2780/5072] Loss: 3.6390\n",
      "Epoch 3 Step [2800/5072] Loss: 3.7529\n",
      "Epoch 3 Step [2820/5072] Loss: 3.7504\n",
      "Epoch 3 Step [2840/5072] Loss: 3.6616\n",
      "Epoch 3 Step [2860/5072] Loss: 3.9988\n",
      "Epoch 3 Step [2880/5072] Loss: 3.6048\n",
      "Epoch 3 Step [2900/5072] Loss: 3.5201\n",
      "Epoch 3 Step [2920/5072] Loss: 3.6925\n",
      "Epoch 3 Step [2940/5072] Loss: 3.8193\n",
      "Epoch 3 Step [2960/5072] Loss: 3.6051\n",
      "Epoch 3 Step [2980/5072] Loss: 3.5681\n",
      "Epoch 3 Step [3000/5072] Loss: 3.4713\n",
      "Epoch 3 Step [3020/5072] Loss: 3.5031\n",
      "Epoch 3 Step [3040/5072] Loss: 3.5871\n",
      "Epoch 3 Step [3060/5072] Loss: 3.6003\n",
      "Epoch 3 Step [3080/5072] Loss: 3.4431\n",
      "Epoch 3 Step [3100/5072] Loss: 3.5201\n",
      "Epoch 3 Step [3120/5072] Loss: 3.4111\n",
      "Epoch 3 Step [3140/5072] Loss: 3.4770\n",
      "Epoch 3 Step [3160/5072] Loss: 3.5323\n",
      "Epoch 3 Step [3180/5072] Loss: 3.6812\n",
      "Epoch 3 Step [3200/5072] Loss: 3.7893\n",
      "Epoch 3 Step [3220/5072] Loss: 3.7537\n",
      "Epoch 3 Step [3240/5072] Loss: 3.3611\n",
      "Epoch 3 Step [3260/5072] Loss: 3.6170\n",
      "Epoch 3 Step [3280/5072] Loss: 3.4966\n",
      "Epoch 3 Step [3300/5072] Loss: 3.5771\n",
      "Epoch 3 Step [3320/5072] Loss: 3.5995\n",
      "Epoch 3 Step [3340/5072] Loss: 3.6156\n",
      "Epoch 3 Step [3360/5072] Loss: 3.1168\n",
      "Epoch 3 Step [3380/5072] Loss: 3.5018\n",
      "Epoch 3 Step [3400/5072] Loss: 3.3770\n",
      "Epoch 3 Step [3420/5072] Loss: 3.5304\n",
      "Epoch 3 Step [3440/5072] Loss: 3.3053\n",
      "Epoch 3 Step [3460/5072] Loss: 3.7320\n",
      "Epoch 3 Step [3480/5072] Loss: 3.4902\n",
      "Epoch 3 Step [3500/5072] Loss: 3.6839\n",
      "Epoch 3 Step [3520/5072] Loss: 3.2656\n",
      "Epoch 3 Step [3540/5072] Loss: 3.3813\n",
      "Epoch 3 Step [3560/5072] Loss: 3.6873\n",
      "Epoch 3 Step [3580/5072] Loss: 3.7775\n",
      "Epoch 3 Step [3600/5072] Loss: 3.8400\n",
      "Epoch 3 Step [3620/5072] Loss: 3.4205\n",
      "Epoch 3 Step [3640/5072] Loss: 3.4040\n",
      "Epoch 3 Step [3660/5072] Loss: 3.7485\n",
      "Epoch 3 Step [3680/5072] Loss: 3.4570\n",
      "Epoch 3 Step [3700/5072] Loss: 3.7019\n",
      "Epoch 3 Step [3720/5072] Loss: 3.6338\n",
      "Epoch 3 Step [3740/5072] Loss: 3.4475\n",
      "Epoch 3 Step [3760/5072] Loss: 3.6844\n",
      "Epoch 3 Step [3780/5072] Loss: 3.5276\n",
      "Epoch 3 Step [3800/5072] Loss: 3.3952\n",
      "Epoch 3 Step [3820/5072] Loss: 3.5937\n",
      "Epoch 3 Step [3840/5072] Loss: 3.9516\n",
      "Epoch 3 Step [3860/5072] Loss: 3.4430\n",
      "Epoch 3 Step [3880/5072] Loss: 3.4497\n",
      "Epoch 3 Step [3900/5072] Loss: 3.6133\n",
      "Epoch 3 Step [3920/5072] Loss: 3.4702\n",
      "Epoch 3 Step [3940/5072] Loss: 3.5884\n",
      "Epoch 3 Step [3960/5072] Loss: 3.4292\n",
      "Epoch 3 Step [3980/5072] Loss: 3.4327\n",
      "Epoch 3 Step [4000/5072] Loss: 3.9518\n",
      "Epoch 3 Step [4020/5072] Loss: 3.5038\n",
      "Epoch 3 Step [4040/5072] Loss: 3.6979\n",
      "Epoch 3 Step [4060/5072] Loss: 3.5850\n",
      "Epoch 3 Step [4080/5072] Loss: 3.5808\n",
      "Epoch 3 Step [4100/5072] Loss: 3.5965\n",
      "Epoch 3 Step [4120/5072] Loss: 3.5078\n",
      "Epoch 3 Step [4140/5072] Loss: 3.6897\n",
      "Epoch 3 Step [4160/5072] Loss: 3.5214\n",
      "Epoch 3 Step [4180/5072] Loss: 3.6836\n",
      "Epoch 3 Step [4200/5072] Loss: 3.5236\n",
      "Epoch 3 Step [4220/5072] Loss: 3.5708\n",
      "Epoch 3 Step [4240/5072] Loss: 3.7514\n",
      "Epoch 3 Step [4260/5072] Loss: 3.4963\n",
      "Epoch 3 Step [4280/5072] Loss: 3.5225\n",
      "Epoch 3 Step [4300/5072] Loss: 3.5806\n",
      "Epoch 3 Step [4320/5072] Loss: 3.5092\n",
      "Epoch 3 Step [4340/5072] Loss: 3.8733\n",
      "Epoch 3 Step [4360/5072] Loss: 3.6657\n",
      "Epoch 3 Step [4380/5072] Loss: 3.9257\n",
      "Epoch 3 Step [4400/5072] Loss: 3.6875\n",
      "Epoch 3 Step [4420/5072] Loss: 3.5781\n",
      "Epoch 3 Step [4440/5072] Loss: 3.4730\n",
      "Epoch 3 Step [4460/5072] Loss: 3.5358\n",
      "Epoch 3 Step [4480/5072] Loss: 3.5820\n",
      "Epoch 3 Step [4500/5072] Loss: 3.4558\n",
      "Epoch 3 Step [4520/5072] Loss: 3.5641\n",
      "Epoch 3 Step [4540/5072] Loss: 3.4381\n",
      "Epoch 3 Step [4560/5072] Loss: 3.1447\n",
      "Epoch 3 Step [4580/5072] Loss: 3.8327\n",
      "Epoch 3 Step [4600/5072] Loss: 3.6995\n",
      "Epoch 3 Step [4620/5072] Loss: 3.3257\n",
      "Epoch 3 Step [4640/5072] Loss: 3.6639\n",
      "Epoch 3 Step [4660/5072] Loss: 3.5372\n",
      "Epoch 3 Step [4680/5072] Loss: 3.5851\n",
      "Epoch 3 Step [4700/5072] Loss: 3.7779\n",
      "Epoch 3 Step [4720/5072] Loss: 3.5915\n",
      "Epoch 3 Step [4740/5072] Loss: 3.6675\n",
      "Epoch 3 Step [4760/5072] Loss: 3.1961\n",
      "Epoch 3 Step [4780/5072] Loss: 3.7729\n",
      "Epoch 3 Step [4800/5072] Loss: 3.5684\n",
      "Epoch 3 Step [4820/5072] Loss: 3.5332\n",
      "Epoch 3 Step [4840/5072] Loss: 3.4237\n",
      "Epoch 3 Step [4860/5072] Loss: 3.6319\n",
      "Epoch 3 Step [4880/5072] Loss: 3.4505\n",
      "Epoch 3 Step [4900/5072] Loss: 3.3239\n",
      "Epoch 3 Step [4920/5072] Loss: 3.6219\n",
      "Epoch 3 Step [4940/5072] Loss: 3.6277\n",
      "Epoch 3 Step [4960/5072] Loss: 3.4547\n",
      "Epoch 3 Step [4980/5072] Loss: 4.0473\n",
      "Epoch 3 Step [5000/5072] Loss: 3.5532\n",
      "Epoch 3 Step [5020/5072] Loss: 3.7210\n",
      "Epoch 3 Step [5040/5072] Loss: 3.6303\n",
      "Epoch 3 Step [5060/5072] Loss: 3.4922\n",
      "Epoch 3 Finished | Acc: 8.83% | Loss: 3.5833\n",
      "Epoch 4 Step [20/5072] Loss: 3.8879\n",
      "Epoch 4 Step [40/5072] Loss: 3.5201\n",
      "Epoch 4 Step [60/5072] Loss: 3.6035\n",
      "Epoch 4 Step [80/5072] Loss: 3.1947\n",
      "Epoch 4 Step [100/5072] Loss: 3.7871\n",
      "Epoch 4 Step [120/5072] Loss: 3.7129\n",
      "Epoch 4 Step [140/5072] Loss: 3.6938\n",
      "Epoch 4 Step [160/5072] Loss: 3.9476\n",
      "Epoch 4 Step [180/5072] Loss: 3.6293\n",
      "Epoch 4 Step [200/5072] Loss: 3.4683\n",
      "Epoch 4 Step [220/5072] Loss: 3.6926\n",
      "Epoch 4 Step [240/5072] Loss: 3.5601\n",
      "Epoch 4 Step [260/5072] Loss: 3.5267\n",
      "Epoch 4 Step [280/5072] Loss: 3.7638\n",
      "Epoch 4 Step [300/5072] Loss: 3.6106\n",
      "Epoch 4 Step [320/5072] Loss: 3.4244\n",
      "Epoch 4 Step [340/5072] Loss: 3.6566\n",
      "Epoch 4 Step [360/5072] Loss: 3.4155\n",
      "Epoch 4 Step [380/5072] Loss: 3.5269\n",
      "Epoch 4 Step [400/5072] Loss: 3.7475\n",
      "Epoch 4 Step [420/5072] Loss: 3.8853\n",
      "Epoch 4 Step [440/5072] Loss: 3.8895\n",
      "Epoch 4 Step [460/5072] Loss: 3.9349\n",
      "Epoch 4 Step [480/5072] Loss: 4.1421\n",
      "Epoch 4 Step [500/5072] Loss: 3.8387\n",
      "Epoch 4 Step [520/5072] Loss: 3.6576\n",
      "Epoch 4 Step [540/5072] Loss: 3.7920\n",
      "Epoch 4 Step [560/5072] Loss: 3.6541\n",
      "Epoch 4 Step [580/5072] Loss: 3.7712\n",
      "Epoch 4 Step [600/5072] Loss: 3.6970\n",
      "Epoch 4 Step [620/5072] Loss: 3.5231\n",
      "Epoch 4 Step [640/5072] Loss: 3.6827\n",
      "Epoch 4 Step [660/5072] Loss: 3.6165\n",
      "Epoch 4 Step [680/5072] Loss: 3.4290\n",
      "Epoch 4 Step [700/5072] Loss: 3.7883\n",
      "Epoch 4 Step [720/5072] Loss: 3.6258\n",
      "Epoch 4 Step [740/5072] Loss: 3.9837\n",
      "Epoch 4 Step [760/5072] Loss: 3.7714\n",
      "Epoch 4 Step [780/5072] Loss: 3.8978\n",
      "Epoch 4 Step [800/5072] Loss: 3.6550\n",
      "Epoch 4 Step [820/5072] Loss: 4.2855\n",
      "Epoch 4 Step [840/5072] Loss: 3.9929\n",
      "Epoch 4 Step [860/5072] Loss: 3.4815\n",
      "Epoch 4 Step [880/5072] Loss: 3.9303\n",
      "Epoch 4 Step [900/5072] Loss: 3.8046\n",
      "Epoch 4 Step [920/5072] Loss: 3.8027\n",
      "Epoch 4 Step [940/5072] Loss: 3.6346\n",
      "Epoch 4 Step [960/5072] Loss: 3.6190\n",
      "Epoch 4 Step [980/5072] Loss: 3.9164\n",
      "Epoch 4 Step [1000/5072] Loss: 3.7072\n",
      "Epoch 4 Step [1020/5072] Loss: 3.9687\n",
      "Epoch 4 Step [1040/5072] Loss: 4.0564\n",
      "Epoch 4 Step [1060/5072] Loss: 3.8240\n",
      "Epoch 4 Step [1080/5072] Loss: 3.8215\n",
      "Epoch 4 Step [1100/5072] Loss: 3.8349\n",
      "Epoch 4 Step [1120/5072] Loss: 3.8196\n",
      "Epoch 4 Step [1140/5072] Loss: 3.6997\n",
      "Epoch 4 Step [1160/5072] Loss: 3.7447\n",
      "Epoch 4 Step [1180/5072] Loss: 4.0033\n",
      "Epoch 4 Step [1200/5072] Loss: 3.9811\n",
      "Epoch 4 Step [1220/5072] Loss: 3.7543\n",
      "Epoch 4 Step [1240/5072] Loss: 4.0094\n",
      "Epoch 4 Step [1260/5072] Loss: 3.9155\n",
      "Epoch 4 Step [1280/5072] Loss: 3.8997\n",
      "Epoch 4 Step [1300/5072] Loss: 4.0197\n",
      "Epoch 4 Step [1320/5072] Loss: 3.8717\n",
      "Epoch 4 Step [1340/5072] Loss: 3.8034\n",
      "Epoch 4 Step [1360/5072] Loss: 3.7485\n",
      "Epoch 4 Step [1380/5072] Loss: 3.8594\n",
      "Epoch 4 Step [1400/5072] Loss: 3.7898\n",
      "Epoch 4 Step [1420/5072] Loss: 3.7543\n",
      "Epoch 4 Step [1440/5072] Loss: 3.9265\n",
      "Epoch 4 Step [1460/5072] Loss: 3.8293\n",
      "Epoch 4 Step [1480/5072] Loss: 3.7860\n",
      "Epoch 4 Step [1500/5072] Loss: 3.8853\n",
      "Epoch 4 Step [1520/5072] Loss: 3.7477\n",
      "Epoch 4 Step [1540/5072] Loss: 3.8453\n",
      "Epoch 4 Step [1560/5072] Loss: 3.6415\n",
      "Epoch 4 Step [1580/5072] Loss: 3.7197\n",
      "Epoch 4 Step [1600/5072] Loss: 3.7629\n",
      "Epoch 4 Step [1620/5072] Loss: 3.7831\n",
      "Epoch 4 Step [1640/5072] Loss: 3.5986\n",
      "Epoch 4 Step [1660/5072] Loss: 3.6892\n",
      "Epoch 4 Step [1680/5072] Loss: 3.7115\n",
      "Epoch 4 Step [1700/5072] Loss: 4.0416\n",
      "Epoch 4 Step [1720/5072] Loss: 3.7405\n",
      "Epoch 4 Step [1740/5072] Loss: 3.7313\n",
      "Epoch 4 Step [1760/5072] Loss: 3.6747\n",
      "Epoch 4 Step [1780/5072] Loss: 4.0200\n",
      "Epoch 4 Step [1800/5072] Loss: 4.0194\n",
      "Epoch 4 Step [1820/5072] Loss: 4.0363\n",
      "Epoch 4 Step [1840/5072] Loss: 3.8079\n",
      "Epoch 4 Step [1860/5072] Loss: 3.9989\n",
      "Epoch 4 Step [1880/5072] Loss: 3.9186\n",
      "Epoch 4 Step [1900/5072] Loss: 3.8588\n",
      "Epoch 4 Step [1920/5072] Loss: 3.8603\n",
      "Epoch 4 Step [1940/5072] Loss: 3.8776\n",
      "Epoch 4 Step [1960/5072] Loss: 3.7245\n",
      "Epoch 4 Step [1980/5072] Loss: 3.7363\n",
      "Epoch 4 Step [2000/5072] Loss: 3.7849\n",
      "Epoch 4 Step [2020/5072] Loss: 3.7874\n",
      "Epoch 4 Step [2040/5072] Loss: 3.7132\n",
      "Epoch 4 Step [2060/5072] Loss: 3.5129\n",
      "Epoch 4 Step [2080/5072] Loss: 3.7652\n",
      "Epoch 4 Step [2100/5072] Loss: 3.5801\n",
      "Epoch 4 Step [2120/5072] Loss: 3.8253\n",
      "Epoch 4 Step [2140/5072] Loss: 3.6569\n",
      "Epoch 4 Step [2160/5072] Loss: 3.6769\n",
      "Epoch 4 Step [2180/5072] Loss: 3.8790\n",
      "Epoch 4 Step [2200/5072] Loss: 3.6163\n",
      "Epoch 4 Step [2220/5072] Loss: 3.7063\n",
      "Epoch 4 Step [2240/5072] Loss: 3.6190\n",
      "Epoch 4 Step [2260/5072] Loss: 3.6403\n",
      "Epoch 4 Step [2280/5072] Loss: 3.7674\n",
      "Epoch 4 Step [2300/5072] Loss: 3.8629\n",
      "Epoch 4 Step [2320/5072] Loss: 3.5408\n",
      "Epoch 4 Step [2340/5072] Loss: 3.6245\n",
      "Epoch 4 Step [2360/5072] Loss: 3.9624\n",
      "Epoch 4 Step [2380/5072] Loss: 3.7394\n",
      "Epoch 4 Step [2400/5072] Loss: 3.7942\n",
      "Epoch 4 Step [2420/5072] Loss: 3.6771\n",
      "Epoch 4 Step [2440/5072] Loss: 3.6950\n",
      "Epoch 4 Step [2460/5072] Loss: 3.5491\n",
      "Epoch 4 Step [2480/5072] Loss: 3.8342\n",
      "Epoch 4 Step [2500/5072] Loss: 3.7156\n",
      "Epoch 4 Step [2520/5072] Loss: 3.7337\n",
      "Epoch 4 Step [2540/5072] Loss: 3.8415\n",
      "Epoch 4 Step [2560/5072] Loss: 3.6761\n",
      "Epoch 4 Step [2580/5072] Loss: 3.7833\n",
      "Epoch 4 Step [2600/5072] Loss: 3.9100\n",
      "Epoch 4 Step [2620/5072] Loss: 3.7188\n",
      "Epoch 4 Step [2640/5072] Loss: 3.7916\n",
      "Epoch 4 Step [2660/5072] Loss: 3.6388\n",
      "Epoch 4 Step [2680/5072] Loss: 3.8419\n",
      "Epoch 4 Step [2700/5072] Loss: 3.7763\n",
      "Epoch 4 Step [2720/5072] Loss: 3.6133\n",
      "Epoch 4 Step [2740/5072] Loss: 4.0309\n",
      "Epoch 4 Step [2760/5072] Loss: 3.8216\n",
      "Epoch 4 Step [2780/5072] Loss: 3.7454\n",
      "Epoch 4 Step [2800/5072] Loss: 3.9926\n",
      "Epoch 4 Step [2820/5072] Loss: 3.8747\n",
      "Epoch 4 Step [2840/5072] Loss: 3.6208\n",
      "Epoch 4 Step [2860/5072] Loss: 3.7973\n",
      "Epoch 4 Step [2880/5072] Loss: 3.8133\n",
      "Epoch 4 Step [2900/5072] Loss: 3.8075\n",
      "Epoch 4 Step [2920/5072] Loss: 3.8568\n",
      "Epoch 4 Step [2940/5072] Loss: 3.5732\n",
      "Epoch 4 Step [2960/5072] Loss: 3.9818\n",
      "Epoch 4 Step [2980/5072] Loss: 3.9066\n",
      "Epoch 4 Step [3000/5072] Loss: 3.6483\n",
      "Epoch 4 Step [3020/5072] Loss: 3.9325\n",
      "Epoch 4 Step [3040/5072] Loss: 3.8415\n",
      "Epoch 4 Step [3060/5072] Loss: 3.7115\n",
      "Epoch 4 Step [3080/5072] Loss: 3.6297\n",
      "Epoch 4 Step [3100/5072] Loss: 3.8775\n",
      "Epoch 4 Step [3120/5072] Loss: 3.7189\n",
      "Epoch 4 Step [3140/5072] Loss: 3.9179\n",
      "Epoch 4 Step [3160/5072] Loss: 3.8004\n",
      "Epoch 4 Step [3180/5072] Loss: 3.8656\n",
      "Epoch 4 Step [3200/5072] Loss: 3.7623\n",
      "Epoch 4 Step [3220/5072] Loss: 3.7097\n",
      "Epoch 4 Step [3240/5072] Loss: 3.7570\n",
      "Epoch 4 Step [3260/5072] Loss: 3.8406\n",
      "Epoch 4 Step [3280/5072] Loss: 3.9195\n",
      "Epoch 4 Step [3300/5072] Loss: 3.7036\n",
      "Epoch 4 Step [3320/5072] Loss: 3.8992\n",
      "Epoch 4 Step [3340/5072] Loss: 3.9529\n",
      "Epoch 4 Step [3360/5072] Loss: 3.7037\n",
      "Epoch 4 Step [3380/5072] Loss: 3.8611\n",
      "Epoch 4 Step [3400/5072] Loss: 3.8540\n",
      "Epoch 4 Step [3420/5072] Loss: 3.8889\n",
      "Epoch 4 Step [3440/5072] Loss: 4.0060\n",
      "Epoch 4 Step [3460/5072] Loss: 3.7502\n",
      "Epoch 4 Step [3480/5072] Loss: 3.5486\n",
      "Epoch 4 Step [3500/5072] Loss: 3.7405\n",
      "Epoch 4 Step [3520/5072] Loss: 3.5159\n",
      "Epoch 4 Step [3540/5072] Loss: 4.0144\n",
      "Epoch 4 Step [3560/5072] Loss: 3.7266\n",
      "Epoch 4 Step [3580/5072] Loss: 3.8078\n",
      "Epoch 4 Step [3600/5072] Loss: 3.8391\n",
      "Epoch 4 Step [3620/5072] Loss: 3.7743\n",
      "Epoch 4 Step [3640/5072] Loss: 4.0464\n",
      "Epoch 4 Step [3660/5072] Loss: 3.9646\n",
      "Epoch 4 Step [3680/5072] Loss: 3.7712\n",
      "Epoch 4 Step [3700/5072] Loss: 3.7448\n",
      "Epoch 4 Step [3720/5072] Loss: 3.7248\n",
      "Epoch 4 Step [3740/5072] Loss: 3.6620\n",
      "Epoch 4 Step [3760/5072] Loss: 3.7475\n",
      "Epoch 4 Step [3780/5072] Loss: 3.8535\n",
      "Epoch 4 Step [3800/5072] Loss: 3.8963\n",
      "Epoch 4 Step [3820/5072] Loss: 3.8824\n",
      "Epoch 4 Step [3840/5072] Loss: 3.6787\n",
      "Epoch 4 Step [3860/5072] Loss: 3.6642\n",
      "Epoch 4 Step [3880/5072] Loss: 3.7067\n",
      "Epoch 4 Step [3900/5072] Loss: 3.6460\n",
      "Epoch 4 Step [3920/5072] Loss: 3.8842\n",
      "Epoch 4 Step [3940/5072] Loss: 3.9736\n",
      "Epoch 4 Step [3960/5072] Loss: 3.6762\n",
      "Epoch 4 Step [3980/5072] Loss: 3.7485\n",
      "Epoch 4 Step [4000/5072] Loss: 3.6918\n",
      "Epoch 4 Step [4020/5072] Loss: 3.8516\n",
      "Epoch 4 Step [4040/5072] Loss: 3.9491\n",
      "Epoch 4 Step [4060/5072] Loss: 4.0097\n",
      "Epoch 4 Step [4080/5072] Loss: 4.0698\n",
      "Epoch 4 Step [4100/5072] Loss: 4.0263\n",
      "Epoch 4 Step [4120/5072] Loss: 3.7216\n",
      "Epoch 4 Step [4140/5072] Loss: 3.5861\n",
      "Epoch 4 Step [4160/5072] Loss: 3.5016\n",
      "Epoch 4 Step [4180/5072] Loss: 4.0019\n",
      "Epoch 4 Step [4200/5072] Loss: 3.8118\n",
      "Epoch 4 Step [4220/5072] Loss: 3.6676\n",
      "Epoch 4 Step [4240/5072] Loss: 3.7445\n",
      "Epoch 4 Step [4260/5072] Loss: 4.0583\n",
      "Epoch 4 Step [4280/5072] Loss: 3.8970\n",
      "Epoch 4 Step [4300/5072] Loss: 3.8273\n",
      "Epoch 4 Step [4320/5072] Loss: 3.7732\n",
      "Epoch 4 Step [4340/5072] Loss: 3.6729\n",
      "Epoch 4 Step [4360/5072] Loss: 3.8721\n",
      "Epoch 4 Step [4380/5072] Loss: 3.7722\n",
      "Epoch 4 Step [4400/5072] Loss: 3.9848\n",
      "Epoch 4 Step [4420/5072] Loss: 3.9193\n",
      "Epoch 4 Step [4440/5072] Loss: 3.7671\n",
      "Epoch 4 Step [4460/5072] Loss: 3.7804\n",
      "Epoch 4 Step [4480/5072] Loss: 4.0026\n",
      "Epoch 4 Step [4500/5072] Loss: 3.6775\n",
      "Epoch 4 Step [4520/5072] Loss: 3.7420\n",
      "Epoch 4 Step [4540/5072] Loss: 3.7338\n",
      "Epoch 4 Step [4560/5072] Loss: 3.8120\n",
      "Epoch 4 Step [4580/5072] Loss: 3.7919\n",
      "Epoch 4 Step [4600/5072] Loss: 3.7130\n",
      "Epoch 4 Step [4620/5072] Loss: 3.6828\n",
      "Epoch 4 Step [4640/5072] Loss: 3.8607\n",
      "Epoch 4 Step [4660/5072] Loss: 3.6794\n",
      "Epoch 4 Step [4680/5072] Loss: 3.8995\n",
      "Epoch 4 Step [4700/5072] Loss: 3.7934\n",
      "Epoch 4 Step [4720/5072] Loss: 3.8077\n",
      "Epoch 4 Step [4740/5072] Loss: 3.5808\n",
      "Epoch 4 Step [4760/5072] Loss: 3.8676\n",
      "Epoch 4 Step [4780/5072] Loss: 3.6385\n",
      "Epoch 4 Step [4800/5072] Loss: 3.5878\n",
      "Epoch 4 Step [4820/5072] Loss: 3.7992\n",
      "Epoch 4 Step [4840/5072] Loss: 3.7597\n",
      "Epoch 4 Step [4860/5072] Loss: 3.8233\n",
      "Epoch 4 Step [4880/5072] Loss: 3.7680\n",
      "Epoch 4 Step [4900/5072] Loss: 3.7485\n",
      "Epoch 4 Step [4920/5072] Loss: 3.7170\n",
      "Epoch 4 Step [4940/5072] Loss: 3.8867\n",
      "Epoch 4 Step [4960/5072] Loss: 3.6474\n",
      "Epoch 4 Step [4980/5072] Loss: 3.7791\n",
      "Epoch 4 Step [5000/5072] Loss: 3.9734\n",
      "Epoch 4 Step [5020/5072] Loss: 3.8120\n",
      "Epoch 4 Step [5040/5072] Loss: 3.9841\n",
      "Epoch 4 Step [5060/5072] Loss: 3.9054\n",
      "Epoch 4 Finished | Acc: 7.45% | Loss: 3.7821\n",
      "Epoch 5 Step [20/5072] Loss: 3.8719\n",
      "Epoch 5 Step [40/5072] Loss: 3.9157\n",
      "Epoch 5 Step [60/5072] Loss: 3.3619\n",
      "Epoch 5 Step [80/5072] Loss: 3.8296\n",
      "Epoch 5 Step [100/5072] Loss: 3.7773\n",
      "Epoch 5 Step [120/5072] Loss: 3.7727\n",
      "Epoch 5 Step [140/5072] Loss: 3.6725\n",
      "Epoch 5 Step [160/5072] Loss: 3.7945\n",
      "Epoch 5 Step [180/5072] Loss: 3.8080\n",
      "Epoch 5 Step [200/5072] Loss: 3.6845\n",
      "Epoch 5 Step [220/5072] Loss: 3.7955\n",
      "Epoch 5 Step [240/5072] Loss: 3.7695\n",
      "Epoch 5 Step [260/5072] Loss: 3.8504\n",
      "Epoch 5 Step [280/5072] Loss: 3.9251\n",
      "Epoch 5 Step [300/5072] Loss: 3.6745\n",
      "Epoch 5 Step [320/5072] Loss: 3.6250\n",
      "Epoch 5 Step [340/5072] Loss: 3.7495\n",
      "Epoch 5 Step [360/5072] Loss: 3.9569\n",
      "Epoch 5 Step [380/5072] Loss: 3.7952\n",
      "Epoch 5 Step [400/5072] Loss: 3.8874\n",
      "Epoch 5 Step [420/5072] Loss: 3.7770\n",
      "Epoch 5 Step [440/5072] Loss: 3.7811\n",
      "Epoch 5 Step [460/5072] Loss: 3.6613\n",
      "Epoch 5 Step [480/5072] Loss: 3.8876\n",
      "Epoch 5 Step [500/5072] Loss: 3.7422\n",
      "Epoch 5 Step [520/5072] Loss: 4.0223\n",
      "Epoch 5 Step [540/5072] Loss: 3.9747\n",
      "Epoch 5 Step [560/5072] Loss: 3.9800\n",
      "Epoch 5 Step [580/5072] Loss: 3.8119\n",
      "Epoch 5 Step [600/5072] Loss: 3.8143\n",
      "Epoch 5 Step [620/5072] Loss: 4.0355\n",
      "Epoch 5 Step [640/5072] Loss: 3.9655\n",
      "Epoch 5 Step [660/5072] Loss: 3.7572\n",
      "Epoch 5 Step [680/5072] Loss: 3.7735\n",
      "Epoch 5 Step [700/5072] Loss: 3.7544\n",
      "Epoch 5 Step [720/5072] Loss: 3.7114\n",
      "Epoch 5 Step [740/5072] Loss: 3.5145\n",
      "Epoch 5 Step [760/5072] Loss: 3.8499\n",
      "Epoch 5 Step [780/5072] Loss: 3.7086\n",
      "Epoch 5 Step [800/5072] Loss: 3.8028\n",
      "Epoch 5 Step [820/5072] Loss: 3.8478\n",
      "Epoch 5 Step [840/5072] Loss: 3.8508\n",
      "Epoch 5 Step [860/5072] Loss: 3.6797\n",
      "Epoch 5 Step [880/5072] Loss: 3.5838\n",
      "Epoch 5 Step [900/5072] Loss: 3.8306\n",
      "Epoch 5 Step [920/5072] Loss: 3.7980\n",
      "Epoch 5 Step [940/5072] Loss: 3.7818\n",
      "Epoch 5 Step [960/5072] Loss: 3.8752\n",
      "Epoch 5 Step [980/5072] Loss: 3.7561\n",
      "Epoch 5 Step [1000/5072] Loss: 3.5459\n",
      "Epoch 5 Step [1020/5072] Loss: 3.8847\n",
      "Epoch 5 Step [1040/5072] Loss: 3.9561\n",
      "Epoch 5 Step [1060/5072] Loss: 3.5953\n",
      "Epoch 5 Step [1080/5072] Loss: 3.7387\n",
      "Epoch 5 Step [1100/5072] Loss: 3.5687\n",
      "Epoch 5 Step [1120/5072] Loss: 3.6386\n",
      "Epoch 5 Step [1140/5072] Loss: 3.4378\n",
      "Epoch 5 Step [1160/5072] Loss: 3.8666\n",
      "Epoch 5 Step [1180/5072] Loss: 3.8336\n",
      "Epoch 5 Step [1200/5072] Loss: 3.6110\n",
      "Epoch 5 Step [1220/5072] Loss: 3.5604\n",
      "Epoch 5 Step [1240/5072] Loss: 3.7651\n",
      "Epoch 5 Step [1260/5072] Loss: 3.8050\n",
      "Epoch 5 Step [1280/5072] Loss: 3.5827\n",
      "Epoch 5 Step [1300/5072] Loss: 3.6447\n",
      "Epoch 5 Step [1320/5072] Loss: 3.8609\n",
      "Epoch 5 Step [1340/5072] Loss: 3.4917\n",
      "Epoch 5 Step [1360/5072] Loss: 3.9082\n",
      "Epoch 5 Step [1380/5072] Loss: 3.7505\n",
      "Epoch 5 Step [1400/5072] Loss: 3.5676\n",
      "Epoch 5 Step [1420/5072] Loss: 3.7480\n",
      "Epoch 5 Step [1440/5072] Loss: 3.7243\n",
      "Epoch 5 Step [1460/5072] Loss: 3.5464\n",
      "Epoch 5 Step [1480/5072] Loss: 3.7395\n",
      "Epoch 5 Step [1500/5072] Loss: 3.8134\n",
      "Epoch 5 Step [1520/5072] Loss: 3.5270\n",
      "Epoch 5 Step [1540/5072] Loss: 3.6812\n",
      "Epoch 5 Step [1560/5072] Loss: 3.5877\n",
      "Epoch 5 Step [1580/5072] Loss: 3.7157\n",
      "Epoch 5 Step [1600/5072] Loss: 3.9154\n",
      "Epoch 5 Step [1620/5072] Loss: 3.7488\n",
      "Epoch 5 Step [1640/5072] Loss: 3.6674\n",
      "Epoch 5 Step [1660/5072] Loss: 3.6160\n",
      "Epoch 5 Step [1680/5072] Loss: 3.5871\n",
      "Epoch 5 Step [1700/5072] Loss: 3.8231\n",
      "Epoch 5 Step [1720/5072] Loss: 3.5478\n",
      "Epoch 5 Step [1740/5072] Loss: 3.9592\n",
      "Epoch 5 Step [1760/5072] Loss: 3.8360\n",
      "Epoch 5 Step [1780/5072] Loss: 3.7058\n",
      "Epoch 5 Step [1800/5072] Loss: 3.7984\n",
      "Epoch 5 Step [1820/5072] Loss: 3.6881\n",
      "Epoch 5 Step [1840/5072] Loss: 3.7772\n",
      "Epoch 5 Step [1860/5072] Loss: 3.6098\n",
      "Epoch 5 Step [1880/5072] Loss: 3.7491\n",
      "Epoch 5 Step [1900/5072] Loss: 3.6371\n",
      "Epoch 5 Step [1920/5072] Loss: 3.6630\n",
      "Epoch 5 Step [1940/5072] Loss: 3.6916\n",
      "Epoch 5 Step [1960/5072] Loss: 3.8698\n",
      "Epoch 5 Step [1980/5072] Loss: 3.8709\n",
      "Epoch 5 Step [2000/5072] Loss: 3.5646\n",
      "Epoch 5 Step [2020/5072] Loss: 3.5594\n",
      "Epoch 5 Step [2040/5072] Loss: 3.7278\n",
      "Epoch 5 Step [2060/5072] Loss: 3.8564\n",
      "Epoch 5 Step [2080/5072] Loss: 3.5700\n",
      "Epoch 5 Step [2100/5072] Loss: 3.8190\n",
      "Epoch 5 Step [2120/5072] Loss: 3.8138\n",
      "Epoch 5 Step [2140/5072] Loss: 3.8552\n",
      "Epoch 5 Step [2160/5072] Loss: 3.6633\n",
      "Epoch 5 Step [2180/5072] Loss: 3.7326\n",
      "Epoch 5 Step [2200/5072] Loss: 4.0564\n",
      "Epoch 5 Step [2220/5072] Loss: 3.4865\n",
      "Epoch 5 Step [2240/5072] Loss: 3.8405\n",
      "Epoch 5 Step [2260/5072] Loss: 3.7401\n",
      "Epoch 5 Step [2280/5072] Loss: 4.0486\n",
      "Epoch 5 Step [2300/5072] Loss: 3.6594\n",
      "Epoch 5 Step [2320/5072] Loss: 4.0064\n",
      "Epoch 5 Step [2340/5072] Loss: 3.5544\n",
      "Epoch 5 Step [2360/5072] Loss: 3.9614\n",
      "Epoch 5 Step [2380/5072] Loss: 3.7724\n",
      "Epoch 5 Step [2400/5072] Loss: 3.9337\n",
      "Epoch 5 Step [2420/5072] Loss: 3.6645\n",
      "Epoch 5 Step [2440/5072] Loss: 3.6383\n",
      "Epoch 5 Step [2460/5072] Loss: 3.7217\n",
      "Epoch 5 Step [2480/5072] Loss: 3.7160\n",
      "Epoch 5 Step [2500/5072] Loss: 3.5465\n",
      "Epoch 5 Step [2520/5072] Loss: 3.7585\n",
      "Epoch 5 Step [2540/5072] Loss: 3.7294\n",
      "Epoch 5 Step [2560/5072] Loss: 3.6011\n",
      "Epoch 5 Step [2580/5072] Loss: 3.7424\n",
      "Epoch 5 Step [2600/5072] Loss: 3.6567\n",
      "Epoch 5 Step [2620/5072] Loss: 4.0137\n",
      "Epoch 5 Step [2640/5072] Loss: 3.6566\n",
      "Epoch 5 Step [2660/5072] Loss: 3.6021\n",
      "Epoch 5 Step [2680/5072] Loss: 3.9245\n",
      "Epoch 5 Step [2700/5072] Loss: 3.7153\n",
      "Epoch 5 Step [2720/5072] Loss: 3.8122\n",
      "Epoch 5 Step [2740/5072] Loss: 3.6990\n",
      "Epoch 5 Step [2760/5072] Loss: 3.7375\n",
      "Epoch 5 Step [2780/5072] Loss: 3.6580\n",
      "Epoch 5 Step [2800/5072] Loss: 3.7023\n",
      "Epoch 5 Step [2820/5072] Loss: 3.5730\n",
      "Epoch 5 Step [2840/5072] Loss: 3.6164\n",
      "Epoch 5 Step [2860/5072] Loss: 3.6887\n",
      "Epoch 5 Step [2880/5072] Loss: 3.9253\n",
      "Epoch 5 Step [2900/5072] Loss: 3.7718\n",
      "Epoch 5 Step [2920/5072] Loss: 3.4814\n",
      "Epoch 5 Step [2940/5072] Loss: 3.8807\n",
      "Epoch 5 Step [2960/5072] Loss: 3.7016\n",
      "Epoch 5 Step [2980/5072] Loss: 3.4609\n",
      "Epoch 5 Step [3000/5072] Loss: 3.5320\n",
      "Epoch 5 Step [3020/5072] Loss: 3.5107\n",
      "Epoch 5 Step [3040/5072] Loss: 3.5395\n",
      "Epoch 5 Step [3060/5072] Loss: 3.8505\n",
      "Epoch 5 Step [3080/5072] Loss: 3.4952\n",
      "Epoch 5 Step [3100/5072] Loss: 3.5550\n",
      "Epoch 5 Step [3120/5072] Loss: 3.8310\n",
      "Epoch 5 Step [3140/5072] Loss: 3.8688\n",
      "Epoch 5 Step [3160/5072] Loss: 3.6818\n",
      "Epoch 5 Step [3180/5072] Loss: 3.6159\n",
      "Epoch 5 Step [3200/5072] Loss: 3.5401\n",
      "Epoch 5 Step [3220/5072] Loss: 3.5347\n",
      "Epoch 5 Step [3240/5072] Loss: 3.6090\n",
      "Epoch 5 Step [3260/5072] Loss: 3.8619\n",
      "Epoch 5 Step [3280/5072] Loss: 3.7439\n",
      "Epoch 5 Step [3300/5072] Loss: 3.6968\n",
      "Epoch 5 Step [3320/5072] Loss: 3.8773\n",
      "Epoch 5 Step [3340/5072] Loss: 3.8056\n",
      "Epoch 5 Step [3360/5072] Loss: 3.6697\n",
      "Epoch 5 Step [3380/5072] Loss: 3.8688\n",
      "Epoch 5 Step [3400/5072] Loss: 3.5550\n",
      "Epoch 5 Step [3420/5072] Loss: 3.8020\n",
      "Epoch 5 Step [3440/5072] Loss: 3.8255\n",
      "Epoch 5 Step [3460/5072] Loss: 3.5944\n",
      "Epoch 5 Step [3480/5072] Loss: 3.6812\n",
      "Epoch 5 Step [3500/5072] Loss: 3.5320\n",
      "Epoch 5 Step [3520/5072] Loss: 3.5995\n",
      "Epoch 5 Step [3540/5072] Loss: 3.6454\n",
      "Epoch 5 Step [3560/5072] Loss: 3.7320\n",
      "Epoch 5 Step [3580/5072] Loss: 3.5453\n",
      "Epoch 5 Step [3600/5072] Loss: 3.2621\n",
      "Epoch 5 Step [3620/5072] Loss: 3.7037\n",
      "Epoch 5 Step [3640/5072] Loss: 3.5181\n",
      "Epoch 5 Step [3660/5072] Loss: 3.6637\n",
      "Epoch 5 Step [3680/5072] Loss: 3.8015\n",
      "Epoch 5 Step [3700/5072] Loss: 3.5045\n",
      "Epoch 5 Step [3720/5072] Loss: 3.7223\n",
      "Epoch 5 Step [3740/5072] Loss: 3.6708\n",
      "Epoch 5 Step [3760/5072] Loss: 3.7431\n",
      "Epoch 5 Step [3780/5072] Loss: 3.6917\n",
      "Epoch 5 Step [3800/5072] Loss: 3.7151\n",
      "Epoch 5 Step [3820/5072] Loss: 3.6727\n",
      "Epoch 5 Step [3840/5072] Loss: 3.4951\n",
      "Epoch 5 Step [3860/5072] Loss: 3.7254\n",
      "Epoch 5 Step [3880/5072] Loss: 3.4970\n",
      "Epoch 5 Step [3900/5072] Loss: 3.5817\n",
      "Epoch 5 Step [3920/5072] Loss: 3.5713\n",
      "Epoch 5 Step [3940/5072] Loss: 3.6200\n",
      "Epoch 5 Step [3960/5072] Loss: 3.6388\n",
      "Epoch 5 Step [3980/5072] Loss: 3.7479\n",
      "Epoch 5 Step [4000/5072] Loss: 3.6904\n",
      "Epoch 5 Step [4020/5072] Loss: 3.7869\n",
      "Epoch 5 Step [4040/5072] Loss: 3.5372\n",
      "Epoch 5 Step [4060/5072] Loss: 3.7730\n",
      "Epoch 5 Step [4080/5072] Loss: 3.4723\n",
      "Epoch 5 Step [4100/5072] Loss: 3.2078\n",
      "Epoch 5 Step [4120/5072] Loss: 3.5987\n",
      "Epoch 5 Step [4140/5072] Loss: 3.7093\n",
      "Epoch 5 Step [4160/5072] Loss: 3.8767\n",
      "Epoch 5 Step [4180/5072] Loss: 3.6663\n",
      "Epoch 5 Step [4200/5072] Loss: 3.5896\n",
      "Epoch 5 Step [4220/5072] Loss: 3.4821\n",
      "Epoch 5 Step [4240/5072] Loss: 3.7963\n",
      "Epoch 5 Step [4260/5072] Loss: 3.8143\n",
      "Epoch 5 Step [4280/5072] Loss: 3.6869\n",
      "Epoch 5 Step [4300/5072] Loss: 3.6419\n",
      "Epoch 5 Step [4320/5072] Loss: 3.5061\n",
      "Epoch 5 Step [4340/5072] Loss: 3.2871\n",
      "Epoch 5 Step [4360/5072] Loss: 3.9512\n",
      "Epoch 5 Step [4380/5072] Loss: 3.7501\n",
      "Epoch 5 Step [4400/5072] Loss: 3.3809\n",
      "Epoch 5 Step [4420/5072] Loss: 3.4971\n",
      "Epoch 5 Step [4440/5072] Loss: 3.6867\n",
      "Epoch 5 Step [4460/5072] Loss: 3.5187\n",
      "Epoch 5 Step [4480/5072] Loss: 3.6770\n",
      "Epoch 5 Step [4500/5072] Loss: 3.7177\n",
      "Epoch 5 Step [4520/5072] Loss: 3.4222\n",
      "Epoch 5 Step [4540/5072] Loss: 3.4982\n",
      "Epoch 5 Step [4560/5072] Loss: 3.7698\n",
      "Epoch 5 Step [4580/5072] Loss: 3.8791\n",
      "Epoch 5 Step [4600/5072] Loss: 3.7190\n",
      "Epoch 5 Step [4620/5072] Loss: 3.7886\n",
      "Epoch 5 Step [4640/5072] Loss: 4.0319\n",
      "Epoch 5 Step [4660/5072] Loss: 3.8191\n",
      "Epoch 5 Step [4680/5072] Loss: 3.3248\n",
      "Epoch 5 Step [4700/5072] Loss: 3.2971\n",
      "Epoch 5 Step [4720/5072] Loss: 3.8378\n",
      "Epoch 5 Step [4740/5072] Loss: 3.6481\n",
      "Epoch 5 Step [4760/5072] Loss: 3.6317\n",
      "Epoch 5 Step [4780/5072] Loss: 3.7020\n",
      "Epoch 5 Step [4800/5072] Loss: 3.5505\n",
      "Epoch 5 Step [4820/5072] Loss: 3.6392\n",
      "Epoch 5 Step [4840/5072] Loss: 3.5865\n",
      "Epoch 5 Step [4860/5072] Loss: 3.4202\n",
      "Epoch 5 Step [4880/5072] Loss: 3.8459\n",
      "Epoch 5 Step [4900/5072] Loss: 3.3846\n",
      "Epoch 5 Step [4920/5072] Loss: 3.5217\n",
      "Epoch 5 Step [4940/5072] Loss: 3.7156\n",
      "Epoch 5 Step [4960/5072] Loss: 3.5141\n",
      "Epoch 5 Step [4980/5072] Loss: 3.5190\n",
      "Epoch 5 Step [5000/5072] Loss: 3.9589\n",
      "Epoch 5 Step [5020/5072] Loss: 3.8683\n",
      "Epoch 5 Step [5040/5072] Loss: 3.4658\n",
      "Epoch 5 Step [5060/5072] Loss: 3.8172\n",
      "Epoch 5 Finished | Acc: 8.04% | Loss: 3.7042\n",
      "Model saved as trained_on_all_classes.pkl\n"
     ]
    }
   ],
   "source": [
    "# 1. Load ENTIRE dataset\n",
    "full_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=None)\n",
    "\n",
    "# 2. Get Persistent Split (Will create 'pretrain_full_train.pkl' and 'pretrain_full_test.pkl')\n",
    "full_train, full_test = get_persistent_splits(full_dataset, 0.8, \"pretrain_full\")\n",
    "\n",
    "train_loader = DataLoader(full_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 3. Initialize Model\n",
    "cnn = CNN(cnn_config, MAX_POOL, (3, FRAME_SIZE, FRAME_SIZE), EMBEDDING_DIM)\n",
    "model = CNNLSTM(cnn, len(full_dataset.classes), LSTM_HIDDEN, LSTM_LAYERS).to(device)\n",
    "\n",
    "# 4. Train\n",
    "print(f\"Pre-training on {len(full_dataset.classes)} classes...\")\n",
    "train(model, epochs=5, accumulation_steps=ACCUM_STEPS, learning_rate=1e-3, train_loader=train_loader, device=device)\n",
    "\n",
    "# 5. Save Master Model\n",
    "save_model(model, \"trained_on_all_classes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "196503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Set from pretrain_full_test.pkl...\n",
      "Evaluating on 51 classes.\n",
      "Loading Master Model from trained_on_all_classes.pkl...\n",
      "Running Inference...\n",
      "\n",
      "========================================\n",
      "MASTER MODEL CLASSIFICATION REPORT\n",
      "========================================\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    brush_hair       0.00      0.00      0.00         0\n",
      "     cartwheel       0.00      0.00      0.00         0\n",
      "         catch       0.00      0.00      0.00         0\n",
      "          chew       0.00      0.00      0.00         0\n",
      "          clap       0.00      0.00      0.00        23\n",
      "         climb       0.00      0.00      0.00        27\n",
      "  climb_stairs       0.00      0.00      0.00        26\n",
      "          dive       0.00      0.00      0.00        24\n",
      "    draw_sword       0.00      0.00      0.00        25\n",
      "       dribble       0.00      0.00      0.00        26\n",
      "         drink       0.00      0.00      0.00        25\n",
      "           eat       0.00      0.00      0.00        18\n",
      "    fall_floor       0.00      0.00      0.00        27\n",
      "       fencing       0.00      0.00      0.00        18\n",
      "     flic_flac       0.00      0.00      0.00        19\n",
      "          golf       0.00      0.00      0.00        26\n",
      "     handstand       0.00      0.00      0.00        20\n",
      "           hit       0.00      0.00      0.00        26\n",
      "           hug       0.00      0.00      0.00        24\n",
      "          jump       0.00      0.00      0.00        27\n",
      "          kick       0.00      0.00      0.00        27\n",
      "     kick_ball       0.00      0.00      0.00        23\n",
      "          kiss       0.00      0.00      0.00        16\n",
      "         laugh       0.00      0.00      0.00        26\n",
      "          pick       0.00      0.00      0.00        12\n",
      "          pour       0.00      0.00      0.00        28\n",
      "        pullup       0.00      0.00      0.00        24\n",
      "         punch       0.00      0.00      0.00        19\n",
      "          push       0.00      0.00      0.00        21\n",
      "        pushup       0.00      0.00      0.00        22\n",
      "     ride_bike       0.00      0.00      0.00        20\n",
      "    ride_horse       0.00      0.00      0.00        20\n",
      "           run       0.00      0.00      0.00        46\n",
      "   shake_hands       0.00      0.00      0.00        32\n",
      "    shoot_ball       0.00      0.00      0.00        28\n",
      "     shoot_bow       0.00      0.00      0.00        23\n",
      "     shoot_gun       0.00      0.00      0.00        23\n",
      "           sit       0.00      0.00      0.00        23\n",
      "         situp       0.00      0.00      0.00        17\n",
      "         smile       0.00      0.00      0.00        22\n",
      "         smoke       0.00      0.00      0.00        22\n",
      "    somersault       0.00      0.00      0.00        24\n",
      "         stand       0.00      0.00      0.00        42\n",
      "swing_baseball       0.03      0.42      0.06        24\n",
      "         sword       0.00      0.00      0.00        27\n",
      "sword_exercise       0.00      0.00      0.00        25\n",
      "          talk       0.00      0.00      0.00        19\n",
      "         throw       0.00      0.00      0.00        19\n",
      "          turn       0.00      0.00      0.00        68\n",
      "          walk       0.13      0.97      0.23       124\n",
      "          wave       0.00      0.00      0.00        22\n",
      "\n",
      "      accuracy                           0.10      1269\n",
      "     macro avg       0.00      0.03      0.01      1269\n",
      "  weighted avg       0.01      0.10      0.02      1269\n",
      "\n",
      "Total Accuracy: 10.24%\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from video_recognition import load_model \n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_FILE = \"trained_on_all_classes.pkl\"\n",
    "TEST_SET_FILE = \"pretrain_full_test.pkl\"\n",
    "BATCH_SIZE = 1 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Test Set from {TEST_SET_FILE}...\")\n",
    "with open(TEST_SET_FILE, 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "# Get the list of ALL 51 classes\n",
    "CLASSES = test_set.dataset.classes \n",
    "print(f\"Evaluating on {len(CLASSES)} classes.\")\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Loading Master Model from {MODEL_FILE}...\")\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 4. REPORTING ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MASTER MODEL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- FIX: Explicitly pass the range of labels to handle missing classes ---\n",
    "all_possible_labels = range(len(CLASSES))\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    labels=all_possible_labels,  # <--- FIX IS HERE\n",
    "    target_names=CLASSES, \n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Total Accuracy: {acc*100:.2f}%\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376e2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Set from pretrain_full_train.pkl...\n",
      "Loading Master Model from trained_on_all_classes.pkl...\n",
      "Resuming Master Model training for 1 more epochs...\n",
      "Epoch 1 Step [20/5072] Loss: 3.6202\n",
      "Epoch 1 Step [40/5072] Loss: 3.8893\n",
      "Epoch 1 Step [60/5072] Loss: 3.6872\n",
      "Epoch 1 Step [80/5072] Loss: 3.5116\n",
      "Epoch 1 Step [100/5072] Loss: 3.8071\n",
      "Epoch 1 Step [120/5072] Loss: 3.4824\n",
      "Epoch 1 Step [140/5072] Loss: 3.2944\n",
      "Epoch 1 Step [160/5072] Loss: 3.6232\n",
      "Epoch 1 Step [180/5072] Loss: 3.6427\n",
      "Epoch 1 Step [200/5072] Loss: 3.5332\n",
      "Epoch 1 Step [220/5072] Loss: 3.6442\n",
      "Epoch 1 Step [240/5072] Loss: 3.4655\n",
      "Epoch 1 Step [260/5072] Loss: 3.8467\n",
      "Epoch 1 Step [280/5072] Loss: 3.5959\n",
      "Epoch 1 Step [300/5072] Loss: 3.7704\n",
      "Epoch 1 Step [320/5072] Loss: 3.6100\n",
      "Epoch 1 Step [340/5072] Loss: 3.2781\n",
      "Epoch 1 Step [360/5072] Loss: 3.7071\n",
      "Epoch 1 Step [380/5072] Loss: 3.6183\n",
      "Epoch 1 Step [400/5072] Loss: 3.5289\n",
      "Epoch 1 Step [420/5072] Loss: 3.4467\n",
      "Epoch 1 Step [440/5072] Loss: 3.6112\n",
      "Epoch 1 Step [460/5072] Loss: 3.7188\n",
      "Epoch 1 Step [480/5072] Loss: 3.6692\n",
      "Epoch 1 Step [500/5072] Loss: 3.7390\n",
      "Epoch 1 Step [520/5072] Loss: 3.8556\n",
      "Epoch 1 Step [540/5072] Loss: 3.3505\n",
      "Epoch 1 Step [560/5072] Loss: 3.6552\n",
      "Epoch 1 Step [580/5072] Loss: 3.5928\n",
      "Epoch 1 Step [600/5072] Loss: 3.6290\n",
      "Epoch 1 Step [620/5072] Loss: 3.3090\n",
      "Epoch 1 Step [640/5072] Loss: 3.5318\n",
      "Epoch 1 Step [660/5072] Loss: 3.5792\n",
      "Epoch 1 Step [680/5072] Loss: 3.7450\n",
      "Epoch 1 Step [700/5072] Loss: 3.3185\n",
      "Epoch 1 Step [720/5072] Loss: 3.5474\n",
      "Epoch 1 Step [740/5072] Loss: 3.4868\n",
      "Epoch 1 Step [760/5072] Loss: 3.6594\n",
      "Epoch 1 Step [780/5072] Loss: 3.7368\n",
      "Epoch 1 Step [800/5072] Loss: 3.4819\n",
      "Epoch 1 Step [820/5072] Loss: 3.4849\n",
      "Epoch 1 Step [840/5072] Loss: 3.6491\n",
      "Epoch 1 Step [860/5072] Loss: 3.6475\n",
      "Epoch 1 Step [880/5072] Loss: 3.3898\n",
      "Epoch 1 Step [900/5072] Loss: 3.6756\n",
      "Epoch 1 Step [920/5072] Loss: 3.6106\n",
      "Epoch 1 Step [940/5072] Loss: 3.8259\n",
      "Epoch 1 Step [960/5072] Loss: 3.9865\n",
      "Epoch 1 Step [980/5072] Loss: 3.5099\n",
      "Epoch 1 Step [1000/5072] Loss: 3.7503\n",
      "Epoch 1 Step [1020/5072] Loss: 3.8561\n",
      "Epoch 1 Step [1040/5072] Loss: 3.3210\n",
      "Epoch 1 Step [1060/5072] Loss: 3.6987\n",
      "Epoch 1 Step [1080/5072] Loss: 3.5258\n",
      "Epoch 1 Step [1100/5072] Loss: 3.5564\n",
      "Epoch 1 Step [1120/5072] Loss: 3.6616\n",
      "Epoch 1 Step [1140/5072] Loss: 3.4632\n",
      "Epoch 1 Step [1160/5072] Loss: 3.5806\n",
      "Epoch 1 Step [1180/5072] Loss: 3.6732\n",
      "Epoch 1 Step [1200/5072] Loss: 3.6914\n",
      "Epoch 1 Step [1220/5072] Loss: 3.5329\n",
      "Epoch 1 Step [1240/5072] Loss: 3.7901\n",
      "Epoch 1 Step [1260/5072] Loss: 3.5645\n",
      "Epoch 1 Step [1280/5072] Loss: 3.7269\n",
      "Epoch 1 Step [1300/5072] Loss: 3.5533\n",
      "Epoch 1 Step [1320/5072] Loss: 3.8079\n",
      "Epoch 1 Step [1340/5072] Loss: 3.4771\n",
      "Epoch 1 Step [1360/5072] Loss: 3.7996\n",
      "Epoch 1 Step [1380/5072] Loss: 3.7641\n",
      "Epoch 1 Step [1400/5072] Loss: 3.6798\n",
      "Epoch 1 Step [1420/5072] Loss: 3.7870\n",
      "Epoch 1 Step [1440/5072] Loss: 3.6696\n",
      "Epoch 1 Step [1460/5072] Loss: 3.3675\n",
      "Epoch 1 Step [1480/5072] Loss: 3.7869\n",
      "Epoch 1 Step [1500/5072] Loss: 3.4132\n",
      "Epoch 1 Step [1520/5072] Loss: 3.5950\n",
      "Epoch 1 Step [1540/5072] Loss: 3.5489\n",
      "Epoch 1 Step [1560/5072] Loss: 3.4521\n",
      "Epoch 1 Step [1580/5072] Loss: 3.6683\n",
      "Epoch 1 Step [1600/5072] Loss: 3.3986\n",
      "Epoch 1 Step [1620/5072] Loss: 3.7342\n",
      "Epoch 1 Step [1640/5072] Loss: 4.0193\n",
      "Epoch 1 Step [1660/5072] Loss: 3.7555\n",
      "Epoch 1 Step [1680/5072] Loss: 3.3017\n",
      "Epoch 1 Step [1700/5072] Loss: 3.6343\n",
      "Epoch 1 Step [1720/5072] Loss: 3.6878\n",
      "Epoch 1 Step [1740/5072] Loss: 3.7221\n",
      "Epoch 1 Step [1760/5072] Loss: 3.8441\n",
      "Epoch 1 Step [1780/5072] Loss: 3.6716\n",
      "Epoch 1 Step [1800/5072] Loss: 3.6440\n",
      "Epoch 1 Step [1820/5072] Loss: 3.5160\n",
      "Epoch 1 Step [1840/5072] Loss: 3.5819\n",
      "Epoch 1 Step [1860/5072] Loss: 3.6511\n",
      "Epoch 1 Step [1880/5072] Loss: 3.4798\n",
      "Epoch 1 Step [1900/5072] Loss: 3.4473\n",
      "Epoch 1 Step [1920/5072] Loss: 3.3273\n",
      "Epoch 1 Step [1940/5072] Loss: 3.6438\n",
      "Epoch 1 Step [1960/5072] Loss: 3.7583\n",
      "Epoch 1 Step [1980/5072] Loss: 3.4243\n",
      "Epoch 1 Step [2000/5072] Loss: 3.9178\n",
      "Epoch 1 Step [2020/5072] Loss: 3.5763\n",
      "Epoch 1 Step [2040/5072] Loss: 3.5413\n",
      "Epoch 1 Step [2060/5072] Loss: 3.5084\n",
      "Epoch 1 Step [2080/5072] Loss: 3.6598\n",
      "Epoch 1 Step [2100/5072] Loss: 3.6043\n",
      "Epoch 1 Step [2120/5072] Loss: 3.5645\n",
      "Epoch 1 Step [2140/5072] Loss: 3.5247\n",
      "Epoch 1 Step [2160/5072] Loss: 3.8165\n",
      "Epoch 1 Step [2180/5072] Loss: 3.5865\n",
      "Epoch 1 Step [2200/5072] Loss: 3.5980\n",
      "Epoch 1 Step [2220/5072] Loss: 3.5556\n",
      "Epoch 1 Step [2240/5072] Loss: 3.7411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- 3. RESUME TRAINING ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResuming Master Model training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMORE_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m more epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMORE_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACCUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# --- 4. SAVE ---\u001b[39;00m\n\u001b[0;32m     27\u001b[0m save_model(model, MODEL_FILE)\n",
      "File \u001b[1;32mc:\\Users\\marti\\Desktop\\Magistrale\\secondo anno\\computational learning\\progetto\\video_recognition.py:230\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epochs, accumulation_steps, learning_rate, train_loader, device)\u001b[0m\n\u001b[0;32m    227\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    229\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[1;32m--> 230\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    233\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MORE_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4 # Standard LR for pre-training (higher than fine-tuning)\n",
    "ACCUM_STEPS = 16\n",
    "BATCH_SIZE = 1\n",
    "USE_WEIGHTED_LOSS = True\n",
    "\n",
    "TRAIN_SET_FILE = \"pretrain_full_train.pkl\" # The full training set\n",
    "MODEL_FILE = \"trained_on_all_classes.pkl\"\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Train Set from {TRAIN_SET_FILE}...\")\n",
    "with open(TRAIN_SET_FILE, 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Loading Master Model from {MODEL_FILE}...\")\n",
    "from video_recognition import load_model, train, save_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 3. RESUME TRAINING ---\n",
    "print(f\"Resuming Master Model training for {MORE_EPOCHS} more epochs...\")\n",
    "train(model, epochs=MORE_EPOCHS, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device, use_weighted_loss=USE_WEIGHTED_LOSS)\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "save_model(model, MODEL_FILE)\n",
    "print(\"Updated Master Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5a25a",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Subset\n",
    "TARGET_CLASSES = ['jump', 'run', 'smile', 'wave']\n",
    "\n",
    "# 2. Load Subset Dataset\n",
    "subset_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=TARGET_CLASSES)\n",
    "\n",
    "# 3. Get Persistent Split (Will create 'finetune_subset_train.pkl' and 'finetune_subset_test.pkl')\n",
    "# IMPORTANT: This ensures that even if you restart the kernel, you test on the EXACT same subset videos.\n",
    "sub_train, sub_test = get_persistent_splits(subset_dataset, 0.8, \"finetune_subset\")\n",
    "\n",
    "sub_loader = DataLoader(sub_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 4. Load Master Model & Modify\n",
    "model = load_model(\"trained_on_all_classes.pkl\")\n",
    "model = model.to(device)\n",
    "model = replace_head_for_finetuning(model, new_num_classes=len(TARGET_CLASSES))\n",
    "model = model.to(device)\n",
    "\n",
    "# 5. Fine-Tune\n",
    "print(f\"Fine-tuning for {TARGET_CLASSES}...\")\n",
    "train(model, epochs=5, accumulation_steps=ACCUM_STEPS, learning_rate=1e-4, train_loader=sub_loader, device=device)\n",
    "\n",
    "# 6. Save Final Model\n",
    "save_model(model, \"finetuned_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde53172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_FILE = \"finetuned_model.pkl\"\n",
    "TEST_SET_FILE = \"finetune_subset_test.pkl\"\n",
    "BATCH_SIZE = 1 # Keep at 1 for precise video-by-video evaluation\n",
    "\n",
    "# Define the classes again to ensure the labels match the report\n",
    "# (Must match the order used in fine-tuning)\n",
    "TARGET_CLASSES = ['jump', 'run', 'smile', 'wave'] \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Test Set from {TEST_SET_FILE}...\")\n",
    "with open(TEST_SET_FILE, 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Loading Model from {MODEL_FILE}...\")\n",
    "# Use the load_model function from your module\n",
    "from video_recognition import load_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "model.eval() # Set to evaluation mode (Important: disables Dropout)\n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 4. REPORTING ---\n",
    "# A. Classification Report (Precision, Recall, F1)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(all_labels, all_preds, target_names=TARGET_CLASSES, zero_division=0))\n",
    "\n",
    "# B. Accuracy Score\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Total Accuracy: {acc*100:.2f}%\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# C. Confusion Matrix Plot\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=TARGET_CLASSES, \n",
    "            yticklabels=TARGET_CLASSES)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MORE_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5 # Keep this low for fine-tuning/resuming\n",
    "ACCUM_STEPS = 10\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "TRAIN_SET_FILE = \"finetune_subset_train.pkl\"\n",
    "MODEL_FILE = \"finetuned_model.pkl\"\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Train Set from {TRAIN_SET_FILE}...\")\n",
    "with open(TRAIN_SET_FILE, 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Loading Model from {MODEL_FILE}...\")\n",
    "from video_recognition import load_model, train, save_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 3. RESUME TRAINING ---\n",
    "print(f\"Resuming training for {MORE_EPOCHS} more epochs...\")\n",
    "train(model, epochs=MORE_EPOCHS, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device)\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "save_model(model, MODEL_FILE)\n",
    "print(\"Updated model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
