{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266e3ae2",
   "metadata": {},
   "source": [
    "# Video Recognition\n",
    "\n",
    "Project on video recognition whith the dataset HMDB51 (https://serre.lab.brown.edu/hmdb51.html). A special focus is given to the efficiency of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c35e7",
   "metadata": {},
   "source": [
    "Training finora (loss bilanciata):\n",
    "- 2 epoche lr=5e-4 FRAME RATE A 3\n",
    "- 5 epoche lr=1e-4 FRAME RATE A 3 Accuracy: 17.63% Training | 13.51% Test\n",
    "- 3 epoche kr=5e-5 FRAME RATE A 3 Accuracy: 19.91% Training | 17.73% Test v1\n",
    "- 5 epoche kr=5e-5 FRAME RATE A 3 Accuracy: 26.61% Training |  22.97% Test v2\n",
    "- 5 epoche kr=1e-5 FRAME RATE A 1 Accuracy: 28.82% Training |  24.21% Test v3\n",
    "\n",
    "FRAME RATE 3 => Circa 1h40min per epoca\n",
    "FRAME RATE 1 => Circa 2h10min per epoca\n",
    "IDEA: FARE UN PO' DI EPOCHE CON FRAME RATE ALTO E POI ABBASSARLO ALLA FINE\n",
    "\n",
    "Fine tuning:\n",
    "- 10 epoche lr=1e-4 solo head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a71d5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5339e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle\n",
    "\n",
    "# Import everything from your new file\n",
    "from video_recognition import (\n",
    "    VideoLoader, CNN, CNNLSTM, train, save_model, load_model, \n",
    "    replace_head_for_finetuning, MAX_POOL, get_persistent_splits\n",
    ")\n",
    "\n",
    "dataset_directory = \"./dataset\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026c4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 224\n",
    "FRAME_RATE_SCALER = 3\n",
    "BATCH_SIZE = 1\n",
    "ACCUM_STEPS = 20\n",
    "EMBEDDING_DIM = 256\n",
    "LSTM_HIDDEN = 128\n",
    "LSTM_LAYERS = 1\n",
    "USE_WEIGHTED_LOSS=True\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "cnn_config = [\n",
    "    {'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 128, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3595200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes loaded: ['clap', 'climb', 'climb_stairs', 'dive', 'draw_sword', 'dribble', 'drink', 'eat', 'fall_floor', 'fencing', 'flic_flac', 'golf', 'handstand', 'hit', 'hug', 'jump', 'kick', 'kick_ball', 'kiss', 'laugh', 'pick', 'pour', 'pullup', 'punch', 'push', 'pushup', 'ride_bike', 'ride_horse', 'run', 'shake_hands', 'shoot_ball', 'shoot_bow', 'shoot_gun', 'sit', 'situp', 'smile', 'smoke', 'somersault', 'stand', 'swing_baseball', 'sword', 'sword_exercise', 'talk', 'throw', 'turn', 'walk', 'wave']\n",
      "Database size: 6341\n",
      "Loading existing split from pretrain_full...\n",
      "Pre-training on 47 classes...\n",
      "Calculating class weights for Weighted Loss...\n",
      "Class Weights (Shape torch.Size([47])): tensor([1.0685, 1.1859, 1.1480, 1.1604, 1.1859, 0.9303, 0.8175, 1.3002, 0.9722,\n",
      "        1.1241, 1.2404, 1.3835, 1.2125, 1.0685, 1.1859, 0.8431, 0.9810, 1.0900,\n",
      "        1.3323, 1.0477, 1.1991, 1.2548, 1.3660, 1.0685, 1.1241, 1.3835, 1.3160,\n",
      "        1.1604, 0.5563, 0.8993, 1.0086, 1.3160, 1.2125, 0.9900, 1.2548, 1.3323,\n",
      "        1.3002, 0.9466, 0.8633, 0.9303, 1.0376, 1.1012, 1.0791, 1.4583, 0.5833,\n",
      "        0.2409, 1.4015])\n",
      "Epoch 1 Step [20/5072] Loss: 3.8407\n",
      "Epoch 1 Step [40/5072] Loss: 3.8528\n",
      "Epoch 1 Step [60/5072] Loss: 3.8164\n",
      "Epoch 1 Step [80/5072] Loss: 3.8679\n",
      "Epoch 1 Step [100/5072] Loss: 3.8851\n",
      "Epoch 1 Step [120/5072] Loss: 3.7443\n",
      "Epoch 1 Step [140/5072] Loss: 3.9917\n",
      "Epoch 1 Step [160/5072] Loss: 3.8758\n",
      "Epoch 1 Step [180/5072] Loss: 3.7752\n",
      "Epoch 1 Step [200/5072] Loss: 4.0182\n",
      "Epoch 1 Step [220/5072] Loss: 3.8193\n",
      "Epoch 1 Step [240/5072] Loss: 3.8723\n",
      "Epoch 1 Step [260/5072] Loss: 3.8312\n",
      "Epoch 1 Step [280/5072] Loss: 3.7756\n",
      "Epoch 1 Step [300/5072] Loss: 3.8652\n",
      "Epoch 1 Step [320/5072] Loss: 3.8179\n",
      "Epoch 1 Step [340/5072] Loss: 3.8121\n",
      "Epoch 1 Step [360/5072] Loss: 3.8646\n",
      "Epoch 1 Step [380/5072] Loss: 3.7467\n",
      "Epoch 1 Step [400/5072] Loss: 3.8434\n",
      "Epoch 1 Step [420/5072] Loss: 3.8820\n",
      "Epoch 1 Step [440/5072] Loss: 3.8126\n",
      "Epoch 1 Step [460/5072] Loss: 3.8532\n",
      "Epoch 1 Step [480/5072] Loss: 3.7799\n",
      "Epoch 1 Step [500/5072] Loss: 3.9009\n",
      "Epoch 1 Step [520/5072] Loss: 3.8187\n",
      "Epoch 1 Step [540/5072] Loss: 3.8852\n",
      "Epoch 1 Step [560/5072] Loss: 3.8484\n",
      "Epoch 1 Step [580/5072] Loss: 3.8961\n",
      "Epoch 1 Step [600/5072] Loss: 3.7206\n",
      "Epoch 1 Step [620/5072] Loss: 3.8441\n",
      "Epoch 1 Step [640/5072] Loss: 3.8487\n",
      "Epoch 1 Step [660/5072] Loss: 3.8266\n",
      "Epoch 1 Step [680/5072] Loss: 3.8162\n",
      "Epoch 1 Step [700/5072] Loss: 3.8215\n",
      "Epoch 1 Step [720/5072] Loss: 3.7828\n",
      "Epoch 1 Step [740/5072] Loss: 3.8103\n",
      "Epoch 1 Step [760/5072] Loss: 3.7812\n",
      "Epoch 1 Step [780/5072] Loss: 3.6997\n",
      "Epoch 1 Step [800/5072] Loss: 3.7155\n",
      "Epoch 1 Step [820/5072] Loss: 3.7782\n",
      "Epoch 1 Step [840/5072] Loss: 3.9000\n",
      "Epoch 1 Step [860/5072] Loss: 3.8631\n",
      "Epoch 1 Step [880/5072] Loss: 3.8382\n",
      "Epoch 1 Step [900/5072] Loss: 3.8480\n",
      "Epoch 1 Step [920/5072] Loss: 3.6285\n",
      "Epoch 1 Step [940/5072] Loss: 3.7222\n",
      "Epoch 1 Step [960/5072] Loss: 3.7021\n",
      "Epoch 1 Step [980/5072] Loss: 3.8122\n",
      "Epoch 1 Step [1000/5072] Loss: 3.7588\n",
      "Epoch 1 Step [1020/5072] Loss: 3.5740\n",
      "Epoch 1 Step [1040/5072] Loss: 3.8446\n",
      "Epoch 1 Step [1060/5072] Loss: 3.8124\n",
      "Epoch 1 Step [1080/5072] Loss: 3.7836\n",
      "Epoch 1 Step [1100/5072] Loss: 3.7361\n",
      "Epoch 1 Step [1120/5072] Loss: 3.6124\n",
      "Epoch 1 Step [1140/5072] Loss: 3.8213\n",
      "Epoch 1 Step [1160/5072] Loss: 3.7371\n",
      "Epoch 1 Step [1180/5072] Loss: 3.9600\n",
      "Epoch 1 Step [1200/5072] Loss: 3.8055\n",
      "Epoch 1 Step [1220/5072] Loss: 3.6954\n",
      "Epoch 1 Step [1240/5072] Loss: 3.7341\n",
      "Epoch 1 Step [1260/5072] Loss: 3.6048\n",
      "Epoch 1 Step [1280/5072] Loss: 3.8492\n",
      "Epoch 1 Step [1300/5072] Loss: 3.7702\n",
      "Epoch 1 Step [1320/5072] Loss: 3.7031\n",
      "Epoch 1 Step [1340/5072] Loss: 3.4911\n",
      "Epoch 1 Step [1360/5072] Loss: 3.8295\n",
      "Epoch 1 Step [1380/5072] Loss: 3.6852\n",
      "Epoch 1 Step [1400/5072] Loss: 3.9724\n",
      "Epoch 1 Step [1420/5072] Loss: 3.9211\n",
      "Epoch 1 Step [1440/5072] Loss: 4.0651\n",
      "Epoch 1 Step [1460/5072] Loss: 3.8205\n",
      "Epoch 1 Step [1480/5072] Loss: 3.8150\n",
      "Epoch 1 Step [1500/5072] Loss: 3.6154\n",
      "Epoch 1 Step [1520/5072] Loss: 3.6746\n",
      "Epoch 1 Step [1540/5072] Loss: 3.7752\n",
      "Epoch 1 Step [1560/5072] Loss: 3.8704\n",
      "Epoch 1 Step [1580/5072] Loss: 3.6831\n",
      "Epoch 1 Step [1600/5072] Loss: 3.7934\n",
      "Epoch 1 Step [1620/5072] Loss: 3.8705\n",
      "Epoch 1 Step [1640/5072] Loss: 3.7681\n",
      "Epoch 1 Step [1660/5072] Loss: 3.6894\n",
      "Epoch 1 Step [1680/5072] Loss: 3.7457\n",
      "Epoch 1 Step [1700/5072] Loss: 3.6405\n",
      "Epoch 1 Step [1720/5072] Loss: 3.6459\n",
      "Epoch 1 Step [1740/5072] Loss: 3.7225\n",
      "Epoch 1 Step [1760/5072] Loss: 3.6358\n",
      "Epoch 1 Step [1780/5072] Loss: 3.6971\n",
      "Epoch 1 Step [1800/5072] Loss: 3.7054\n",
      "Epoch 1 Step [1820/5072] Loss: 3.8225\n",
      "Epoch 1 Step [1840/5072] Loss: 3.6891\n",
      "Epoch 1 Step [1860/5072] Loss: 3.6632\n",
      "Epoch 1 Step [1880/5072] Loss: 3.6039\n",
      "Epoch 1 Step [1900/5072] Loss: 3.6843\n",
      "Epoch 1 Step [1920/5072] Loss: 3.8321\n",
      "Epoch 1 Step [1940/5072] Loss: 3.6397\n",
      "Epoch 1 Step [1960/5072] Loss: 3.7562\n",
      "Epoch 1 Step [1980/5072] Loss: 3.7298\n",
      "Epoch 1 Step [2000/5072] Loss: 3.6834\n",
      "Epoch 1 Step [2020/5072] Loss: 3.6591\n",
      "Epoch 1 Step [2040/5072] Loss: 3.7698\n",
      "Epoch 1 Step [2060/5072] Loss: 3.5958\n",
      "Epoch 1 Step [2080/5072] Loss: 3.7965\n",
      "Epoch 1 Step [2100/5072] Loss: 3.7867\n",
      "Epoch 1 Step [2120/5072] Loss: 3.7253\n",
      "Epoch 1 Step [2140/5072] Loss: 3.4915\n",
      "Epoch 1 Step [2160/5072] Loss: 3.4286\n",
      "Epoch 1 Step [2180/5072] Loss: 3.5574\n",
      "Epoch 1 Step [2200/5072] Loss: 3.5678\n",
      "Epoch 1 Step [2220/5072] Loss: 3.9545\n",
      "Epoch 1 Step [2240/5072] Loss: 3.6354\n",
      "Epoch 1 Step [2260/5072] Loss: 3.5654\n",
      "Epoch 1 Step [2280/5072] Loss: 3.4559\n",
      "Epoch 1 Step [2300/5072] Loss: 3.6536\n",
      "Epoch 1 Step [2320/5072] Loss: 3.7950\n",
      "Epoch 1 Step [2340/5072] Loss: 3.5815\n",
      "Epoch 1 Step [2360/5072] Loss: 3.7450\n",
      "Epoch 1 Step [2380/5072] Loss: 3.5423\n",
      "Epoch 1 Step [2400/5072] Loss: 3.6883\n",
      "Epoch 1 Step [2420/5072] Loss: 3.3935\n",
      "Epoch 1 Step [2440/5072] Loss: 3.4227\n",
      "Epoch 1 Step [2460/5072] Loss: 3.6945\n",
      "Epoch 1 Step [2480/5072] Loss: 3.4739\n",
      "Epoch 1 Step [2500/5072] Loss: 3.5894\n",
      "Epoch 1 Step [2520/5072] Loss: 3.5750\n",
      "Epoch 1 Step [2540/5072] Loss: 3.9083\n",
      "Epoch 1 Step [2560/5072] Loss: 3.8437\n",
      "Epoch 1 Step [2580/5072] Loss: 3.4971\n",
      "Epoch 1 Step [2600/5072] Loss: 3.5340\n",
      "Epoch 1 Step [2620/5072] Loss: 3.8717\n",
      "Epoch 1 Step [2640/5072] Loss: 3.4382\n",
      "Epoch 1 Step [2660/5072] Loss: 3.4939\n",
      "Epoch 1 Step [2680/5072] Loss: 3.5679\n",
      "Epoch 1 Step [2700/5072] Loss: 3.8385\n",
      "Epoch 1 Step [2720/5072] Loss: 3.4918\n",
      "Epoch 1 Step [2740/5072] Loss: 3.7264\n",
      "Epoch 1 Step [2760/5072] Loss: 3.4684\n",
      "Epoch 1 Step [2780/5072] Loss: 3.5851\n",
      "Epoch 1 Step [2800/5072] Loss: 3.6090\n",
      "Epoch 1 Step [2820/5072] Loss: 3.6493\n",
      "Epoch 1 Step [2840/5072] Loss: 3.6100\n",
      "Epoch 1 Step [2860/5072] Loss: 3.4617\n",
      "Epoch 1 Step [2880/5072] Loss: 3.7118\n",
      "Epoch 1 Step [2900/5072] Loss: 3.4798\n",
      "Epoch 1 Step [2920/5072] Loss: 3.6285\n",
      "Epoch 1 Step [2940/5072] Loss: 3.6339\n",
      "Epoch 1 Step [2960/5072] Loss: 3.5102\n",
      "Epoch 1 Step [2980/5072] Loss: 3.6775\n",
      "Epoch 1 Step [3000/5072] Loss: 3.7365\n",
      "Epoch 1 Step [3020/5072] Loss: 3.7939\n",
      "Epoch 1 Step [3040/5072] Loss: 3.6560\n",
      "Epoch 1 Step [3060/5072] Loss: 3.7610\n",
      "Epoch 1 Step [3080/5072] Loss: 3.6552\n",
      "Epoch 1 Step [3100/5072] Loss: 3.7314\n",
      "Epoch 1 Step [3120/5072] Loss: 3.4156\n",
      "Epoch 1 Step [3140/5072] Loss: 3.9481\n",
      "Epoch 1 Step [3160/5072] Loss: 3.8824\n",
      "Epoch 1 Step [3180/5072] Loss: 3.7574\n",
      "Epoch 1 Step [3200/5072] Loss: 3.9037\n",
      "Epoch 1 Step [3220/5072] Loss: 3.7820\n",
      "Epoch 1 Step [3240/5072] Loss: 3.9717\n",
      "Epoch 1 Step [3260/5072] Loss: 3.6749\n",
      "Epoch 1 Step [3280/5072] Loss: 3.8697\n",
      "Epoch 1 Step [3300/5072] Loss: 3.7157\n",
      "Epoch 1 Step [3320/5072] Loss: 3.8243\n",
      "Epoch 1 Step [3340/5072] Loss: 3.7105\n",
      "Epoch 1 Step [3360/5072] Loss: 3.5620\n",
      "Epoch 1 Step [3380/5072] Loss: 3.6747\n",
      "Epoch 1 Step [3400/5072] Loss: 3.4166\n",
      "Epoch 1 Step [3420/5072] Loss: 3.5475\n",
      "Epoch 1 Step [3440/5072] Loss: 3.7836\n",
      "Epoch 1 Step [3460/5072] Loss: 3.8467\n",
      "Epoch 1 Step [3480/5072] Loss: 3.7162\n",
      "Epoch 1 Step [3500/5072] Loss: 3.5911\n",
      "Epoch 1 Step [3520/5072] Loss: 3.6265\n",
      "Epoch 1 Step [3540/5072] Loss: 3.8170\n",
      "Epoch 1 Step [3560/5072] Loss: 3.9161\n",
      "Epoch 1 Step [3580/5072] Loss: 3.7834\n",
      "Epoch 1 Step [3600/5072] Loss: 3.5455\n",
      "Epoch 1 Step [3620/5072] Loss: 3.7903\n",
      "Epoch 1 Step [3640/5072] Loss: 3.6448\n",
      "Epoch 1 Step [3660/5072] Loss: 3.5334\n",
      "Epoch 1 Step [3680/5072] Loss: 3.4563\n",
      "Epoch 1 Step [3700/5072] Loss: 3.6710\n",
      "Epoch 1 Step [3720/5072] Loss: 3.8638\n",
      "Epoch 1 Step [3740/5072] Loss: 3.6519\n",
      "Epoch 1 Step [3760/5072] Loss: 3.7413\n",
      "Epoch 1 Step [3780/5072] Loss: 3.6419\n",
      "Epoch 1 Step [3800/5072] Loss: 3.5768\n",
      "Epoch 1 Step [3820/5072] Loss: 3.6679\n",
      "Epoch 1 Step [3840/5072] Loss: 3.4297\n",
      "Epoch 1 Step [3860/5072] Loss: 3.5858\n",
      "Epoch 1 Step [3880/5072] Loss: 3.5901\n",
      "Epoch 1 Step [3900/5072] Loss: 3.7052\n",
      "Epoch 1 Step [3920/5072] Loss: 3.5089\n",
      "Epoch 1 Step [3940/5072] Loss: 3.5416\n",
      "Epoch 1 Step [3960/5072] Loss: 3.7534\n",
      "Epoch 1 Step [3980/5072] Loss: 3.7718\n",
      "Epoch 1 Step [4000/5072] Loss: 3.4299\n",
      "Epoch 1 Step [4020/5072] Loss: 3.8142\n",
      "Epoch 1 Step [4040/5072] Loss: 3.5178\n",
      "Epoch 1 Step [4060/5072] Loss: 3.7583\n",
      "Epoch 1 Step [4080/5072] Loss: 3.6681\n",
      "Epoch 1 Step [4100/5072] Loss: 3.6334\n",
      "Epoch 1 Step [4120/5072] Loss: 3.3868\n",
      "Epoch 1 Step [4140/5072] Loss: 3.3639\n",
      "Epoch 1 Step [4160/5072] Loss: 3.7717\n",
      "Epoch 1 Step [4180/5072] Loss: 3.8140\n",
      "Epoch 1 Step [4200/5072] Loss: 3.7482\n",
      "Epoch 1 Step [4220/5072] Loss: 3.7925\n",
      "Epoch 1 Step [4240/5072] Loss: 3.5806\n",
      "Epoch 1 Step [4260/5072] Loss: 3.7191\n",
      "Epoch 1 Step [4280/5072] Loss: 3.6927\n",
      "Epoch 1 Step [4300/5072] Loss: 3.3382\n",
      "Epoch 1 Step [4320/5072] Loss: 3.6311\n",
      "Epoch 1 Step [4340/5072] Loss: 3.6029\n",
      "Epoch 1 Step [4360/5072] Loss: 3.7681\n",
      "Epoch 1 Step [4380/5072] Loss: 3.6665\n",
      "Epoch 1 Step [4400/5072] Loss: 3.5514\n",
      "Epoch 1 Step [4420/5072] Loss: 3.8048\n",
      "Epoch 1 Step [4440/5072] Loss: 3.7381\n",
      "Epoch 1 Step [4460/5072] Loss: 3.7821\n",
      "Epoch 1 Step [4480/5072] Loss: 3.7914\n",
      "Epoch 1 Step [4500/5072] Loss: 3.5281\n",
      "Epoch 1 Step [4520/5072] Loss: 3.4720\n",
      "Epoch 1 Step [4540/5072] Loss: 3.6648\n",
      "Epoch 1 Step [4560/5072] Loss: 3.4507\n",
      "Epoch 1 Step [4580/5072] Loss: 3.6274\n",
      "Epoch 1 Step [4600/5072] Loss: 3.6487\n",
      "Epoch 1 Step [4620/5072] Loss: 3.5699\n",
      "Epoch 1 Step [4640/5072] Loss: 3.9143\n",
      "Epoch 1 Step [4660/5072] Loss: 3.6606\n",
      "Epoch 1 Step [4680/5072] Loss: 3.8165\n",
      "Epoch 1 Step [4700/5072] Loss: 3.4614\n",
      "Epoch 1 Step [4720/5072] Loss: 3.5366\n",
      "Epoch 1 Step [4740/5072] Loss: 3.6112\n",
      "Epoch 1 Step [4760/5072] Loss: 3.7125\n",
      "Epoch 1 Step [4780/5072] Loss: 3.4366\n",
      "Epoch 1 Step [4800/5072] Loss: 3.4689\n",
      "Epoch 1 Step [4820/5072] Loss: 3.8537\n",
      "Epoch 1 Step [4840/5072] Loss: 3.4662\n",
      "Epoch 1 Step [4860/5072] Loss: 3.3232\n",
      "Epoch 1 Step [4880/5072] Loss: 3.5961\n",
      "Epoch 1 Step [4900/5072] Loss: 3.7060\n",
      "Epoch 1 Step [4920/5072] Loss: 3.5625\n",
      "Epoch 1 Step [4940/5072] Loss: 3.7748\n",
      "Epoch 1 Step [4960/5072] Loss: 3.6531\n",
      "Epoch 1 Step [4980/5072] Loss: 3.5899\n",
      "Epoch 1 Step [5000/5072] Loss: 3.4140\n",
      "Epoch 1 Step [5020/5072] Loss: 3.7598\n",
      "Epoch 1 Step [5040/5072] Loss: 3.6054\n",
      "Epoch 1 Step [5060/5072] Loss: 3.5264\n",
      "Epoch 1 Finished | Acc: 8.52% | Loss: 3.7008\n",
      "Model saved as trained_on_all_classes.pkl\n"
     ]
    }
   ],
   "source": [
    "# 1. Load ENTIRE dataset\n",
    "full_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=None)\n",
    "\n",
    "# 2. Get Persistent Split (Will create 'pretrain_full_train.pkl' and 'pretrain_full_test.pkl')\n",
    "full_train, full_test = get_persistent_splits(full_dataset, 0.8, \"pretrain_full\")\n",
    "\n",
    "train_loader = DataLoader(full_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 3. Initialize Model\n",
    "cnn = CNN(cnn_config, MAX_POOL, (3, FRAME_SIZE, FRAME_SIZE), EMBEDDING_DIM)\n",
    "model = CNNLSTM(cnn, len(full_dataset.classes), LSTM_HIDDEN, LSTM_LAYERS).to(device)\n",
    "\n",
    "# 4. Train\n",
    "print(f\"Pre-training on {len(full_dataset.classes)} classes...\")\n",
    "train(model, epochs=1, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device, use_weighted_loss=USE_WEIGHTED_LOSS)\n",
    "\n",
    "# 5. Save Master Model\n",
    "save_model(model, \"trained_on_all_classes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767504de",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Set from pretrain_full_test.pkl...\n",
      "Evaluating on 47 classes.\n",
      "Loading Master Model from trained_on_all_classes_v3.pkl...\n",
      "Running Inference...\n",
      "\n",
      "========================================\n",
      "MASTER MODEL CLASSIFICATION REPORT\n",
      "========================================\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          clap       0.00      0.00      0.00        29\n",
      "         climb       0.17      0.18      0.17        17\n",
      "  climb_stairs       0.50      0.17      0.25        18\n",
      "          dive       0.75      0.09      0.16        34\n",
      "    draw_sword       0.19      0.42      0.26        12\n",
      "       dribble       0.32      0.72      0.44        29\n",
      "         drink       0.00      0.00      0.00        32\n",
      "           eat       0.00      0.00      0.00        25\n",
      "    fall_floor       0.50      0.04      0.07        25\n",
      "       fencing       0.89      0.40      0.55        20\n",
      "     flic_flac       0.00      0.00      0.00        20\n",
      "          golf       0.29      0.41      0.34        27\n",
      "     handstand       0.43      0.12      0.19        24\n",
      "           hit       0.22      0.08      0.11        26\n",
      "           hug       0.32      0.33      0.33        27\n",
      "          jump       0.22      0.17      0.20        23\n",
      "          kick       0.00      0.00      0.00        20\n",
      "     kick_ball       0.30      0.28      0.29        29\n",
      "          kiss       0.00      0.00      0.00        21\n",
      "         laugh       0.28      0.44      0.34        25\n",
      "          pick       0.00      0.00      0.00        16\n",
      "          pour       0.21      0.35      0.26        20\n",
      "        pullup       0.67      0.32      0.43        25\n",
      "         punch       0.38      0.32      0.35        25\n",
      "          push       0.10      0.05      0.07        20\n",
      "        pushup       0.43      0.24      0.31        25\n",
      "     ride_bike       0.10      0.10      0.10        21\n",
      "    ride_horse       0.07      0.04      0.05        23\n",
      "           run       0.00      0.00      0.00        38\n",
      "   shake_hands       0.21      0.26      0.23        42\n",
      "    shoot_ball       0.35      0.58      0.44        24\n",
      "     shoot_bow       0.39      0.37      0.38        30\n",
      "     shoot_gun       1.00      0.14      0.25        14\n",
      "           sit       0.10      0.03      0.05        33\n",
      "         situp       0.27      0.42      0.33        19\n",
      "         smile       0.33      0.10      0.15        21\n",
      "         smoke       0.40      0.23      0.29        26\n",
      "    somersault       0.26      0.54      0.35        26\n",
      "         stand       0.00      0.00      0.00        29\n",
      "swing_baseball       0.33      0.48      0.39        27\n",
      "         sword       0.50      0.13      0.21        23\n",
      "sword_exercise       0.38      0.10      0.16        29\n",
      "          talk       0.60      0.15      0.24        20\n",
      "         throw       0.57      0.29      0.38        28\n",
      "          turn       0.21      0.11      0.14        55\n",
      "          walk       0.16      0.75      0.27       100\n",
      "          wave       1.00      0.04      0.07        27\n",
      "\n",
      "      accuracy                           0.24      1269\n",
      "     macro avg       0.31      0.21      0.20      1269\n",
      "  weighted avg       0.29      0.24      0.21      1269\n",
      "\n",
      "Total Accuracy: 24.11%\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from video_recognition import load_model \n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_FILE = \"trained_on_all_classes_v3.pkl\"\n",
    "TEST_SET_FILE = \"pretrain_full_test.pkl\"\n",
    "BATCH_SIZE = 1 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Test Set from {TEST_SET_FILE}...\")\n",
    "with open(TEST_SET_FILE, 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "# Get the list of ALL 51 classes\n",
    "CLASSES = test_set.dataset.classes \n",
    "print(f\"Evaluating on {len(CLASSES)} classes.\")\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Loading Master Model from {MODEL_FILE}...\")\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 4. REPORTING ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MASTER MODEL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- FIX: Explicitly pass the range of labels to handle missing classes ---\n",
    "all_possible_labels = range(len(CLASSES))\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    labels=all_possible_labels,  # <--- FIX IS HERE\n",
    "    target_names=CLASSES, \n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Total Accuracy: {acc*100:.2f}%\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde071b",
   "metadata": {},
   "source": [
    "## Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Set from pretrain_full_train.pkl...\n",
      "Loading Master Model from trained_on_all_classes_v3.pkl...\n",
      "Resuming Master Model training for 2 more epochs...\n",
      "Calculating class weights for Weighted Loss...\n",
      "Class Weights (Shape torch.Size([47])): tensor([1.0685, 1.1859, 1.1480, 1.1604, 1.1859, 0.9303, 0.8175, 1.3002, 0.9722,\n",
      "        1.1241, 1.2404, 1.3835, 1.2125, 1.0685, 1.1859, 0.8431, 0.9810, 1.0900,\n",
      "        1.3323, 1.0477, 1.1991, 1.2548, 1.3660, 1.0685, 1.1241, 1.3835, 1.3160,\n",
      "        1.1604, 0.5563, 0.8993, 1.0086, 1.3160, 1.2125, 0.9900, 1.2548, 1.3323,\n",
      "        1.3002, 0.9466, 0.8633, 0.9303, 1.0376, 1.1012, 1.0791, 1.4583, 0.5833,\n",
      "        0.2409, 1.4015])\n",
      "Epoch 1 Step [20/5072] Loss: 2.8876\n",
      "Epoch 1 Step [40/5072] Loss: 2.9556\n",
      "Epoch 1 Step [60/5072] Loss: 2.1816\n",
      "Epoch 1 Step [80/5072] Loss: 2.7428\n",
      "Epoch 1 Step [100/5072] Loss: 2.6394\n",
      "Epoch 1 Step [120/5072] Loss: 2.9346\n",
      "Epoch 1 Step [140/5072] Loss: 3.1019\n",
      "Epoch 1 Step [160/5072] Loss: 2.7486\n",
      "Epoch 1 Step [180/5072] Loss: 2.5492\n",
      "Epoch 1 Step [200/5072] Loss: 2.9734\n",
      "Epoch 1 Step [220/5072] Loss: 3.0075\n",
      "Epoch 1 Step [240/5072] Loss: 3.2308\n",
      "Epoch 1 Step [260/5072] Loss: 3.0485\n",
      "Epoch 1 Step [280/5072] Loss: 2.9076\n",
      "Epoch 1 Step [300/5072] Loss: 2.6273\n",
      "Epoch 1 Step [320/5072] Loss: 2.7526\n",
      "Epoch 1 Step [340/5072] Loss: 2.8714\n",
      "Epoch 1 Step [360/5072] Loss: 3.0122\n",
      "Epoch 1 Step [380/5072] Loss: 2.5303\n",
      "Epoch 1 Step [400/5072] Loss: 2.7353\n",
      "Epoch 1 Step [420/5072] Loss: 3.1636\n",
      "Epoch 1 Step [440/5072] Loss: 3.4643\n",
      "Epoch 1 Step [460/5072] Loss: 2.7660\n",
      "Epoch 1 Step [480/5072] Loss: 2.7560\n",
      "Epoch 1 Step [500/5072] Loss: 2.7028\n",
      "Epoch 1 Step [520/5072] Loss: 2.8142\n",
      "Epoch 1 Step [540/5072] Loss: 2.8484\n",
      "Epoch 1 Step [560/5072] Loss: 2.7859\n",
      "Epoch 1 Step [580/5072] Loss: 2.9326\n",
      "Epoch 1 Step [600/5072] Loss: 2.9065\n",
      "Epoch 1 Step [620/5072] Loss: 2.7141\n",
      "Epoch 1 Step [640/5072] Loss: 2.2678\n",
      "Epoch 1 Step [660/5072] Loss: 2.8334\n",
      "Epoch 1 Step [680/5072] Loss: 2.6877\n",
      "Epoch 1 Step [700/5072] Loss: 3.0395\n",
      "Epoch 1 Step [720/5072] Loss: 2.6253\n",
      "Epoch 1 Step [740/5072] Loss: 2.8693\n",
      "Epoch 1 Step [760/5072] Loss: 2.4732\n",
      "Epoch 1 Step [780/5072] Loss: 2.5120\n",
      "Epoch 1 Step [800/5072] Loss: 2.5366\n",
      "Epoch 1 Step [820/5072] Loss: 2.5029\n",
      "Epoch 1 Step [840/5072] Loss: 2.4159\n",
      "Epoch 1 Step [860/5072] Loss: 2.7613\n",
      "Epoch 1 Step [880/5072] Loss: 2.7494\n",
      "Epoch 1 Step [900/5072] Loss: 2.8309\n",
      "Epoch 1 Step [920/5072] Loss: 3.1292\n",
      "Epoch 1 Step [940/5072] Loss: 2.8122\n",
      "Epoch 1 Step [960/5072] Loss: 2.3616\n",
      "Epoch 1 Step [980/5072] Loss: 2.5566\n",
      "Epoch 1 Step [1000/5072] Loss: 2.8413\n",
      "Epoch 1 Step [1020/5072] Loss: 2.5829\n",
      "Epoch 1 Step [1040/5072] Loss: 2.6448\n",
      "Epoch 1 Step [1060/5072] Loss: 2.9556\n",
      "Epoch 1 Step [1080/5072] Loss: 2.2880\n",
      "Epoch 1 Step [1100/5072] Loss: 3.0552\n",
      "Epoch 1 Step [1120/5072] Loss: 3.0530\n",
      "Epoch 1 Step [1140/5072] Loss: 2.6432\n",
      "Epoch 1 Step [1160/5072] Loss: 2.8529\n",
      "Epoch 1 Step [1180/5072] Loss: 2.1358\n",
      "Epoch 1 Step [1200/5072] Loss: 3.0481\n",
      "Epoch 1 Step [1220/5072] Loss: 2.7700\n",
      "Epoch 1 Step [1240/5072] Loss: 2.3855\n",
      "Epoch 1 Step [1260/5072] Loss: 2.6515\n",
      "Epoch 1 Step [1280/5072] Loss: 2.4598\n",
      "Epoch 1 Step [1300/5072] Loss: 2.4472\n",
      "Epoch 1 Step [1320/5072] Loss: 2.5322\n",
      "Epoch 1 Step [1340/5072] Loss: 2.8726\n",
      "Epoch 1 Step [1360/5072] Loss: 2.6722\n",
      "Epoch 1 Step [1380/5072] Loss: 2.3148\n",
      "Epoch 1 Step [1400/5072] Loss: 3.2458\n",
      "Epoch 1 Step [1420/5072] Loss: 2.7960\n",
      "Epoch 1 Step [1440/5072] Loss: 2.4713\n",
      "Epoch 1 Step [1460/5072] Loss: 2.5486\n",
      "Epoch 1 Step [1480/5072] Loss: 2.6902\n",
      "Epoch 1 Step [1500/5072] Loss: 2.5092\n",
      "Epoch 1 Step [1520/5072] Loss: 2.6687\n",
      "Epoch 1 Step [1540/5072] Loss: 2.8179\n",
      "Epoch 1 Step [1560/5072] Loss: 2.4938\n",
      "Epoch 1 Step [1580/5072] Loss: 2.9508\n",
      "Epoch 1 Step [1600/5072] Loss: 2.2090\n",
      "Epoch 1 Step [1620/5072] Loss: 2.8234\n",
      "Epoch 1 Step [1640/5072] Loss: 3.1984\n",
      "Epoch 1 Step [1660/5072] Loss: 2.5791\n",
      "Epoch 1 Step [1680/5072] Loss: 2.6578\n",
      "Epoch 1 Step [1700/5072] Loss: 2.8275\n",
      "Epoch 1 Step [1720/5072] Loss: 2.4102\n",
      "Epoch 1 Step [1740/5072] Loss: 2.7902\n",
      "Epoch 1 Step [1760/5072] Loss: 3.1748\n",
      "Epoch 1 Step [1780/5072] Loss: 2.6867\n",
      "Epoch 1 Step [1800/5072] Loss: 2.9027\n",
      "Epoch 1 Step [1820/5072] Loss: 2.8976\n",
      "Epoch 1 Step [1840/5072] Loss: 2.5251\n",
      "Epoch 1 Step [1860/5072] Loss: 2.7305\n",
      "Epoch 1 Step [1880/5072] Loss: 2.4397\n",
      "Epoch 1 Step [1900/5072] Loss: 2.9648\n",
      "Epoch 1 Step [1920/5072] Loss: 2.8176\n",
      "Epoch 1 Step [1940/5072] Loss: 2.6485\n",
      "Epoch 1 Step [1960/5072] Loss: 2.5990\n",
      "Epoch 1 Step [1980/5072] Loss: 2.6437\n",
      "Epoch 1 Step [2000/5072] Loss: 2.7653\n",
      "Epoch 1 Step [2020/5072] Loss: 2.8646\n",
      "Epoch 1 Step [2040/5072] Loss: 2.5903\n",
      "Epoch 1 Step [2060/5072] Loss: 2.5709\n",
      "Epoch 1 Step [2080/5072] Loss: 2.6670\n",
      "Epoch 1 Step [2100/5072] Loss: 2.8935\n",
      "Epoch 1 Step [2120/5072] Loss: 2.4764\n",
      "Epoch 1 Step [2140/5072] Loss: 2.6583\n",
      "Epoch 1 Step [2160/5072] Loss: 2.8352\n",
      "Epoch 1 Step [2180/5072] Loss: 2.8211\n",
      "Epoch 1 Step [2200/5072] Loss: 3.0273\n",
      "Epoch 1 Step [2220/5072] Loss: 2.6913\n",
      "Epoch 1 Step [2240/5072] Loss: 2.5238\n",
      "Epoch 1 Step [2260/5072] Loss: 2.8952\n",
      "Epoch 1 Step [2280/5072] Loss: 2.6592\n",
      "Epoch 1 Step [2300/5072] Loss: 2.8196\n",
      "Epoch 1 Step [2320/5072] Loss: 2.9832\n",
      "Epoch 1 Step [2340/5072] Loss: 2.8433\n",
      "Epoch 1 Step [2360/5072] Loss: 2.5223\n",
      "Epoch 1 Step [2380/5072] Loss: 2.5265\n",
      "Epoch 1 Step [2400/5072] Loss: 2.4508\n",
      "Epoch 1 Step [2420/5072] Loss: 2.8231\n",
      "Epoch 1 Step [2440/5072] Loss: 2.7751\n",
      "Epoch 1 Step [2460/5072] Loss: 2.5966\n",
      "Epoch 1 Step [2480/5072] Loss: 3.3678\n",
      "Epoch 1 Step [2500/5072] Loss: 2.6662\n",
      "Epoch 1 Step [2520/5072] Loss: 2.9909\n",
      "Epoch 1 Step [2540/5072] Loss: 2.5850\n",
      "Epoch 1 Step [2560/5072] Loss: 2.7133\n",
      "Epoch 1 Step [2580/5072] Loss: 2.8436\n",
      "Epoch 1 Step [2600/5072] Loss: 2.7923\n",
      "Epoch 1 Step [2620/5072] Loss: 3.2753\n",
      "Epoch 1 Step [2640/5072] Loss: 2.9194\n",
      "Epoch 1 Step [2660/5072] Loss: 2.7502\n",
      "Epoch 1 Step [2680/5072] Loss: 2.9630\n",
      "Epoch 1 Step [2700/5072] Loss: 2.9915\n",
      "Epoch 1 Step [2720/5072] Loss: 2.5816\n",
      "Epoch 1 Step [2740/5072] Loss: 3.2673\n",
      "Epoch 1 Step [2760/5072] Loss: 2.9402\n",
      "Epoch 1 Step [2780/5072] Loss: 2.9130\n",
      "Epoch 1 Step [2800/5072] Loss: 2.9967\n",
      "Epoch 1 Step [2820/5072] Loss: 2.7826\n",
      "Epoch 1 Step [2840/5072] Loss: 2.7285\n",
      "Epoch 1 Step [2860/5072] Loss: 3.0296\n",
      "Epoch 1 Step [2880/5072] Loss: 2.3967\n",
      "Epoch 1 Step [2900/5072] Loss: 2.7807\n",
      "Epoch 1 Step [2920/5072] Loss: 2.6083\n",
      "Epoch 1 Step [2940/5072] Loss: 2.7331\n",
      "Epoch 1 Step [2960/5072] Loss: 2.7746\n",
      "Epoch 1 Step [2980/5072] Loss: 2.7472\n",
      "Epoch 1 Step [3000/5072] Loss: 2.9379\n",
      "Epoch 1 Step [3020/5072] Loss: 2.6729\n",
      "Epoch 1 Step [3040/5072] Loss: 3.2131\n",
      "Epoch 1 Step [3060/5072] Loss: 2.9363\n",
      "Epoch 1 Step [3080/5072] Loss: 2.9675\n",
      "Epoch 1 Step [3100/5072] Loss: 2.8449\n",
      "Epoch 1 Step [3120/5072] Loss: 2.9314\n",
      "Epoch 1 Step [3140/5072] Loss: 2.5596\n",
      "Epoch 1 Step [3160/5072] Loss: 2.5417\n",
      "Epoch 1 Step [3180/5072] Loss: 2.6129\n",
      "Epoch 1 Step [3200/5072] Loss: 2.8896\n",
      "Epoch 1 Step [3220/5072] Loss: 2.4485\n",
      "Epoch 1 Step [3240/5072] Loss: 2.9199\n",
      "Epoch 1 Step [3260/5072] Loss: 3.1409\n",
      "Epoch 1 Step [3280/5072] Loss: 2.7866\n",
      "Epoch 1 Step [3300/5072] Loss: 2.7297\n",
      "Epoch 1 Step [3320/5072] Loss: 2.5196\n",
      "Epoch 1 Step [3340/5072] Loss: 2.4981\n",
      "Epoch 1 Step [3360/5072] Loss: 2.5210\n",
      "Epoch 1 Step [3380/5072] Loss: 3.0564\n",
      "Epoch 1 Step [3400/5072] Loss: 2.4826\n",
      "Epoch 1 Step [3420/5072] Loss: 2.6626\n",
      "Epoch 1 Step [3440/5072] Loss: 2.8197\n",
      "Epoch 1 Step [3460/5072] Loss: 2.9478\n",
      "Epoch 1 Step [3480/5072] Loss: 2.7397\n",
      "Epoch 1 Step [3500/5072] Loss: 2.5245\n",
      "Epoch 1 Step [3520/5072] Loss: 2.7826\n",
      "Epoch 1 Step [3540/5072] Loss: 2.7054\n",
      "Epoch 1 Step [3560/5072] Loss: 2.6481\n",
      "Epoch 1 Step [3580/5072] Loss: 2.4831\n",
      "Epoch 1 Step [3600/5072] Loss: 2.6277\n",
      "Epoch 1 Step [3620/5072] Loss: 2.7647\n",
      "Epoch 1 Step [3640/5072] Loss: 2.2086\n",
      "Epoch 1 Step [3660/5072] Loss: 2.6703\n",
      "Epoch 1 Step [3680/5072] Loss: 2.7993\n",
      "Epoch 1 Step [3700/5072] Loss: 2.7023\n",
      "Epoch 1 Step [3720/5072] Loss: 2.4283\n",
      "Epoch 1 Step [3740/5072] Loss: 2.7671\n",
      "Epoch 1 Step [3760/5072] Loss: 2.5364\n",
      "Epoch 1 Step [3780/5072] Loss: 3.0964\n",
      "Epoch 1 Step [3800/5072] Loss: 2.9378\n",
      "Epoch 1 Step [3820/5072] Loss: 3.0447\n",
      "Epoch 1 Step [3840/5072] Loss: 2.8741\n",
      "Epoch 1 Step [3860/5072] Loss: 2.7127\n",
      "Epoch 1 Step [3880/5072] Loss: 3.0768\n",
      "Epoch 1 Step [3900/5072] Loss: 3.0191\n",
      "Epoch 1 Step [3920/5072] Loss: 3.0077\n",
      "Epoch 1 Step [3940/5072] Loss: 3.2550\n",
      "Epoch 1 Step [3960/5072] Loss: 2.6766\n",
      "Epoch 1 Step [3980/5072] Loss: 3.3871\n",
      "Epoch 1 Step [4000/5072] Loss: 3.1872\n",
      "Epoch 1 Step [4020/5072] Loss: 2.9620\n",
      "Epoch 1 Step [4040/5072] Loss: 2.7182\n",
      "Epoch 1 Step [4060/5072] Loss: 2.5501\n",
      "Epoch 1 Step [4080/5072] Loss: 2.6909\n",
      "Epoch 1 Step [4100/5072] Loss: 2.3339\n",
      "Epoch 1 Step [4120/5072] Loss: 2.7057\n",
      "Epoch 1 Step [4140/5072] Loss: 2.6554\n",
      "Epoch 1 Step [4160/5072] Loss: 2.4852\n",
      "Epoch 1 Step [4180/5072] Loss: 2.4555\n",
      "Epoch 1 Step [4200/5072] Loss: 2.6447\n",
      "Epoch 1 Step [4220/5072] Loss: 3.2111\n",
      "Epoch 1 Step [4240/5072] Loss: 2.8057\n",
      "Epoch 1 Step [4260/5072] Loss: 3.2619\n",
      "Epoch 1 Step [4280/5072] Loss: 2.5021\n",
      "Epoch 1 Step [4300/5072] Loss: 2.6235\n",
      "Epoch 1 Step [4320/5072] Loss: 2.7504\n",
      "Epoch 1 Step [4340/5072] Loss: 2.7900\n",
      "Epoch 1 Step [4360/5072] Loss: 2.8047\n",
      "Epoch 1 Step [4380/5072] Loss: 3.1167\n",
      "Epoch 1 Step [4400/5072] Loss: 2.9137\n",
      "Epoch 1 Step [4420/5072] Loss: 2.9274\n",
      "Epoch 1 Step [4440/5072] Loss: 3.1567\n",
      "Epoch 1 Step [4460/5072] Loss: 2.9643\n",
      "Epoch 1 Step [4480/5072] Loss: 2.8824\n",
      "Epoch 1 Step [4500/5072] Loss: 2.4633\n",
      "Epoch 1 Step [4520/5072] Loss: 3.1391\n",
      "Epoch 1 Step [4540/5072] Loss: 2.8229\n",
      "Epoch 1 Step [4560/5072] Loss: 2.8201\n",
      "Epoch 1 Step [4580/5072] Loss: 2.3948\n",
      "Epoch 1 Step [4600/5072] Loss: 2.3232\n",
      "Epoch 1 Step [4620/5072] Loss: 2.6776\n",
      "Epoch 1 Step [4640/5072] Loss: 2.5781\n",
      "Epoch 1 Step [4660/5072] Loss: 2.4539\n",
      "Epoch 1 Step [4680/5072] Loss: 2.5489\n",
      "Epoch 1 Step [4700/5072] Loss: 2.9083\n",
      "Epoch 1 Step [4720/5072] Loss: 3.1686\n",
      "Epoch 1 Step [4740/5072] Loss: 2.7976\n",
      "Epoch 1 Step [4760/5072] Loss: 2.7282\n",
      "Epoch 1 Step [4780/5072] Loss: 2.8640\n",
      "Epoch 1 Step [4800/5072] Loss: 2.7127\n",
      "Epoch 1 Step [4820/5072] Loss: 2.5713\n",
      "Epoch 1 Step [4840/5072] Loss: 2.8193\n",
      "Epoch 1 Step [4860/5072] Loss: 2.8738\n",
      "Epoch 1 Step [4880/5072] Loss: 2.4771\n",
      "Epoch 1 Step [4900/5072] Loss: 2.4886\n",
      "Epoch 1 Step [4920/5072] Loss: 2.4802\n",
      "Epoch 1 Step [4940/5072] Loss: 2.7003\n",
      "Epoch 1 Step [4960/5072] Loss: 2.6598\n",
      "Epoch 1 Step [4980/5072] Loss: 2.4960\n",
      "Epoch 1 Step [5000/5072] Loss: 2.6210\n",
      "Epoch 1 Step [5020/5072] Loss: 2.6051\n",
      "Epoch 1 Step [5040/5072] Loss: 3.1382\n",
      "Epoch 1 Step [5060/5072] Loss: 3.0536\n",
      "Epoch 1 Finished | Acc: 28.73% | Loss: 2.7609\n",
      "Epoch 2 Step [20/5072] Loss: 2.8814\n",
      "Epoch 2 Step [40/5072] Loss: 2.7007\n",
      "Epoch 2 Step [60/5072] Loss: 2.4350\n",
      "Epoch 2 Step [80/5072] Loss: 3.0095\n",
      "Epoch 2 Step [100/5072] Loss: 2.9779\n",
      "Epoch 2 Step [120/5072] Loss: 2.6063\n",
      "Epoch 2 Step [140/5072] Loss: 2.7620\n",
      "Epoch 2 Step [160/5072] Loss: 3.0232\n",
      "Epoch 2 Step [180/5072] Loss: 2.6261\n",
      "Epoch 2 Step [200/5072] Loss: 2.7458\n",
      "Epoch 2 Step [220/5072] Loss: 2.9320\n",
      "Epoch 2 Step [240/5072] Loss: 2.8756\n",
      "Epoch 2 Step [260/5072] Loss: 2.7119\n",
      "Epoch 2 Step [280/5072] Loss: 2.5873\n",
      "Epoch 2 Step [300/5072] Loss: 2.3647\n",
      "Epoch 2 Step [320/5072] Loss: 2.6637\n",
      "Epoch 2 Step [340/5072] Loss: 2.8389\n",
      "Epoch 2 Step [360/5072] Loss: 3.0794\n",
      "Epoch 2 Step [380/5072] Loss: 2.4696\n",
      "Epoch 2 Step [400/5072] Loss: 2.9152\n",
      "Epoch 2 Step [420/5072] Loss: 3.0631\n",
      "Epoch 2 Step [440/5072] Loss: 2.6003\n",
      "Epoch 2 Step [460/5072] Loss: 2.6969\n",
      "Epoch 2 Step [480/5072] Loss: 2.6893\n",
      "Epoch 2 Step [500/5072] Loss: 2.9003\n",
      "Epoch 2 Step [520/5072] Loss: 2.6955\n",
      "Epoch 2 Step [540/5072] Loss: 2.6895\n",
      "Epoch 2 Step [560/5072] Loss: 2.9863\n",
      "Epoch 2 Step [580/5072] Loss: 2.4785\n",
      "Epoch 2 Step [600/5072] Loss: 2.7955\n",
      "Epoch 2 Step [620/5072] Loss: 2.5668\n",
      "Epoch 2 Step [640/5072] Loss: 2.4223\n",
      "Epoch 2 Step [660/5072] Loss: 3.3180\n",
      "Epoch 2 Step [680/5072] Loss: 2.8024\n",
      "Epoch 2 Step [700/5072] Loss: 2.3373\n",
      "Epoch 2 Step [720/5072] Loss: 2.3936\n",
      "Epoch 2 Step [740/5072] Loss: 3.0035\n",
      "Epoch 2 Step [760/5072] Loss: 2.6449\n",
      "Epoch 2 Step [780/5072] Loss: 3.0736\n",
      "Epoch 2 Step [800/5072] Loss: 2.5902\n",
      "Epoch 2 Step [820/5072] Loss: 2.8197\n",
      "Epoch 2 Step [840/5072] Loss: 3.0026\n",
      "Epoch 2 Step [860/5072] Loss: 2.8400\n",
      "Epoch 2 Step [880/5072] Loss: 2.4370\n",
      "Epoch 2 Step [900/5072] Loss: 3.4821\n",
      "Epoch 2 Step [920/5072] Loss: 2.6947\n",
      "Epoch 2 Step [940/5072] Loss: 2.9033\n",
      "Epoch 2 Step [960/5072] Loss: 2.8032\n",
      "Epoch 2 Step [980/5072] Loss: 2.5571\n",
      "Epoch 2 Step [1000/5072] Loss: 2.7290\n",
      "Epoch 2 Step [1020/5072] Loss: 2.9369\n",
      "Epoch 2 Step [1040/5072] Loss: 2.8691\n",
      "Epoch 2 Step [1060/5072] Loss: 2.9458\n",
      "Epoch 2 Step [1080/5072] Loss: 2.5237\n",
      "Epoch 2 Step [1100/5072] Loss: 2.3972\n",
      "Epoch 2 Step [1120/5072] Loss: 2.5627\n",
      "Epoch 2 Step [1140/5072] Loss: 2.3744\n",
      "Epoch 2 Step [1160/5072] Loss: 2.8440\n",
      "Epoch 2 Step [1180/5072] Loss: 3.0915\n",
      "Epoch 2 Step [1200/5072] Loss: 2.6929\n",
      "Epoch 2 Step [1220/5072] Loss: 2.8618\n",
      "Epoch 2 Step [1240/5072] Loss: 2.4669\n",
      "Epoch 2 Step [1260/5072] Loss: 2.5972\n",
      "Epoch 2 Step [1280/5072] Loss: 2.8886\n",
      "Epoch 2 Step [1300/5072] Loss: 2.3945\n",
      "Epoch 2 Step [1320/5072] Loss: 2.4731\n",
      "Epoch 2 Step [1340/5072] Loss: 3.4507\n",
      "Epoch 2 Step [1360/5072] Loss: 2.8320\n",
      "Epoch 2 Step [1380/5072] Loss: 2.8000\n",
      "Epoch 2 Step [1400/5072] Loss: 2.3751\n",
      "Epoch 2 Step [1420/5072] Loss: 2.8147\n",
      "Epoch 2 Step [1440/5072] Loss: 2.8635\n",
      "Epoch 2 Step [1460/5072] Loss: 2.9371\n",
      "Epoch 2 Step [1480/5072] Loss: 2.7402\n",
      "Epoch 2 Step [1500/5072] Loss: 3.0700\n",
      "Epoch 2 Step [1520/5072] Loss: 2.5133\n",
      "Epoch 2 Step [1540/5072] Loss: 2.4506\n",
      "Epoch 2 Step [1560/5072] Loss: 2.5155\n",
      "Epoch 2 Step [1580/5072] Loss: 2.8515\n",
      "Epoch 2 Step [1600/5072] Loss: 2.9829\n",
      "Epoch 2 Step [1620/5072] Loss: 2.5100\n",
      "Epoch 2 Step [1640/5072] Loss: 3.0344\n",
      "Epoch 2 Step [1660/5072] Loss: 2.6868\n",
      "Epoch 2 Step [1680/5072] Loss: 2.2466\n",
      "Epoch 2 Step [1700/5072] Loss: 2.6865\n",
      "Epoch 2 Step [1720/5072] Loss: 2.5858\n",
      "Epoch 2 Step [1740/5072] Loss: 2.8431\n",
      "Epoch 2 Step [1760/5072] Loss: 2.4817\n",
      "Epoch 2 Step [1780/5072] Loss: 3.1808\n",
      "Epoch 2 Step [1800/5072] Loss: 3.0022\n",
      "Epoch 2 Step [1820/5072] Loss: 2.7622\n",
      "Epoch 2 Step [1840/5072] Loss: 2.2562\n",
      "Epoch 2 Step [1860/5072] Loss: 2.9286\n",
      "Epoch 2 Step [1880/5072] Loss: 2.8539\n",
      "Epoch 2 Step [1900/5072] Loss: 2.4908\n",
      "Epoch 2 Step [1920/5072] Loss: 2.9577\n",
      "Epoch 2 Step [1940/5072] Loss: 2.5453\n",
      "Epoch 2 Step [1960/5072] Loss: 2.4019\n",
      "Epoch 2 Step [1980/5072] Loss: 3.0083\n",
      "Epoch 2 Step [2000/5072] Loss: 2.8611\n",
      "Epoch 2 Step [2020/5072] Loss: 2.9682\n",
      "Epoch 2 Step [2040/5072] Loss: 2.7337\n",
      "Epoch 2 Step [2060/5072] Loss: 2.6693\n",
      "Epoch 2 Step [2080/5072] Loss: 2.4107\n",
      "Epoch 2 Step [2100/5072] Loss: 2.1000\n",
      "Epoch 2 Step [2120/5072] Loss: 2.7669\n",
      "Epoch 2 Step [2140/5072] Loss: 2.3766\n",
      "Epoch 2 Step [2160/5072] Loss: 2.9372\n",
      "Epoch 2 Step [2180/5072] Loss: 2.5159\n",
      "Epoch 2 Step [2200/5072] Loss: 2.9857\n",
      "Epoch 2 Step [2220/5072] Loss: 2.3809\n",
      "Epoch 2 Step [2240/5072] Loss: 2.8569\n",
      "Epoch 2 Step [2260/5072] Loss: 2.4461\n",
      "Epoch 2 Step [2280/5072] Loss: 2.7801\n",
      "Epoch 2 Step [2300/5072] Loss: 2.4480\n",
      "Epoch 2 Step [2320/5072] Loss: 2.8079\n",
      "Epoch 2 Step [2340/5072] Loss: 3.0649\n",
      "Epoch 2 Step [2360/5072] Loss: 2.8200\n",
      "Epoch 2 Step [2380/5072] Loss: 2.7584\n",
      "Epoch 2 Step [2400/5072] Loss: 2.9689\n",
      "Epoch 2 Step [2420/5072] Loss: 2.6585\n",
      "Epoch 2 Step [2440/5072] Loss: 2.7159\n",
      "Epoch 2 Step [2460/5072] Loss: 2.6731\n",
      "Epoch 2 Step [2480/5072] Loss: 2.4094\n",
      "Epoch 2 Step [2500/5072] Loss: 2.5290\n",
      "Epoch 2 Step [2520/5072] Loss: 2.6571\n",
      "Epoch 2 Step [2540/5072] Loss: 2.9078\n",
      "Epoch 2 Step [2560/5072] Loss: 2.4750\n",
      "Epoch 2 Step [2580/5072] Loss: 3.0342\n",
      "Epoch 2 Step [2600/5072] Loss: 2.7027\n",
      "Epoch 2 Step [2620/5072] Loss: 2.6371\n",
      "Epoch 2 Step [2640/5072] Loss: 2.9236\n",
      "Epoch 2 Step [2660/5072] Loss: 2.7394\n",
      "Epoch 2 Step [2680/5072] Loss: 3.0883\n",
      "Epoch 2 Step [2700/5072] Loss: 2.9532\n",
      "Epoch 2 Step [2720/5072] Loss: 2.6900\n",
      "Epoch 2 Step [2740/5072] Loss: 2.5570\n",
      "Epoch 2 Step [2760/5072] Loss: 2.5385\n",
      "Epoch 2 Step [2780/5072] Loss: 3.1570\n",
      "Epoch 2 Step [2800/5072] Loss: 2.5779\n",
      "Epoch 2 Step [2820/5072] Loss: 2.8812\n",
      "Epoch 2 Step [2840/5072] Loss: 3.2931\n",
      "Epoch 2 Step [2860/5072] Loss: 2.8587\n",
      "Epoch 2 Step [2880/5072] Loss: 2.7371\n",
      "Epoch 2 Step [2900/5072] Loss: 2.7785\n",
      "Epoch 2 Step [2920/5072] Loss: 2.9834\n",
      "Epoch 2 Step [2940/5072] Loss: 2.3612\n",
      "Epoch 2 Step [2960/5072] Loss: 2.5887\n",
      "Epoch 2 Step [2980/5072] Loss: 3.2397\n",
      "Epoch 2 Step [3000/5072] Loss: 2.9268\n",
      "Epoch 2 Step [3020/5072] Loss: 2.7339\n",
      "Epoch 2 Step [3040/5072] Loss: 2.6292\n",
      "Epoch 2 Step [3060/5072] Loss: 2.7777\n",
      "Epoch 2 Step [3080/5072] Loss: 2.7741\n",
      "Epoch 2 Step [3100/5072] Loss: 2.8292\n",
      "Epoch 2 Step [3120/5072] Loss: 2.7712\n",
      "Epoch 2 Step [3140/5072] Loss: 2.6949\n",
      "Epoch 2 Step [3160/5072] Loss: 2.6500\n",
      "Epoch 2 Step [3180/5072] Loss: 2.7961\n",
      "Epoch 2 Step [3200/5072] Loss: 3.0463\n",
      "Epoch 2 Step [3220/5072] Loss: 2.9490\n",
      "Epoch 2 Step [3240/5072] Loss: 3.0045\n",
      "Epoch 2 Step [3260/5072] Loss: 2.8302\n",
      "Epoch 2 Step [3280/5072] Loss: 2.7034\n",
      "Epoch 2 Step [3300/5072] Loss: 2.5780\n",
      "Epoch 2 Step [3320/5072] Loss: 2.8745\n",
      "Epoch 2 Step [3340/5072] Loss: 2.6986\n",
      "Epoch 2 Step [3360/5072] Loss: 3.1181\n",
      "Epoch 2 Step [3380/5072] Loss: 2.0868\n",
      "Epoch 2 Step [3400/5072] Loss: 2.9569\n",
      "Epoch 2 Step [3420/5072] Loss: 2.2317\n",
      "Epoch 2 Step [3440/5072] Loss: 2.6912\n",
      "Epoch 2 Step [3460/5072] Loss: 2.5280\n",
      "Epoch 2 Step [3480/5072] Loss: 2.5519\n",
      "Epoch 2 Step [3500/5072] Loss: 2.9428\n",
      "Epoch 2 Step [3520/5072] Loss: 2.9099\n",
      "Epoch 2 Step [3540/5072] Loss: 2.5526\n",
      "Epoch 2 Step [3560/5072] Loss: 3.4067\n",
      "Epoch 2 Step [3580/5072] Loss: 2.6501\n",
      "Epoch 2 Step [3600/5072] Loss: 2.9381\n",
      "Epoch 2 Step [3620/5072] Loss: 2.9384\n",
      "Epoch 2 Step [3640/5072] Loss: 2.1169\n",
      "Epoch 2 Step [3660/5072] Loss: 2.6538\n",
      "Epoch 2 Step [3680/5072] Loss: 2.8710\n",
      "Epoch 2 Step [3700/5072] Loss: 2.6857\n",
      "Epoch 2 Step [3720/5072] Loss: 2.6374\n",
      "Epoch 2 Step [3740/5072] Loss: 2.6629\n",
      "Epoch 2 Step [3760/5072] Loss: 2.8423\n",
      "Epoch 2 Step [3780/5072] Loss: 2.8449\n",
      "Epoch 2 Step [3800/5072] Loss: 2.8224\n",
      "Epoch 2 Step [3820/5072] Loss: 3.1194\n",
      "Epoch 2 Step [3840/5072] Loss: 2.7568\n",
      "Epoch 2 Step [3860/5072] Loss: 2.9866\n",
      "Epoch 2 Step [3880/5072] Loss: 2.7723\n",
      "Epoch 2 Step [3900/5072] Loss: 3.0829\n",
      "Epoch 2 Step [3920/5072] Loss: 3.2792\n",
      "Epoch 2 Step [3940/5072] Loss: 2.5496\n",
      "Epoch 2 Step [3960/5072] Loss: 3.1334\n",
      "Epoch 2 Step [3980/5072] Loss: 2.3940\n",
      "Epoch 2 Step [4000/5072] Loss: 2.6664\n",
      "Epoch 2 Step [4020/5072] Loss: 2.8171\n",
      "Epoch 2 Step [4040/5072] Loss: 2.5356\n",
      "Epoch 2 Step [4060/5072] Loss: 2.4764\n",
      "Epoch 2 Step [4080/5072] Loss: 2.6655\n",
      "Epoch 2 Step [4100/5072] Loss: 2.6658\n",
      "Epoch 2 Step [4120/5072] Loss: 2.9882\n",
      "Epoch 2 Step [4140/5072] Loss: 2.7651\n",
      "Epoch 2 Step [4160/5072] Loss: 2.7784\n",
      "Epoch 2 Step [4180/5072] Loss: 2.6805\n",
      "Epoch 2 Step [4200/5072] Loss: 2.2255\n",
      "Epoch 2 Step [4220/5072] Loss: 2.7422\n",
      "Epoch 2 Step [4240/5072] Loss: 2.9473\n",
      "Epoch 2 Step [4260/5072] Loss: 2.4872\n",
      "Epoch 2 Step [4280/5072] Loss: 2.7083\n",
      "Epoch 2 Step [4300/5072] Loss: 3.1149\n",
      "Epoch 2 Step [4320/5072] Loss: 2.2451\n",
      "Epoch 2 Step [4340/5072] Loss: 2.6415\n",
      "Epoch 2 Step [4360/5072] Loss: 2.4214\n",
      "Epoch 2 Step [4380/5072] Loss: 3.2145\n",
      "Epoch 2 Step [4400/5072] Loss: 2.9700\n",
      "Epoch 2 Step [4420/5072] Loss: 2.8111\n",
      "Epoch 2 Step [4440/5072] Loss: 2.3856\n",
      "Epoch 2 Step [4460/5072] Loss: 2.3197\n",
      "Epoch 2 Step [4480/5072] Loss: 2.6707\n",
      "Epoch 2 Step [4500/5072] Loss: 2.5049\n",
      "Epoch 2 Step [4520/5072] Loss: 2.4415\n",
      "Epoch 2 Step [4540/5072] Loss: 2.9692\n",
      "Epoch 2 Step [4560/5072] Loss: 2.9876\n",
      "Epoch 2 Step [4580/5072] Loss: 3.2193\n",
      "Epoch 2 Step [4600/5072] Loss: 3.0785\n",
      "Epoch 2 Step [4620/5072] Loss: 2.7576\n",
      "Epoch 2 Step [4640/5072] Loss: 2.7829\n",
      "Epoch 2 Step [4660/5072] Loss: 2.6991\n",
      "Epoch 2 Step [4680/5072] Loss: 3.0262\n",
      "Epoch 2 Step [4700/5072] Loss: 2.7387\n",
      "Epoch 2 Step [4720/5072] Loss: 2.6482\n",
      "Epoch 2 Step [4740/5072] Loss: 2.7450\n",
      "Epoch 2 Step [4760/5072] Loss: 3.1181\n",
      "Epoch 2 Step [4780/5072] Loss: 2.4551\n",
      "Epoch 2 Step [4800/5072] Loss: 2.9631\n",
      "Epoch 2 Step [4820/5072] Loss: 2.8519\n",
      "Epoch 2 Step [4840/5072] Loss: 2.5520\n",
      "Epoch 2 Step [4860/5072] Loss: 2.6625\n",
      "Epoch 2 Step [4880/5072] Loss: 2.8667\n",
      "Epoch 2 Step [4900/5072] Loss: 2.7680\n",
      "Epoch 2 Step [4920/5072] Loss: 2.8094\n",
      "Epoch 2 Step [4940/5072] Loss: 2.9049\n",
      "Epoch 2 Step [4960/5072] Loss: 2.9948\n",
      "Epoch 2 Step [4980/5072] Loss: 2.4011\n",
      "Epoch 2 Step [5000/5072] Loss: 2.7689\n",
      "Epoch 2 Step [5020/5072] Loss: 2.6309\n",
      "Epoch 2 Step [5040/5072] Loss: 2.7619\n",
      "Epoch 2 Step [5060/5072] Loss: 2.4298\n",
      "Epoch 2 Finished | Acc: 28.92% | Loss: 2.7483\n",
      "Model saved as trained_on_all_classes_v3.pkl\n",
      "Updated Master Model saved.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MORE_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "ACCUM_STEPS = 20\n",
    "BATCH_SIZE = 1\n",
    "FRAME_RATE_SCALER = 1\n",
    "USE_WEIGHTED_LOSS = True\n",
    "\n",
    "TRAIN_SET_FILE = \"pretrain_full_train.pkl\" # The full training set\n",
    "MODEL_FILE = \"trained_on_all_classes_v3.pkl\"\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Train Set from {TRAIN_SET_FILE}...\")\n",
    "with open(TRAIN_SET_FILE, 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Loading Master Model from {MODEL_FILE}...\")\n",
    "from video_recognition import load_model, train, save_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 3. RESUME TRAINING ---\n",
    "print(f\"Resuming Master Model training for {MORE_EPOCHS} more epochs...\")\n",
    "train(model, epochs=MORE_EPOCHS, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device, use_weighted_loss=USE_WEIGHTED_LOSS)\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "save_model(model, \"trained_on_all_classes_v3.pkl\")\n",
    "print(\"Updated Master Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5a25a",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3738db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Head: Linear(in_features=128, out_features=47, bias=True)\n",
      "New Head: Linear(in_features=128, out_features=5, bias=True)\n",
      "Classes loaded: ['dribble', 'golf', 'kick_ball', 'shoot_bow', 'swing_baseball']\n",
      "Database size: 633\n",
      "Loading existing split from finetune_subset...\n",
      "Class Distribution: Counter({'swing_baseball': 124, 'dribble': 117, 'kick_ball': 100, 'shoot_bow': 86, 'golf': 79})\n",
      "Computed Class Weights: tensor([0.8650, 1.2810, 1.0120, 1.1767, 0.8161])\n",
      "Fine-tuning for ['golf', 'dribble', 'swing_baseball', 'shoot_bow', 'kick_ball']...\n",
      "Starting Phase 1 Training (Head Only)...\n",
      "Epoch [1/10] Step [50] Avg Loss: 1.8105\n",
      "Epoch [1/10] Step [100] Avg Loss: 1.6763\n",
      "Epoch [1/10] Step [150] Avg Loss: 1.6952\n",
      "Epoch [1/10] Step [200] Avg Loss: 1.6395\n",
      "Epoch [1/10] Step [250] Avg Loss: 1.7471\n",
      "Epoch [1/10] Step [300] Avg Loss: 1.6082\n",
      "Epoch [1/10] Step [350] Avg Loss: 1.5524\n",
      "Epoch [1/10] Step [400] Avg Loss: 1.7577\n",
      "Epoch [1/10] Step [450] Avg Loss: 1.5533\n",
      "Epoch [1/10] Step [500] Avg Loss: 1.6595\n",
      "Epoch 1 Complete | Loss: 1.6750 | Acc: 25.49%\n",
      "Epoch [2/10] Step [50] Avg Loss: 1.6206\n",
      "Epoch [2/10] Step [100] Avg Loss: 1.7007\n",
      "Epoch [2/10] Step [150] Avg Loss: 1.6095\n",
      "Epoch [2/10] Step [200] Avg Loss: 1.5745\n",
      "Epoch [2/10] Step [250] Avg Loss: 1.5581\n",
      "Epoch [2/10] Step [300] Avg Loss: 1.7211\n",
      "Epoch [2/10] Step [350] Avg Loss: 1.5387\n",
      "Epoch [2/10] Step [400] Avg Loss: 1.7006\n",
      "Epoch [2/10] Step [450] Avg Loss: 1.5614\n",
      "Epoch [2/10] Step [500] Avg Loss: 1.5121\n",
      "Epoch 2 Complete | Loss: 1.6080 | Acc: 27.08%\n",
      "Epoch [3/10] Step [50] Avg Loss: 1.6918\n",
      "Epoch [3/10] Step [100] Avg Loss: 1.5166\n",
      "Epoch [3/10] Step [150] Avg Loss: 1.5740\n",
      "Epoch [3/10] Step [200] Avg Loss: 1.5827\n",
      "Epoch [3/10] Step [250] Avg Loss: 1.5848\n",
      "Epoch [3/10] Step [300] Avg Loss: 1.6244\n",
      "Epoch [3/10] Step [350] Avg Loss: 1.7246\n",
      "Epoch [3/10] Step [400] Avg Loss: 1.5049\n",
      "Epoch [3/10] Step [450] Avg Loss: 1.5623\n",
      "Epoch [3/10] Step [500] Avg Loss: 1.5509\n",
      "Epoch 3 Complete | Loss: 1.5908 | Acc: 29.45%\n",
      "Epoch [4/10] Step [50] Avg Loss: 1.5557\n",
      "Epoch [4/10] Step [100] Avg Loss: 1.5427\n",
      "Epoch [4/10] Step [150] Avg Loss: 1.4761\n",
      "Epoch [4/10] Step [200] Avg Loss: 1.5702\n",
      "Epoch [4/10] Step [250] Avg Loss: 1.5917\n",
      "Epoch [4/10] Step [300] Avg Loss: 1.3396\n",
      "Epoch [4/10] Step [350] Avg Loss: 1.6163\n",
      "Epoch [4/10] Step [400] Avg Loss: 1.4238\n",
      "Epoch [4/10] Step [450] Avg Loss: 1.5323\n",
      "Epoch [4/10] Step [500] Avg Loss: 1.4933\n",
      "Epoch 4 Complete | Loss: 1.5147 | Acc: 36.76%\n",
      "Epoch [5/10] Step [50] Avg Loss: 1.4252\n",
      "Epoch [5/10] Step [100] Avg Loss: 1.5219\n",
      "Epoch [5/10] Step [150] Avg Loss: 1.5060\n",
      "Epoch [5/10] Step [200] Avg Loss: 1.6112\n",
      "Epoch [5/10] Step [250] Avg Loss: 1.4561\n",
      "Epoch [5/10] Step [300] Avg Loss: 1.4808\n",
      "Epoch [5/10] Step [350] Avg Loss: 1.5231\n",
      "Epoch [5/10] Step [400] Avg Loss: 1.3718\n",
      "Epoch [5/10] Step [450] Avg Loss: 1.6117\n",
      "Epoch [5/10] Step [500] Avg Loss: 1.4899\n",
      "Epoch 5 Complete | Loss: 1.4986 | Acc: 35.77%\n",
      "Epoch [6/10] Step [50] Avg Loss: 1.4456\n",
      "Epoch [6/10] Step [100] Avg Loss: 1.5011\n",
      "Epoch [6/10] Step [150] Avg Loss: 1.4395\n",
      "Epoch [6/10] Step [200] Avg Loss: 1.5218\n",
      "Epoch [6/10] Step [250] Avg Loss: 1.5734\n",
      "Epoch [6/10] Step [300] Avg Loss: 1.4871\n",
      "Epoch [6/10] Step [350] Avg Loss: 1.6278\n",
      "Epoch [6/10] Step [400] Avg Loss: 1.4184\n",
      "Epoch [6/10] Step [450] Avg Loss: 1.3933\n",
      "Epoch [6/10] Step [500] Avg Loss: 1.3653\n",
      "Epoch 6 Complete | Loss: 1.4760 | Acc: 36.96%\n",
      "Epoch [7/10] Step [50] Avg Loss: 1.3816\n",
      "Epoch [7/10] Step [100] Avg Loss: 1.5984\n",
      "Epoch [7/10] Step [150] Avg Loss: 1.4989\n",
      "Epoch [7/10] Step [200] Avg Loss: 1.4790\n",
      "Epoch [7/10] Step [250] Avg Loss: 1.4853\n",
      "Epoch [7/10] Step [300] Avg Loss: 1.4996\n",
      "Epoch [7/10] Step [350] Avg Loss: 1.2918\n",
      "Epoch [7/10] Step [400] Avg Loss: 1.4699\n",
      "Epoch [7/10] Step [450] Avg Loss: 1.4517\n",
      "Epoch [7/10] Step [500] Avg Loss: 1.4496\n",
      "Epoch 7 Complete | Loss: 1.4587 | Acc: 39.53%\n",
      "Epoch [8/10] Step [50] Avg Loss: 1.4924\n",
      "Epoch [8/10] Step [100] Avg Loss: 1.5440\n",
      "Epoch [8/10] Step [150] Avg Loss: 1.5260\n",
      "Epoch [8/10] Step [200] Avg Loss: 1.4598\n",
      "Epoch [8/10] Step [250] Avg Loss: 1.3432\n",
      "Epoch [8/10] Step [300] Avg Loss: 1.3178\n",
      "Epoch [8/10] Step [350] Avg Loss: 1.3687\n",
      "Epoch [8/10] Step [400] Avg Loss: 1.3006\n",
      "Epoch [8/10] Step [450] Avg Loss: 1.2938\n",
      "Epoch [8/10] Step [500] Avg Loss: 1.3374\n",
      "Epoch 8 Complete | Loss: 1.4038 | Acc: 41.90%\n",
      "Epoch [9/10] Step [50] Avg Loss: 1.4384\n",
      "Epoch [9/10] Step [100] Avg Loss: 1.3786\n",
      "Epoch [9/10] Step [150] Avg Loss: 1.2806\n",
      "Epoch [9/10] Step [200] Avg Loss: 1.3882\n",
      "Epoch [9/10] Step [250] Avg Loss: 1.3700\n",
      "Epoch [9/10] Step [300] Avg Loss: 1.4632\n",
      "Epoch [9/10] Step [350] Avg Loss: 1.2433\n",
      "Epoch [9/10] Step [400] Avg Loss: 1.4944\n",
      "Epoch [9/10] Step [450] Avg Loss: 1.3626\n",
      "Epoch [9/10] Step [500] Avg Loss: 1.2445\n",
      "Epoch 9 Complete | Loss: 1.3706 | Acc: 45.85%\n",
      "Epoch [10/10] Step [50] Avg Loss: 1.4560\n",
      "Epoch [10/10] Step [100] Avg Loss: 1.2375\n",
      "Epoch [10/10] Step [150] Avg Loss: 1.4452\n",
      "Epoch [10/10] Step [200] Avg Loss: 1.4037\n",
      "Epoch [10/10] Step [250] Avg Loss: 1.2911\n",
      "Epoch [10/10] Step [300] Avg Loss: 1.4232\n",
      "Epoch [10/10] Step [350] Avg Loss: 1.2090\n",
      "Epoch [10/10] Step [400] Avg Loss: 1.2383\n",
      "Epoch [10/10] Step [450] Avg Loss: 1.3304\n",
      "Epoch [10/10] Step [500] Avg Loss: 1.3628\n",
      "Epoch 10 Complete | Loss: 1.3374 | Acc: 46.44%\n",
      "Model saved as finetuned_model_sports.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "TARGET_CLASSES = ['golf', 'dribble', 'swing_baseball', 'shoot_bow', 'kick_ball']\n",
    "MODEL_FILE_FINE = \"finetuned_model_sports.pkl\"\n",
    "MODEL_FILE = \"trained_on_all_classes_v3.pkl\"\n",
    "FINE_TUNE_EPOCHS=10\n",
    "LEARNING_RATE = 1e-4\n",
    "ACCUM_STEPS = 15\n",
    "FRAME_SIZE = 224\n",
    "FRAME_RATE_SCALER = 1\n",
    "BATCH_SIZE = 1\n",
    "USE_WEIGHTED_LOSS = True\n",
    "\n",
    "model = load_model(MODEL_FILE) \n",
    "\n",
    "print(\"Old Head:\", model.fc)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "input_features = model.fc.in_features \n",
    "\n",
    "num_target_classes = len(TARGET_CLASSES) \n",
    "model.fc = nn.Linear(input_features, num_target_classes)\n",
    "\n",
    "print(f\"New Head: {model.fc}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create Dataset\n",
    "subset_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=TARGET_CLASSES)\n",
    "\n",
    "# Split (sub_train is a torch.utils.data.Subset object)\n",
    "sub_train, sub_test = get_persistent_splits(subset_dataset, 0.8, \"finetune_subset\")\n",
    "\n",
    "sub_loader = DataLoader(sub_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_indices = sub_train.indices\n",
    "all_labels = [subset_dataset.db[i][1] for i in train_indices]\n",
    "\n",
    "class_counts = Counter(all_labels)\n",
    "print(\"Class Distribution:\", class_counts)\n",
    "\n",
    "sorted_counts = [class_counts[cls_name] for cls_name in subset_dataset.classes]\n",
    "\n",
    "total_samples = sum(sorted_counts)\n",
    "num_classes = len(subset_dataset.classes)\n",
    "\n",
    "weights = [total_samples / (num_classes * count) for count in sorted_counts]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "print(f\"Computed Class Weights: {class_weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(f\"Fine-tuning for {TARGET_CLASSES}...\")\n",
    "print(\"Starting Phase 1 Training (Head Only)...\")\n",
    "\n",
    "for epoch in range(FINE_TUNE_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    interval_loss = 0.0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- FIX 4: Loop over 'sub_loader', not 'train_loader' ---\n",
    "    for i, (inputs, labels) in enumerate(sub_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        loss = loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Stats\n",
    "        loss_val = loss.item() * ACCUM_STEPS\n",
    "        running_loss += loss_val\n",
    "        interval_loss += loss_val\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{FINE_TUNE_EPOCHS}] Step [{i+1}] Avg Loss: {interval_loss/50:.4f}\")\n",
    "            interval_loss = 0.0\n",
    "\n",
    "    epoch_acc = 100 * correct / total\n",
    "    avg_loss = running_loss / len(sub_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete | Loss: {avg_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Save Final Model\n",
    "save_model(model, MODEL_FILE_FINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde53172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Set from finetune_subset_test.pkl...\n",
      "Loading Model from finetuned_model_sports.pkl...\n",
      "Running Inference...\n",
      "\n",
      "========================================\n",
      "FINAL CLASSIFICATION REPORT\n",
      "========================================\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          golf       0.57      0.89      0.69        28\n",
      "       dribble       0.59      0.38      0.47        26\n",
      "swing_baseball       0.56      0.32      0.41        28\n",
      "     shoot_bow       0.65      0.50      0.57        26\n",
      "     kick_ball       0.37      0.58      0.45        19\n",
      "\n",
      "      accuracy                           0.54       127\n",
      "     macro avg       0.55      0.54      0.52       127\n",
      "  weighted avg       0.56      0.54      0.52       127\n",
      "\n",
      "Total Accuracy: 53.54%\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "TEST_SET_FILE = \"finetune_subset_test.pkl\"\n",
    "BATCH_SIZE = 1 # Keep at 1 for precise video-by-video evaluation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Test Set from {TEST_SET_FILE}...\")\n",
    "with open(TEST_SET_FILE, 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Loading Model from {MODEL_FILE_FINE}...\")\n",
    "# Use the load_model function from your module\n",
    "from video_recognition import load_model\n",
    "model = load_model(MODEL_FILE_FINE)\n",
    "model = model.to(device)\n",
    "model.eval() # Set to evaluation mode (Important: disables Dropout)\n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 4. REPORTING ---\n",
    "# A. Classification Report (Precision, Recall, F1)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(all_labels, all_preds, target_names=TARGET_CLASSES, zero_division=0))\n",
    "\n",
    "# B. Accuracy Score\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Total Accuracy: {acc*100:.2f}%\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# C. Confusion Matrix Plot\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=TARGET_CLASSES, \n",
    "            yticklabels=TARGET_CLASSES)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63526c0e",
   "metadata": {},
   "source": [
    "CONTINURE FINTUNE TRAINING (HEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "TARGET_CLASSES = ['golf', 'dribble', 'swing_baseball', 'shoot_bow', 'kick_ball']\n",
    "MODEL_FILE_FINE = \"finetuned_model_sports.pkl\"\n",
    "FINE_TUNE_EPOCHS=5\n",
    "LEARNING_RATE = 1e-5\n",
    "ACCUM_STEPS = 15\n",
    "FRAME_SIZE = 224\n",
    "FRAME_RATE_SCALER = 1\n",
    "BATCH_SIZE = 1\n",
    "USE_WEIGHTED_LOSS = True\n",
    "\n",
    "model = load_model(MODEL_FILE_FINE) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create Dataset\n",
    "subset_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=TARGET_CLASSES)\n",
    "\n",
    "# Split (sub_train is a torch.utils.data.Subset object)\n",
    "sub_train, sub_test = get_persistent_splits(subset_dataset, 0.8, \"finetune_subset\")\n",
    "\n",
    "sub_loader = DataLoader(sub_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_indices = sub_train.indices\n",
    "all_labels = [subset_dataset.db[i][1] for i in train_indices]\n",
    "\n",
    "class_counts = Counter(all_labels)\n",
    "print(\"Class Distribution:\", class_counts)\n",
    "\n",
    "sorted_counts = [class_counts[cls_name] for cls_name in subset_dataset.classes]\n",
    "\n",
    "total_samples = sum(sorted_counts)\n",
    "num_classes = len(subset_dataset.classes)\n",
    "\n",
    "weights = [total_samples / (num_classes * count) for count in sorted_counts]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "print(f\"Computed Class Weights: {class_weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(f\"Fine-tuning for {TARGET_CLASSES}...\")\n",
    "print(\"Starting Phase 1 Training (Head Only)...\")\n",
    "\n",
    "for epoch in range(FINE_TUNE_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    interval_loss = 0.0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # --- FIX 4: Loop over 'sub_loader', not 'train_loader' ---\n",
    "    for i, (inputs, labels) in enumerate(sub_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        loss = loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Stats\n",
    "        loss_val = loss.item() * ACCUM_STEPS\n",
    "        running_loss += loss_val\n",
    "        interval_loss += loss_val\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{FINE_TUNE_EPOCHS}] Step [{i+1}] Avg Loss: {interval_loss/50:.4f}\")\n",
    "            interval_loss = 0.0\n",
    "\n",
    "    epoch_acc = 100 * correct / total\n",
    "    avg_loss = running_loss / len(sub_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Complete | Loss: {avg_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Save Final Model\n",
    "save_model(model, MODEL_FILE_FINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878e0c8",
   "metadata": {},
   "source": [
    "CONTINUTE FINETUNE TRAINING (ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Set from finetune_subset_train.pkl...\n",
      "Loading Model from finetuned_model_sports.pkl...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x00'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_FILE_FINE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvideo_recognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model, train, save_model\n\u001b[1;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_FILE_FINE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- 3. RESUME TRAINING ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marti\\Desktop\\Magistrale\\secondo anno\\computational learning\\progetto\\video_recognition.py:188\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_file)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(model_file):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 188\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'."
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MORE_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4 # Keep this low for fine-tuning/resuming\n",
    "ACCUM_STEPS = 20\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "TRAIN_SET_FILE = \"finetune_subset_train.pkl\"\n",
    "MODEL_FILE_FINE = \"finetuned_model_sports.pkl\"\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Train Set from {TRAIN_SET_FILE}...\")\n",
    "with open(TRAIN_SET_FILE, 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Loading Model from {MODEL_FILE_FINE}...\")\n",
    "from video_recognition import load_model, train, save_model\n",
    "model = load_model(MODEL_FILE_FINE)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 3. RESUME TRAINING ---\n",
    "print(f\"Resuming training for {MORE_EPOCHS} more epochs...\")\n",
    "train(model, epochs=MORE_EPOCHS, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device, use_weighted_loss=USE_WEIGHTED_LOSS)\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "save_model(model, MODEL_FILE_FINE)\n",
    "print(\"Updated model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039eea8",
   "metadata": {},
   "source": [
    "Extra idea, Smoke detector, Ecc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f89967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained_on_all_classes_v3.pkl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
