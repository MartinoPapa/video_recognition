{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266e3ae2",
   "metadata": {},
   "source": [
    "# Video Recognition\n",
    "\n",
    "Project on video recognition whith the dataset HMDB51 (https://serre.lab.brown.edu/hmdb51.html). A special focus is given to the efficiency of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c35e7",
   "metadata": {},
   "source": [
    "Training finora (loss bilanciata):\n",
    "- 2 epoche lr=5e-4 FRAME RATE A 3\n",
    "- 5 epoche lr=1e-4 FRAME RATE A 3\n",
    "- 1 epoche kr=5e-5 FRAME RATE A 3\n",
    "IDEA: FARE UN PO' DI EPOCHE CON FRAME RATE ALTO E POI ABBASSARLO ALLA FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a71d5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5339e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle\n",
    "\n",
    "# Import everything from your new file\n",
    "from video_recognition import (\n",
    "    VideoLoader, CNN, CNNLSTM, train, save_model, load_model, \n",
    "    replace_head_for_finetuning, MAX_POOL, get_persistent_splits\n",
    ")\n",
    "\n",
    "dataset_directory = \"./dataset\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026c4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 224\n",
    "FRAME_RATE_SCALER = 3\n",
    "BATCH_SIZE = 1\n",
    "ACCUM_STEPS = 20\n",
    "EMBEDDING_DIM = 256\n",
    "LSTM_HIDDEN = 128\n",
    "LSTM_LAYERS = 1\n",
    "USE_WEIGHTED_LOSS=True\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "cnn_config = [\n",
    "    {'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'out_channels': 128, 'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3595200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes loaded: ['clap', 'climb', 'climb_stairs', 'dive', 'draw_sword', 'dribble', 'drink', 'eat', 'fall_floor', 'fencing', 'flic_flac', 'golf', 'handstand', 'hit', 'hug', 'jump', 'kick', 'kick_ball', 'kiss', 'laugh', 'pick', 'pour', 'pullup', 'punch', 'push', 'pushup', 'ride_bike', 'ride_horse', 'run', 'shake_hands', 'shoot_ball', 'shoot_bow', 'shoot_gun', 'sit', 'situp', 'smile', 'smoke', 'somersault', 'stand', 'swing_baseball', 'sword', 'sword_exercise', 'talk', 'throw', 'turn', 'walk', 'wave']\n",
      "Database size: 6341\n",
      "Loading existing split from pretrain_full...\n",
      "Pre-training on 47 classes...\n",
      "Calculating class weights for Weighted Loss...\n",
      "Class Weights (Shape torch.Size([47])): tensor([1.0685, 1.1859, 1.1480, 1.1604, 1.1859, 0.9303, 0.8175, 1.3002, 0.9722,\n",
      "        1.1241, 1.2404, 1.3835, 1.2125, 1.0685, 1.1859, 0.8431, 0.9810, 1.0900,\n",
      "        1.3323, 1.0477, 1.1991, 1.2548, 1.3660, 1.0685, 1.1241, 1.3835, 1.3160,\n",
      "        1.1604, 0.5563, 0.8993, 1.0086, 1.3160, 1.2125, 0.9900, 1.2548, 1.3323,\n",
      "        1.3002, 0.9466, 0.8633, 0.9303, 1.0376, 1.1012, 1.0791, 1.4583, 0.5833,\n",
      "        0.2409, 1.4015])\n",
      "Epoch 1 Step [20/5072] Loss: 3.8407\n",
      "Epoch 1 Step [40/5072] Loss: 3.8528\n",
      "Epoch 1 Step [60/5072] Loss: 3.8164\n",
      "Epoch 1 Step [80/5072] Loss: 3.8679\n",
      "Epoch 1 Step [100/5072] Loss: 3.8851\n",
      "Epoch 1 Step [120/5072] Loss: 3.7443\n",
      "Epoch 1 Step [140/5072] Loss: 3.9917\n",
      "Epoch 1 Step [160/5072] Loss: 3.8758\n",
      "Epoch 1 Step [180/5072] Loss: 3.7752\n",
      "Epoch 1 Step [200/5072] Loss: 4.0182\n",
      "Epoch 1 Step [220/5072] Loss: 3.8193\n",
      "Epoch 1 Step [240/5072] Loss: 3.8723\n",
      "Epoch 1 Step [260/5072] Loss: 3.8312\n",
      "Epoch 1 Step [280/5072] Loss: 3.7756\n",
      "Epoch 1 Step [300/5072] Loss: 3.8652\n",
      "Epoch 1 Step [320/5072] Loss: 3.8179\n",
      "Epoch 1 Step [340/5072] Loss: 3.8121\n",
      "Epoch 1 Step [360/5072] Loss: 3.8646\n",
      "Epoch 1 Step [380/5072] Loss: 3.7467\n",
      "Epoch 1 Step [400/5072] Loss: 3.8434\n",
      "Epoch 1 Step [420/5072] Loss: 3.8820\n",
      "Epoch 1 Step [440/5072] Loss: 3.8126\n",
      "Epoch 1 Step [460/5072] Loss: 3.8532\n",
      "Epoch 1 Step [480/5072] Loss: 3.7799\n",
      "Epoch 1 Step [500/5072] Loss: 3.9009\n",
      "Epoch 1 Step [520/5072] Loss: 3.8187\n",
      "Epoch 1 Step [540/5072] Loss: 3.8852\n",
      "Epoch 1 Step [560/5072] Loss: 3.8484\n",
      "Epoch 1 Step [580/5072] Loss: 3.8961\n",
      "Epoch 1 Step [600/5072] Loss: 3.7206\n",
      "Epoch 1 Step [620/5072] Loss: 3.8441\n",
      "Epoch 1 Step [640/5072] Loss: 3.8487\n",
      "Epoch 1 Step [660/5072] Loss: 3.8266\n",
      "Epoch 1 Step [680/5072] Loss: 3.8162\n",
      "Epoch 1 Step [700/5072] Loss: 3.8215\n",
      "Epoch 1 Step [720/5072] Loss: 3.7828\n",
      "Epoch 1 Step [740/5072] Loss: 3.8103\n",
      "Epoch 1 Step [760/5072] Loss: 3.7812\n",
      "Epoch 1 Step [780/5072] Loss: 3.6997\n",
      "Epoch 1 Step [800/5072] Loss: 3.7155\n",
      "Epoch 1 Step [820/5072] Loss: 3.7782\n",
      "Epoch 1 Step [840/5072] Loss: 3.9000\n",
      "Epoch 1 Step [860/5072] Loss: 3.8631\n",
      "Epoch 1 Step [880/5072] Loss: 3.8382\n",
      "Epoch 1 Step [900/5072] Loss: 3.8480\n",
      "Epoch 1 Step [920/5072] Loss: 3.6285\n",
      "Epoch 1 Step [940/5072] Loss: 3.7222\n",
      "Epoch 1 Step [960/5072] Loss: 3.7021\n",
      "Epoch 1 Step [980/5072] Loss: 3.8122\n",
      "Epoch 1 Step [1000/5072] Loss: 3.7588\n",
      "Epoch 1 Step [1020/5072] Loss: 3.5740\n",
      "Epoch 1 Step [1040/5072] Loss: 3.8446\n",
      "Epoch 1 Step [1060/5072] Loss: 3.8124\n",
      "Epoch 1 Step [1080/5072] Loss: 3.7836\n",
      "Epoch 1 Step [1100/5072] Loss: 3.7361\n",
      "Epoch 1 Step [1120/5072] Loss: 3.6124\n",
      "Epoch 1 Step [1140/5072] Loss: 3.8213\n",
      "Epoch 1 Step [1160/5072] Loss: 3.7371\n",
      "Epoch 1 Step [1180/5072] Loss: 3.9600\n",
      "Epoch 1 Step [1200/5072] Loss: 3.8055\n",
      "Epoch 1 Step [1220/5072] Loss: 3.6954\n",
      "Epoch 1 Step [1240/5072] Loss: 3.7341\n",
      "Epoch 1 Step [1260/5072] Loss: 3.6048\n",
      "Epoch 1 Step [1280/5072] Loss: 3.8492\n",
      "Epoch 1 Step [1300/5072] Loss: 3.7702\n",
      "Epoch 1 Step [1320/5072] Loss: 3.7031\n",
      "Epoch 1 Step [1340/5072] Loss: 3.4911\n",
      "Epoch 1 Step [1360/5072] Loss: 3.8295\n",
      "Epoch 1 Step [1380/5072] Loss: 3.6852\n",
      "Epoch 1 Step [1400/5072] Loss: 3.9724\n",
      "Epoch 1 Step [1420/5072] Loss: 3.9211\n",
      "Epoch 1 Step [1440/5072] Loss: 4.0651\n",
      "Epoch 1 Step [1460/5072] Loss: 3.8205\n",
      "Epoch 1 Step [1480/5072] Loss: 3.8150\n",
      "Epoch 1 Step [1500/5072] Loss: 3.6154\n",
      "Epoch 1 Step [1520/5072] Loss: 3.6746\n",
      "Epoch 1 Step [1540/5072] Loss: 3.7752\n",
      "Epoch 1 Step [1560/5072] Loss: 3.8704\n",
      "Epoch 1 Step [1580/5072] Loss: 3.6831\n",
      "Epoch 1 Step [1600/5072] Loss: 3.7934\n",
      "Epoch 1 Step [1620/5072] Loss: 3.8705\n",
      "Epoch 1 Step [1640/5072] Loss: 3.7681\n",
      "Epoch 1 Step [1660/5072] Loss: 3.6894\n",
      "Epoch 1 Step [1680/5072] Loss: 3.7457\n",
      "Epoch 1 Step [1700/5072] Loss: 3.6405\n",
      "Epoch 1 Step [1720/5072] Loss: 3.6459\n",
      "Epoch 1 Step [1740/5072] Loss: 3.7225\n",
      "Epoch 1 Step [1760/5072] Loss: 3.6358\n",
      "Epoch 1 Step [1780/5072] Loss: 3.6971\n",
      "Epoch 1 Step [1800/5072] Loss: 3.7054\n",
      "Epoch 1 Step [1820/5072] Loss: 3.8225\n",
      "Epoch 1 Step [1840/5072] Loss: 3.6891\n",
      "Epoch 1 Step [1860/5072] Loss: 3.6632\n",
      "Epoch 1 Step [1880/5072] Loss: 3.6039\n",
      "Epoch 1 Step [1900/5072] Loss: 3.6843\n",
      "Epoch 1 Step [1920/5072] Loss: 3.8321\n",
      "Epoch 1 Step [1940/5072] Loss: 3.6397\n",
      "Epoch 1 Step [1960/5072] Loss: 3.7562\n",
      "Epoch 1 Step [1980/5072] Loss: 3.7298\n",
      "Epoch 1 Step [2000/5072] Loss: 3.6834\n",
      "Epoch 1 Step [2020/5072] Loss: 3.6591\n",
      "Epoch 1 Step [2040/5072] Loss: 3.7698\n",
      "Epoch 1 Step [2060/5072] Loss: 3.5958\n",
      "Epoch 1 Step [2080/5072] Loss: 3.7965\n",
      "Epoch 1 Step [2100/5072] Loss: 3.7867\n",
      "Epoch 1 Step [2120/5072] Loss: 3.7253\n",
      "Epoch 1 Step [2140/5072] Loss: 3.4915\n",
      "Epoch 1 Step [2160/5072] Loss: 3.4286\n",
      "Epoch 1 Step [2180/5072] Loss: 3.5574\n",
      "Epoch 1 Step [2200/5072] Loss: 3.5678\n",
      "Epoch 1 Step [2220/5072] Loss: 3.9545\n",
      "Epoch 1 Step [2240/5072] Loss: 3.6354\n",
      "Epoch 1 Step [2260/5072] Loss: 3.5654\n",
      "Epoch 1 Step [2280/5072] Loss: 3.4559\n",
      "Epoch 1 Step [2300/5072] Loss: 3.6536\n",
      "Epoch 1 Step [2320/5072] Loss: 3.7950\n",
      "Epoch 1 Step [2340/5072] Loss: 3.5815\n",
      "Epoch 1 Step [2360/5072] Loss: 3.7450\n",
      "Epoch 1 Step [2380/5072] Loss: 3.5423\n",
      "Epoch 1 Step [2400/5072] Loss: 3.6883\n",
      "Epoch 1 Step [2420/5072] Loss: 3.3935\n",
      "Epoch 1 Step [2440/5072] Loss: 3.4227\n",
      "Epoch 1 Step [2460/5072] Loss: 3.6945\n",
      "Epoch 1 Step [2480/5072] Loss: 3.4739\n",
      "Epoch 1 Step [2500/5072] Loss: 3.5894\n",
      "Epoch 1 Step [2520/5072] Loss: 3.5750\n",
      "Epoch 1 Step [2540/5072] Loss: 3.9083\n",
      "Epoch 1 Step [2560/5072] Loss: 3.8437\n",
      "Epoch 1 Step [2580/5072] Loss: 3.4971\n",
      "Epoch 1 Step [2600/5072] Loss: 3.5340\n",
      "Epoch 1 Step [2620/5072] Loss: 3.8717\n",
      "Epoch 1 Step [2640/5072] Loss: 3.4382\n",
      "Epoch 1 Step [2660/5072] Loss: 3.4939\n",
      "Epoch 1 Step [2680/5072] Loss: 3.5679\n",
      "Epoch 1 Step [2700/5072] Loss: 3.8385\n",
      "Epoch 1 Step [2720/5072] Loss: 3.4918\n",
      "Epoch 1 Step [2740/5072] Loss: 3.7264\n",
      "Epoch 1 Step [2760/5072] Loss: 3.4684\n",
      "Epoch 1 Step [2780/5072] Loss: 3.5851\n",
      "Epoch 1 Step [2800/5072] Loss: 3.6090\n",
      "Epoch 1 Step [2820/5072] Loss: 3.6493\n",
      "Epoch 1 Step [2840/5072] Loss: 3.6100\n",
      "Epoch 1 Step [2860/5072] Loss: 3.4617\n",
      "Epoch 1 Step [2880/5072] Loss: 3.7118\n",
      "Epoch 1 Step [2900/5072] Loss: 3.4798\n",
      "Epoch 1 Step [2920/5072] Loss: 3.6285\n",
      "Epoch 1 Step [2940/5072] Loss: 3.6339\n",
      "Epoch 1 Step [2960/5072] Loss: 3.5102\n",
      "Epoch 1 Step [2980/5072] Loss: 3.6775\n",
      "Epoch 1 Step [3000/5072] Loss: 3.7365\n",
      "Epoch 1 Step [3020/5072] Loss: 3.7939\n",
      "Epoch 1 Step [3040/5072] Loss: 3.6560\n",
      "Epoch 1 Step [3060/5072] Loss: 3.7610\n",
      "Epoch 1 Step [3080/5072] Loss: 3.6552\n",
      "Epoch 1 Step [3100/5072] Loss: 3.7314\n",
      "Epoch 1 Step [3120/5072] Loss: 3.4156\n",
      "Epoch 1 Step [3140/5072] Loss: 3.9481\n",
      "Epoch 1 Step [3160/5072] Loss: 3.8824\n",
      "Epoch 1 Step [3180/5072] Loss: 3.7574\n",
      "Epoch 1 Step [3200/5072] Loss: 3.9037\n",
      "Epoch 1 Step [3220/5072] Loss: 3.7820\n",
      "Epoch 1 Step [3240/5072] Loss: 3.9717\n",
      "Epoch 1 Step [3260/5072] Loss: 3.6749\n",
      "Epoch 1 Step [3280/5072] Loss: 3.8697\n",
      "Epoch 1 Step [3300/5072] Loss: 3.7157\n",
      "Epoch 1 Step [3320/5072] Loss: 3.8243\n",
      "Epoch 1 Step [3340/5072] Loss: 3.7105\n",
      "Epoch 1 Step [3360/5072] Loss: 3.5620\n",
      "Epoch 1 Step [3380/5072] Loss: 3.6747\n",
      "Epoch 1 Step [3400/5072] Loss: 3.4166\n",
      "Epoch 1 Step [3420/5072] Loss: 3.5475\n",
      "Epoch 1 Step [3440/5072] Loss: 3.7836\n",
      "Epoch 1 Step [3460/5072] Loss: 3.8467\n",
      "Epoch 1 Step [3480/5072] Loss: 3.7162\n",
      "Epoch 1 Step [3500/5072] Loss: 3.5911\n",
      "Epoch 1 Step [3520/5072] Loss: 3.6265\n",
      "Epoch 1 Step [3540/5072] Loss: 3.8170\n",
      "Epoch 1 Step [3560/5072] Loss: 3.9161\n",
      "Epoch 1 Step [3580/5072] Loss: 3.7834\n",
      "Epoch 1 Step [3600/5072] Loss: 3.5455\n",
      "Epoch 1 Step [3620/5072] Loss: 3.7903\n",
      "Epoch 1 Step [3640/5072] Loss: 3.6448\n",
      "Epoch 1 Step [3660/5072] Loss: 3.5334\n",
      "Epoch 1 Step [3680/5072] Loss: 3.4563\n",
      "Epoch 1 Step [3700/5072] Loss: 3.6710\n",
      "Epoch 1 Step [3720/5072] Loss: 3.8638\n",
      "Epoch 1 Step [3740/5072] Loss: 3.6519\n",
      "Epoch 1 Step [3760/5072] Loss: 3.7413\n",
      "Epoch 1 Step [3780/5072] Loss: 3.6419\n",
      "Epoch 1 Step [3800/5072] Loss: 3.5768\n",
      "Epoch 1 Step [3820/5072] Loss: 3.6679\n",
      "Epoch 1 Step [3840/5072] Loss: 3.4297\n",
      "Epoch 1 Step [3860/5072] Loss: 3.5858\n",
      "Epoch 1 Step [3880/5072] Loss: 3.5901\n",
      "Epoch 1 Step [3900/5072] Loss: 3.7052\n",
      "Epoch 1 Step [3920/5072] Loss: 3.5089\n",
      "Epoch 1 Step [3940/5072] Loss: 3.5416\n",
      "Epoch 1 Step [3960/5072] Loss: 3.7534\n",
      "Epoch 1 Step [3980/5072] Loss: 3.7718\n",
      "Epoch 1 Step [4000/5072] Loss: 3.4299\n",
      "Epoch 1 Step [4020/5072] Loss: 3.8142\n",
      "Epoch 1 Step [4040/5072] Loss: 3.5178\n",
      "Epoch 1 Step [4060/5072] Loss: 3.7583\n",
      "Epoch 1 Step [4080/5072] Loss: 3.6681\n",
      "Epoch 1 Step [4100/5072] Loss: 3.6334\n",
      "Epoch 1 Step [4120/5072] Loss: 3.3868\n",
      "Epoch 1 Step [4140/5072] Loss: 3.3639\n",
      "Epoch 1 Step [4160/5072] Loss: 3.7717\n",
      "Epoch 1 Step [4180/5072] Loss: 3.8140\n",
      "Epoch 1 Step [4200/5072] Loss: 3.7482\n",
      "Epoch 1 Step [4220/5072] Loss: 3.7925\n",
      "Epoch 1 Step [4240/5072] Loss: 3.5806\n",
      "Epoch 1 Step [4260/5072] Loss: 3.7191\n",
      "Epoch 1 Step [4280/5072] Loss: 3.6927\n",
      "Epoch 1 Step [4300/5072] Loss: 3.3382\n",
      "Epoch 1 Step [4320/5072] Loss: 3.6311\n",
      "Epoch 1 Step [4340/5072] Loss: 3.6029\n",
      "Epoch 1 Step [4360/5072] Loss: 3.7681\n",
      "Epoch 1 Step [4380/5072] Loss: 3.6665\n",
      "Epoch 1 Step [4400/5072] Loss: 3.5514\n",
      "Epoch 1 Step [4420/5072] Loss: 3.8048\n",
      "Epoch 1 Step [4440/5072] Loss: 3.7381\n",
      "Epoch 1 Step [4460/5072] Loss: 3.7821\n",
      "Epoch 1 Step [4480/5072] Loss: 3.7914\n",
      "Epoch 1 Step [4500/5072] Loss: 3.5281\n",
      "Epoch 1 Step [4520/5072] Loss: 3.4720\n",
      "Epoch 1 Step [4540/5072] Loss: 3.6648\n",
      "Epoch 1 Step [4560/5072] Loss: 3.4507\n",
      "Epoch 1 Step [4580/5072] Loss: 3.6274\n",
      "Epoch 1 Step [4600/5072] Loss: 3.6487\n",
      "Epoch 1 Step [4620/5072] Loss: 3.5699\n",
      "Epoch 1 Step [4640/5072] Loss: 3.9143\n",
      "Epoch 1 Step [4660/5072] Loss: 3.6606\n",
      "Epoch 1 Step [4680/5072] Loss: 3.8165\n",
      "Epoch 1 Step [4700/5072] Loss: 3.4614\n",
      "Epoch 1 Step [4720/5072] Loss: 3.5366\n",
      "Epoch 1 Step [4740/5072] Loss: 3.6112\n",
      "Epoch 1 Step [4760/5072] Loss: 3.7125\n",
      "Epoch 1 Step [4780/5072] Loss: 3.4366\n",
      "Epoch 1 Step [4800/5072] Loss: 3.4689\n",
      "Epoch 1 Step [4820/5072] Loss: 3.8537\n",
      "Epoch 1 Step [4840/5072] Loss: 3.4662\n",
      "Epoch 1 Step [4860/5072] Loss: 3.3232\n",
      "Epoch 1 Step [4880/5072] Loss: 3.5961\n",
      "Epoch 1 Step [4900/5072] Loss: 3.7060\n",
      "Epoch 1 Step [4920/5072] Loss: 3.5625\n",
      "Epoch 1 Step [4940/5072] Loss: 3.7748\n",
      "Epoch 1 Step [4960/5072] Loss: 3.6531\n",
      "Epoch 1 Step [4980/5072] Loss: 3.5899\n",
      "Epoch 1 Step [5000/5072] Loss: 3.4140\n",
      "Epoch 1 Step [5020/5072] Loss: 3.7598\n",
      "Epoch 1 Step [5040/5072] Loss: 3.6054\n",
      "Epoch 1 Step [5060/5072] Loss: 3.5264\n",
      "Epoch 1 Finished | Acc: 8.52% | Loss: 3.7008\n",
      "Model saved as trained_on_all_classes.pkl\n"
     ]
    }
   ],
   "source": [
    "# 1. Load ENTIRE dataset\n",
    "full_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=None)\n",
    "\n",
    "# 2. Get Persistent Split (Will create 'pretrain_full_train.pkl' and 'pretrain_full_test.pkl')\n",
    "full_train, full_test = get_persistent_splits(full_dataset, 0.8, \"pretrain_full\")\n",
    "\n",
    "train_loader = DataLoader(full_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 3. Initialize Model\n",
    "cnn = CNN(cnn_config, MAX_POOL, (3, FRAME_SIZE, FRAME_SIZE), EMBEDDING_DIM)\n",
    "model = CNNLSTM(cnn, len(full_dataset.classes), LSTM_HIDDEN, LSTM_LAYERS).to(device)\n",
    "\n",
    "# 4. Train\n",
    "print(f\"Pre-training on {len(full_dataset.classes)} classes...\")\n",
    "train(model, epochs=1, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device, use_weighted_loss=USE_WEIGHTED_LOSS)\n",
    "\n",
    "# 5. Save Master Model\n",
    "save_model(model, \"trained_on_all_classes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767504de",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "196503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Set from pretrain_full_test.pkl...\n",
      "Evaluating on 47 classes.\n",
      "Loading Master Model from trained_on_all_classes.pkl...\n",
      "Running Inference...\n",
      "\n",
      "========================================\n",
      "MASTER MODEL CLASSIFICATION REPORT\n",
      "========================================\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          clap       0.00      0.00      0.00        29\n",
      "         climb       0.05      0.06      0.05        17\n",
      "  climb_stairs       0.00      0.00      0.00        18\n",
      "          dive       0.50      0.03      0.06        34\n",
      "    draw_sword       0.12      0.17      0.14        12\n",
      "       dribble       0.25      0.59      0.35        29\n",
      "         drink       0.00      0.00      0.00        32\n",
      "           eat       0.00      0.00      0.00        25\n",
      "    fall_floor       0.00      0.00      0.00        25\n",
      "       fencing       1.00      0.05      0.10        20\n",
      "     flic_flac       0.00      0.00      0.00        20\n",
      "          golf       0.23      0.22      0.23        27\n",
      "     handstand       0.16      0.17      0.16        24\n",
      "           hit       0.10      0.04      0.06        26\n",
      "           hug       0.17      0.19      0.18        27\n",
      "          jump       0.12      0.04      0.06        23\n",
      "          kick       0.00      0.00      0.00        20\n",
      "     kick_ball       0.14      0.03      0.06        29\n",
      "          kiss       0.00      0.00      0.00        21\n",
      "         laugh       0.20      0.36      0.26        25\n",
      "          pick       0.00      0.00      0.00        16\n",
      "          pour       0.23      0.25      0.24        20\n",
      "        pullup       0.00      0.00      0.00        25\n",
      "         punch       0.50      0.32      0.39        25\n",
      "          push       0.00      0.00      0.00        20\n",
      "        pushup       0.31      0.16      0.21        25\n",
      "     ride_bike       0.17      0.05      0.07        21\n",
      "    ride_horse       0.00      0.00      0.00        23\n",
      "           run       0.00      0.00      0.00        38\n",
      "   shake_hands       0.12      0.19      0.15        42\n",
      "    shoot_ball       0.19      0.54      0.28        24\n",
      "     shoot_bow       0.26      0.27      0.26        30\n",
      "     shoot_gun       1.00      0.07      0.13        14\n",
      "           sit       0.00      0.00      0.00        33\n",
      "         situp       0.11      0.11      0.11        19\n",
      "         smile       0.25      0.05      0.08        21\n",
      "         smoke       0.26      0.19      0.22        26\n",
      "    somersault       0.22      0.27      0.24        26\n",
      "         stand       0.00      0.00      0.00        29\n",
      "swing_baseball       0.29      0.37      0.33        27\n",
      "         sword       0.00      0.00      0.00        23\n",
      "sword_exercise       0.31      0.17      0.22        29\n",
      "          talk       0.00      0.00      0.00        20\n",
      "         throw       0.50      0.21      0.30        28\n",
      "          turn       0.00      0.00      0.00        55\n",
      "          walk       0.14      0.86      0.25       100\n",
      "          wave       0.00      0.00      0.00        27\n",
      "\n",
      "      accuracy                           0.17      1269\n",
      "     macro avg       0.17      0.13      0.11      1269\n",
      "  weighted avg       0.16      0.17      0.12      1269\n",
      "\n",
      "Total Accuracy: 17.26%\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from video_recognition import load_model \n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_FILE = \"trained_on_all_classes.pkl\"\n",
    "TEST_SET_FILE = \"pretrain_full_test.pkl\"\n",
    "BATCH_SIZE = 1 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Test Set from {TEST_SET_FILE}...\")\n",
    "with open(TEST_SET_FILE, 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "# Get the list of ALL 51 classes\n",
    "CLASSES = test_set.dataset.classes \n",
    "print(f\"Evaluating on {len(CLASSES)} classes.\")\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Loading Master Model from {MODEL_FILE}...\")\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 4. REPORTING ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MASTER MODEL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# --- FIX: Explicitly pass the range of labels to handle missing classes ---\n",
    "all_possible_labels = range(len(CLASSES))\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    labels=all_possible_labels,  # <--- FIX IS HERE\n",
    "    target_names=CLASSES, \n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Total Accuracy: {acc*100:.2f}%\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde071b",
   "metadata": {},
   "source": [
    "## Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Set from pretrain_full_train.pkl...\n",
      "Loading Master Model from trained_on_all_classes.pkl...\n",
      "Resuming Master Model training for 1 more epochs...\n",
      "Calculating class weights for Weighted Loss...\n",
      "Class Weights (Shape torch.Size([47])): tensor([1.0685, 1.1859, 1.1480, 1.1604, 1.1859, 0.9303, 0.8175, 1.3002, 0.9722,\n",
      "        1.1241, 1.2404, 1.3835, 1.2125, 1.0685, 1.1859, 0.8431, 0.9810, 1.0900,\n",
      "        1.3323, 1.0477, 1.1991, 1.2548, 1.3660, 1.0685, 1.1241, 1.3835, 1.3160,\n",
      "        1.1604, 0.5563, 0.8993, 1.0086, 1.3160, 1.2125, 0.9900, 1.2548, 1.3323,\n",
      "        1.3002, 0.9466, 0.8633, 0.9303, 1.0376, 1.1012, 1.0791, 1.4583, 0.5833,\n",
      "        0.2409, 1.4015])\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MORE_EPOCHS = 1\n",
    "LEARNING_RATE = 5e-5 # Standard LR for pre-training (higher than fine-tuning)\n",
    "ACCUM_STEPS = 20\n",
    "BATCH_SIZE = 1\n",
    "FRAME_RATE_SCALER = 3\n",
    "USE_WEIGHTED_LOSS = True\n",
    "\n",
    "TRAIN_SET_FILE = \"pretrain_full_train.pkl\" # The full training set\n",
    "MODEL_FILE = \"trained_on_all_classes.pkl\"\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Train Set from {TRAIN_SET_FILE}...\")\n",
    "with open(TRAIN_SET_FILE, 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Loading Master Model from {MODEL_FILE}...\")\n",
    "from video_recognition import load_model, train, save_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 3. RESUME TRAINING ---\n",
    "print(f\"Resuming Master Model training for {MORE_EPOCHS} more epochs...\")\n",
    "train(model, epochs=MORE_EPOCHS, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device, use_weighted_loss=USE_WEIGHTED_LOSS)\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "save_model(model, MODEL_FILE)\n",
    "print(\"Updated Master Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5a25a",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Subset\n",
    "TARGET_CLASSES = ['jump', 'run', 'smile', 'wave']\n",
    "\n",
    "# 2. Load Subset Dataset\n",
    "subset_dataset = VideoLoader(dataset_directory, FRAME_SIZE, FRAME_RATE_SCALER, classes_to_use=TARGET_CLASSES)\n",
    "\n",
    "# 3. Get Persistent Split (Will create 'finetune_subset_train.pkl' and 'finetune_subset_test.pkl')\n",
    "# IMPORTANT: This ensures that even if you restart the kernel, you test on the EXACT same subset videos.\n",
    "sub_train, sub_test = get_persistent_splits(subset_dataset, 0.8, \"finetune_subset\")\n",
    "\n",
    "sub_loader = DataLoader(sub_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "# 4. Load Master Model & Modify\n",
    "model = load_model(\"trained_on_all_classes.pkl\")\n",
    "model = model.to(device)\n",
    "model = replace_head_for_finetuning(model, new_num_classes=len(TARGET_CLASSES))\n",
    "model = model.to(device)\n",
    "\n",
    "# 5. Fine-Tune\n",
    "print(f\"Fine-tuning for {TARGET_CLASSES}...\")\n",
    "train(model, epochs=5, accumulation_steps=ACCUM_STEPS, learning_rate=1e-4, train_loader=sub_loader, device=device)\n",
    "\n",
    "# 6. Save Final Model\n",
    "save_model(model, \"finetuned_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde53172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MODEL_FILE = \"finetuned_model.pkl\"\n",
    "TEST_SET_FILE = \"finetune_subset_test.pkl\"\n",
    "BATCH_SIZE = 1 # Keep at 1 for precise video-by-video evaluation\n",
    "\n",
    "# Define the classes again to ensure the labels match the report\n",
    "# (Must match the order used in fine-tuning)\n",
    "TARGET_CLASSES = ['jump', 'run', 'smile', 'wave'] \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Test Set from {TEST_SET_FILE}...\")\n",
    "with open(TEST_SET_FILE, 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Loading Model from {MODEL_FILE}...\")\n",
    "# Use the load_model function from your module\n",
    "from video_recognition import load_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "model.eval() # Set to evaluation mode (Important: disables Dropout)\n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "print(\"Running Inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 4. REPORTING ---\n",
    "# A. Classification Report (Precision, Recall, F1)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(classification_report(all_labels, all_preds, target_names=TARGET_CLASSES, zero_division=0))\n",
    "\n",
    "# B. Accuracy Score\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Total Accuracy: {acc*100:.2f}%\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# C. Confusion Matrix Plot\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=TARGET_CLASSES, \n",
    "            yticklabels=TARGET_CLASSES)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "MORE_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5 # Keep this low for fine-tuning/resuming\n",
    "ACCUM_STEPS = 10\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "TRAIN_SET_FILE = \"finetune_subset_train.pkl\"\n",
    "MODEL_FILE = \"finetuned_model.pkl\"\n",
    "\n",
    "# --- 2. LOAD DATA & MODEL ---\n",
    "print(f\"Loading Train Set from {TRAIN_SET_FILE}...\")\n",
    "with open(TRAIN_SET_FILE, 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Loading Model from {MODEL_FILE}...\")\n",
    "from video_recognition import load_model, train, save_model\n",
    "model = load_model(MODEL_FILE)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- 3. RESUME TRAINING ---\n",
    "print(f\"Resuming training for {MORE_EPOCHS} more epochs...\")\n",
    "train(model, epochs=MORE_EPOCHS, accumulation_steps=ACCUM_STEPS, learning_rate=LEARNING_RATE, train_loader=train_loader, device=device)\n",
    "\n",
    "# --- 4. SAVE ---\n",
    "save_model(model, MODEL_FILE)\n",
    "print(\"Updated model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f89967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
